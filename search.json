[
  {
    "objectID": "api/data/transform.html",
    "href": "api/data/transform.html",
    "title": "Transform",
    "section": "",
    "text": "source\n\n\n\n log_transform_y (data:tuple)\n\nLog-10 transform of the target value\nTakes and returns all (X, y, X_id, depth_order) tuple to be able to pipe the function\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ntuple\n(X, y, X_id, depth_order)"
  },
  {
    "objectID": "api/data/transform.html#features-spectra",
    "href": "api/data/transform.html#features-spectra",
    "title": "Transform",
    "section": "Features (spectra)",
    "text": "Features (spectra)\n\nsource\n\nTakeDerivative\n\n TakeDerivative (window_length=11, polyorder=1, deriv=1)\n\nCreates scikit-learn derivation custom transformer\nArgs: window_length: int, optional Specify savgol filter smoothing window length\npolyorder: int, optional\n    Specify order of the polynom used to interpolate derived signal\n\nderiv: int, optional\n    Specify derivation degree\nReturns: scikit-learn custom transformer\n\nsrc_dir = 'test'\nfnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n          'depth-order-smp.npy', 'target-smp.npy', \n          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\n\ntfm = TakeDerivative()\nplot_spectra(tfm.fit_transform(X), X_names, figsize=(12,4))\n\n\n\n\n\nsource\n\n\nSNV\n\n SNV ()\n\nCreates scikit-learn SNV custom transformer\nArgs: None\nReturns: scikit-learn custom transformer\n\ntfm = SNV()\nplot_spectra(tfm.fit_transform(X), X_names, figsize=(12,4))\n\n\n\n\n\nclass Center(BaseEstimator, TransformerMixin):\n    \"\"\"Creates scikit-learn Centering custom transformer\n\n    Args:\n        None\n\n    Returns:\n        scikit-learn custom transformer\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X - np.mean(X, axis=1).reshape(-1, 1)\n\n\ntfm = Center()\nplot_spectra(tfm.fit_transform(X), X_names, figsize=(12,4))\n\n\n\n\n\nsource\n\n\nDropSpectralRegions\n\n DropSpectralRegions (wavenumbers, regions=[2389, 2269])\n\nCreates scikit-learn custom transformer dropping specific spectral region(s)\nArgs: wavenumbers: list List of wavenumbers where absorbance measured\nregions: list\n    List of region(s) to drop\nReturns: scikit-learn custom transformer\n\ntfm = DropSpectralRegions(X_names, regions=CO2_REGION)\nplot_spectra(tfm.fit_transform(X), X_names, figsize=(12,4))"
  },
  {
    "objectID": "api/data/selection.html",
    "href": "api/data/selection.html",
    "title": "Selection",
    "section": "",
    "text": "source\n\nselect_y\n\n select_y (data:tuple, low:float=0.12, high:float=999)\n\nSelect data based on the limit values of the target\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\ntuple\n\n(X, y, X_id, depth_order)\n\n\nlow\nfloat\n0.12\nLowest limit\n\n\nhigh\nfloat\n999\nHighest limit\n\n\n\n\nsource\n\n\nselect_tax_order\n\n select_tax_order (data:tuple, tax_order:int=None)\n\nSelect data based on Soil Taxonomy order\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\ntuple\n\n(X, y, X_id, depth_order)\n\n\ntax_order\nint\nNone\nValue between 0 and 12\n\n\n\n\nsource\n\n\nselect_X\n\n select_X (data:tuple, low:int=0)\n\nSelect data based on the limit values (only low) of the features\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\ntuple\n\n(X, y, X_id, depth_order)\n\n\nlow\nint\n0\nLowest absorbance value\n\n\n\n\nsource\n\n\nget_y_by_order\n\n get_y_by_order (y, tax_order, tax_lookup)"
  },
  {
    "objectID": "api/data/loading.html",
    "href": "api/data/loading.html",
    "title": "Loading",
    "section": "",
    "text": "source\n\nload_kssl\n\n load_kssl (src_dir:str, fnames:List[str]=['spectra-features.npy',\n            'spectra-wavenumbers.npy', 'depth-order.npy', 'target.npy',\n            'tax-order-lu.pkl', 'spectra-id.npy'],\n            loaders_lut:dict={'.npy': <function load at 0x7f2f96962550>,\n            '.pkl': <built-in function load>})\n\nFunction loading USDA KSSL dataset focusing here on Exchangeable Potassium (analyte_id=725).\nReturns: A tuple (X, X_names, depth_order, y, tax) with: X: spectra (numpy.ndarray) X_names: spectra wavenumbers (numpy.ndarray) depth_order: depth and order of samples (numpy.ndarray) y: exchangeable potassium content (numpy.ndarray) tax_lookup: look up table order_id -> order_name (Dictionary) X_id: unique id of spectra\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\nfolder path containing data\n\n\nfnames\ntyping.List[str]\n[‘spectra-features.npy’, ‘spectra-wavenumbers.npy’, ‘depth-order.npy’, ‘target.npy’, ‘tax-order-lu.pkl’, ‘spectra-id.npy’]\nfilenames to open (in order)\n\n\nloaders_lut\ndict\n{‘.npy’: <function load at 0x7f2f96962550>, ‘.pkl’: }\nloaders lookup table\n\n\n\nLoads in one call all required data: the Mid-Infrared spectra (the features), associated exchangeable potassium wet chemistry (the target) and additional data such as wavenumbers name, soil depth and others.\nFor instance to open a subsample of the dataset (see setup to download the full dataset):\n\nsrc_dir = 'test'\nfnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n          'depth-order-smp.npy', 'target-smp.npy', \n          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (100, 1764)\ny shape: (100,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[ 0.  1.]\n [19.  4.]\n [43. 12.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}\n\n\n\ntest_eq(X.shape, (100, 1764))\ntest_eq(y.shape, (100,))\ntest_eq(len(X_names), 1764)\ntest_eq(depth_order.shape, (100,2))\ntest_eq(len(tax_lookup), 13)"
  },
  {
    "objectID": "api/data/torch.html",
    "href": "api/data/torch.html",
    "title": "PyTorch data loaders and transforms",
    "section": "",
    "text": "source\n\n\n\n SpectralDataset (X, y, tax_order, transform=None)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\nsource\n\n\n\n\n DataLoaders (*args, transform=None, batch_size=32)\n\nConvert numpy error to Pytorch data loaders (generators) Args: *args: one or many tuple as ((X_train, y_train, tax_order), (X_test, y_test, tax_order)) transform: callable class (class)\nReturns: (training_generator, validation_generator)"
  },
  {
    "objectID": "api/data/torch.html#transforms",
    "href": "api/data/torch.html#transforms",
    "title": "PyTorch data loaders and transforms",
    "section": "Transforms",
    "text": "Transforms\n\nsource\n\nSNV_transform\n\n SNV_transform ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nNoop\n\n Noop ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "api/data/torch.html#example-of-use",
    "href": "api/data/torch.html#example-of-use",
    "title": "PyTorch data loaders and transforms",
    "section": "Example of use",
    "text": "Example of use\n\nLoad and preprocess data\n\nsrc_dir = 'test'\nfnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n          'depth-order-smp.npy', 'target-smp.npy', \n          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\n\ndata = X, y, X_id, depth_order\nX, y, X_id, depth_order = compose(*transforms)(data)\n\n\n\nTrain/test split\n\ndata = train_test_split(X, y, depth_order[:, 1], test_size=0.1, random_state=42)\nX_train, X_test, y_train, y_test, tax_order_train, tax_order_test = data\n\n\ndata = train_test_split(X_train, y_train, tax_order_train, test_size=0.1, random_state=42)\nX_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = data\n\n\n\nCreate the generators\n\ndls = DataLoaders((X_train, y_train, tax_order_train), \n                  (X_valid, y_valid, tax_order_valid), \n                  (X_test, y_test, tax_order_test), transform=SNV_transform())\n\ntraining_generator, validation_generator, test_generator = dls.loaders()\n\n\n\nIterate over data (features, targets) mini batches\n\nfor features, target, tax in training_generator:\n    print(f'Batch of features (spectra): {features.shape}')\n    print(f'Batch of targets: {target.shape}')\n    print(f'Batch of Soil taxonomy orders id: {tax.shape}')\n\nBatch of features (spectra): torch.Size([32, 1, 1764])\nBatch of targets: torch.Size([32, 1])\nBatch of Soil taxonomy orders id: torch.Size([32, 1])\nBatch of features (spectra): torch.Size([31, 1, 1764])\nBatch of targets: torch.Size([31, 1])\nBatch of Soil taxonomy orders id: torch.Size([31, 1])\n\n\n\nfor features, target, _ in validation_generator:\n    print(f'Batch of features (spectra): {features.shape}')\n    print(f'Batch of targets: {target.shape}')\n\nBatch of features (spectra): torch.Size([8, 1, 1764])\nBatch of targets: torch.Size([8, 1])\n\n\n\nfor features, target, _ in test_generator:\n    print(f'Batch of features (spectra): {features.shape}')\n    print(f'Batch of targets: {target.shape}')\n\nBatch of features (spectra): torch.Size([8, 1, 1764])\nBatch of targets: torch.Size([8, 1])"
  },
  {
    "objectID": "api/vis/core.html",
    "href": "api/vis/core.html",
    "title": "Visualization",
    "section": "",
    "text": "source\n\n\n\n set_style (style:dict)\n\n\n\n\n\nType\nDetails\n\n\n\n\nstyle\ndict\nDictionary of plt.rcParams"
  },
  {
    "objectID": "api/vis/core.html#eda",
    "href": "api/vis/core.html#eda",
    "title": "Visualization",
    "section": "EDA",
    "text": "EDA\n\nsource\n\nplot_spectra\n\n plot_spectra (X:numpy.ndarray, X_names:numpy.ndarray, figsize=(18, 5),\n               sample=20)\n\nPlot Mid-infrared spectra\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\nSpectra (n_samples, n_wavenumbers)\n\n\nX_names\nndarray\n\nWavenumbers (n_wavenumbers)\n\n\nfigsize\ntuple\n(18, 5)\nWavenumbers\n\n\nsample\nint\n20\nSize of random subset\n\n\n\n\nsource\n\n\nsummary_plot\n\n summary_plot (y:numpy.ndarray, depth_order:numpy.ndarray,\n               tax_lookup:dict)\n\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget variable (n_samples)\n\n\ndepth_order\nndarray\nSoil and Depth (n_samples, 2)\n\n\ntax_lookup\ndict\n{‘alfisols’: 0,‘mollisols’: 1, …}\n\n\n\nTo see an example of use, see Paper with code / 1. Exploratory Data Analysis"
  },
  {
    "objectID": "api/vis/core.html#validation-curves",
    "href": "api/vis/core.html#validation-curves",
    "title": "Visualization",
    "section": "Validation curves",
    "text": "Validation curves\n\nsource\n\nplot_validation_curve\n\n plot_validation_curve (x, losses, ax=None, plot_kwargs={},\n                        fill_between_kwargs={})"
  },
  {
    "objectID": "api/vis/core.html#learning-curves",
    "href": "api/vis/core.html#learning-curves",
    "title": "Visualization",
    "section": "Learning curves",
    "text": "Learning curves\n\nsource\n\nplot_learning_curve\n\n plot_learning_curve (x, losses_train, losses_valid, ax=None,\n                      train_kwargs={}, valid_kwargs={})\n\n\nsource\n\n\nplot_capacity\n\n plot_capacity (x, capacity, ax=None, **kwargs)"
  },
  {
    "objectID": "api/training/metrics.html",
    "href": "api/training/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nrpd\n\n rpd (y:numpy.ndarray, y_hat:numpy.ndarray)\n\nRatio of Performance to Deviation\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget true value\n\n\ny_hat\nndarray\nTarget predicted value\n\n\n\n\ny = np.array([1, 2, 3, 4])\ny_hat = np.array([2, 3, 4, 5])\nis_close(rpd(y, y_hat), 1.29, eps=0.001)\n\nTrue\n\n\n\nsource\n\n\nrpiq\n\n rpiq (y:numpy.ndarray, y_hat:numpy.ndarray)\n\nRatio of Performance to Inter-Quartile\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget true value\n\n\ny_hat\nndarray\nTarget predicted value\n\n\n\n\ny = np.array([1, 2, 3, 4])\ny_hat = np.array([2, 3, 4, 5])\nis_close(rpiq(y, y_hat), 1.5)\n\nTrue\n\n\n\nsource\n\n\nstb\n\n stb (y:numpy.ndarray, y_hat:numpy.ndarray)\n\nStandardized Bias\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget true value\n\n\ny_hat\nndarray\nTarget predicted value\n\n\n\n\ny = np.array([1, 2, 3, 4])\ny_hat = np.array([2, 3, 4, 5])\nis_close(stb(y, y_hat), -0.666, eps=0.001)\n\nTrue\n\n\n\nsource\n\n\nmape\n\n mape (y:numpy.ndarray, y_hat:numpy.ndarray)\n\nMean Absolute Percentage Error\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget true value\n\n\ny_hat\nndarray\nTarget predicted value\n\n\n\n\ny = np.array([1, 2, 3, 4])\ny_hat = np.array([2, 3, 4, 5])\nis_close(mape(y, y_hat), 52.083, eps=0.001)\n\nTrue\n\n\n\nsource\n\n\nlccc\n\n lccc (y:numpy.ndarray, y_hat:numpy.ndarray)\n\nLin’s concordance correlation coefficient\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nTarget true value\n\n\ny_hat\nndarray\nTarget predicted value\n\n\n\n\ny = np.array([1, 2, 3, 4])\ny_hat = np.array([2, 3, 4, 5])\nis_close(lccc(y, y_hat), 0.714, eps=0.001)\n\nTrue\n\n\n\nsource\n\n\neval_reg\n\n eval_reg (y:numpy.ndarray, y_hat:numpy.ndarray, is_log:bool=True)\n\nReturn metrics bundle (rpd, rpiq, r2, lccc, rmse, mse, mae, mape, bias, stb)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nTarget true value\n\n\ny_hat\nndarray\n\nTarget predicted value\n\n\nis_log\nbool\nTrue\nTrue if evaluated values are log-10 transformed\n\n\n\n\neval_reg(y, y_hat)\n\n{'rpd': 1.2909944487358056,\n 'rpiq': 1.5,\n 'r2': 0.19999999999999996,\n 'lccc': 0.7142857142857144,\n 'rmse': 45226.70146053103,\n 'mse': 2045454525.0,\n 'mae': 24997.5,\n 'mape': 900.0,\n 'bias': -1.0,\n 'stb': -0.6666666666666666}"
  },
  {
    "objectID": "api/training/plsr.html",
    "href": "api/training/plsr.html",
    "title": "Training & validation (PLSR)",
    "section": "",
    "text": "source\n\n\n\n Learner (data, model)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n PLS_model (X_names, pipeline_kwargs={})\n\nPartial Least Squares model runner\n\nsource\n\n\n\n\n Learners (tax_lookup, seeds=range(0, 20), split_ratio=0.1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nsrc_dir = 'test'\nfnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n          'depth-order-smp.npy', 'target-smp.npy', \n          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\n\ndest_dir_model = Path('test/dumps-test/plsr/train_eval/all/models')\nseeds = range(2)\nlearners = Learners(tax_lookup, seeds=seeds)\nlearners.train((X, y, depth_order[:, -1]), \n               n_cpts_range=range(40, 70, 2),\n               delta=2e-3,\n               dest_dir_model=dest_dir_model)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n# of components chosen: 44\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n# of components chosen: 48"
  },
  {
    "objectID": "api/training/cnn.html",
    "href": "api/training/cnn.html",
    "title": "Training & validation (CNN)",
    "section": "",
    "text": "if 'google.colab' in str(get_ipython()):\n    from google.colab import drive\n    drive.mount('/content/drive',  force_remount=False)\n    !pip install mirzai\nelse:\nsource"
  },
  {
    "objectID": "api/training/cnn.html#how-to-use-the-model-learner-and-learners",
    "href": "api/training/cnn.html#how-to-use-the-model-learner-and-learners",
    "title": "Training & validation (CNN)",
    "section": "How to use the Model, Learner and Learners?",
    "text": "How to use the Model, Learner and Learners?\n\n1. Load data\n\nsrc_dir = 'test'\nfnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n          'depth-order-smp.npy', 'target-smp.npy', \n          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\n# Or real data\n#src_dir = '../_data'\n#fnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n#          'depth-order.npy', 'target.npy', \n#          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\n\ndata = X, y, X_id, depth_order\nX, y, X_id, depth_order = compose(*transforms)(data)\nprint(X.shape)\n\n(79, 1764)\n\n\n\n\n2. Configure\n\n# Is a GPU available?\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(f'Runtime is: {device}')\n\nparams_scheduler = {\n    'base_lr': 3e-5,\n    'max_lr': 1e-3,\n    'step_size_up': 5,\n    'mode': 'triangular',\n    'cycle_momentum': False\n}\n\nn_epochs = 21\nseeds = range(2)\n\nRuntime is: cpu\n\n\n\n\n3. Train\n\n# Replace following Paths with yours\ndest_dir_loss = Path('test/dumps-test/cnn/train_eval/all/losses')\ndest_dir_model = Path('test/dumps-test/cnn/train_eval/all/models')\n\n\nlearners = Learners(Model, tax_lookup, seeds=seeds, split_ratio=0.1, device=device)\nlearners.train((X, y, depth_order[:, -1]), \n               dest_dir_loss=dest_dir_loss,\n               dest_dir_model=dest_dir_model,\n               n_epochs=n_epochs,\n               sc_kwargs=params_scheduler)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2095176726579666 | Validation loss: 0.28754910826683044\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 1\nTraining loss: 0.20893988013267517 | Validation loss: 0.2862740755081177\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 2\nTraining loss: 0.20712845027446747 | Validation loss: 0.28408676385879517\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 3\nTraining loss: 0.20162831246852875 | Validation loss: 0.2809036374092102\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 4\nTraining loss: 0.1960785835981369 | Validation loss: 0.2767537534236908\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 5\nTraining loss: 0.18827631324529648 | Validation loss: 0.27165573835372925\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 6\nTraining loss: 0.18386302888393402 | Validation loss: 0.26757434010505676\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 7\nTraining loss: 0.17783822119235992 | Validation loss: 0.26450157165527344\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 8\nTraining loss: 0.17546673864126205 | Validation loss: 0.26242998242378235\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 9\nTraining loss: 0.17405737936496735 | Validation loss: 0.26132825016975403\nValidation loss (ends of cycles): [0.28754911]\n------------------------------\nEpoch: 10\nTraining loss: 0.1731555312871933 | Validation loss: 0.2611456513404846\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 11\nTraining loss: 0.171500563621521 | Validation loss: 0.26005008816719055\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 12\nTraining loss: 0.17028532177209854 | Validation loss: 0.2580767273902893\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 13\nTraining loss: 0.16719596087932587 | Validation loss: 0.2550870180130005\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 14\nTraining loss: 0.16363266110420227 | Validation loss: 0.25126326084136963\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 15\nTraining loss: 0.15928955376148224 | Validation loss: 0.24669373035430908\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 16\nTraining loss: 0.1538691222667694 | Validation loss: 0.24314354360103607\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 17\nTraining loss: 0.1488434001803398 | Validation loss: 0.24059247970581055\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 18\nTraining loss: 0.14692240208387375 | Validation loss: 0.2389926314353943\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 19\nTraining loss: 0.14454391598701477 | Validation loss: 0.238305926322937\nValidation loss (ends of cycles): [0.28754911 0.26114565]\n------------------------------\nEpoch: 20\nTraining loss: 0.14511099457740784 | Validation loss: 0.23853451013565063\nValidation loss (ends of cycles): [0.28754911 0.26114565 0.23853451]\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.25518064200878143 | Validation loss: 0.20745772123336792\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 1\nTraining loss: 0.2529909759759903 | Validation loss: 0.20660457015037537\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 2\nTraining loss: 0.25137291848659515 | Validation loss: 0.20521025359630585\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 3\nTraining loss: 0.2424357831478119 | Validation loss: 0.20297007262706757\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 4\nTraining loss: 0.23724160343408585 | Validation loss: 0.2000746876001358\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 5\nTraining loss: 0.22914429008960724 | Validation loss: 0.19651295244693756\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 6\nTraining loss: 0.22360220551490784 | Validation loss: 0.1936747431755066\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 7\nTraining loss: 0.21922896057367325 | Validation loss: 0.19150234758853912\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 8\nTraining loss: 0.2143341824412346 | Validation loss: 0.19003605842590332\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 9\nTraining loss: 0.2121415138244629 | Validation loss: 0.18925482034683228\nValidation loss (ends of cycles): [0.20745772]\n------------------------------\nEpoch: 10\nTraining loss: 0.2118835374712944 | Validation loss: 0.18918269872665405\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.21091558784246445 | Validation loss: 0.18842312693595886\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.2112690731883049 | Validation loss: 0.1870431900024414\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.20744766294956207 | Validation loss: 0.1849963366985321\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.20407014340162277 | Validation loss: 0.18240754306316376\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.19959653913974762 | Validation loss: 0.1791967898607254\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.19397148489952087 | Validation loss: 0.17683245241641998\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.1890103444457054 | Validation loss: 0.17504656314849854\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.18793968856334686 | Validation loss: 0.1737639605998993\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.18513494729995728 | Validation loss: 0.17320512235164642\nValidation loss (ends of cycles): [0.20745772 0.1891827 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.18279500305652618 | Validation loss: 0.17318619787693024\nValidation loss (ends of cycles): [0.20745772 0.1891827  0.1731862 ]\n\n\n\n\n4. Evaluate\n\n# Replace following Paths with yours\nsrc_dir_model = Path('test/dumps-test/cnn/train_eval/all/models')\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_global_all, y_hats_all, y_trues_all, ns_all = learners.evaluate((X, y, depth_order[:, -1]),\n                                                                      src_dir_model=src_dir_model)\n\n\nlearners.evaluate((X, y, depth_order[:, -1]), src_dir_model=src_dir_model)\n\n(Empty DataFrame\n Columns: []\n Index: [],\n Empty DataFrame\n Columns: []\n Index: [],\n Empty DataFrame\n Columns: []\n Index: [],\n Empty DataFrame\n Columns: []\n Index: [])\n\n\n\n\n5. Learning rate finder\n\nsplit_ratio = 0.1\n\n# Train/test split\nX_train, X_test, y_train, y_test, tax_order_train, tax_order_test = train_test_split(X, \n                                                                                     y, \n                                                                                     depth_order[:,1], \n                                                                                     test_size=split_ratio,\n                                                                                     random_state=42)\n\n# Further train/valid split\nX_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = train_test_split(X_train, \n                                                                                      y_train,\n                                                                                      tax_order_train, \n                                                                                      test_size=split_ratio, \n                                                                                      random_state=42)\n\n\ndls = DataLoaders((X_train, y_train, tax_order_train), \n                  (X_valid, y_valid, tax_order_valid),\n                  (X_test, y_test, tax_order_test), \n                  transform=SNV_transform())\n\ntraining_generator, validation_generator, test_generator = dls.loaders()\n\n\nn_epochs = 2\nstep_size_up = 5\ncriterion = MSELoss() # Mean Squared Error loss\nbase_lr, max_lr = 3e-5, 1e-3 # Based on learning rate finder\n\n\n## LR finder\nmodel = Model(X.shape[1], out_channel=16).to(device)\n\nopt = Adam(model.parameters(), lr=1e-4)\nmodel = model.apply(weights_init)\n\nscheduler = CyclicLR(opt, base_lr=base_lr, max_lr=max_lr,\n                     step_size_up=step_size_up, mode='triangular',\n                     cycle_momentum=False)\n\nlearner = Learner(model, criterion, opt, n_epochs=n_epochs, \n                  scheduler=scheduler, early_stopper=None,\n                  tax_lookup=tax_lookup.values(), verbose=True)\n\nlrs, losses = learner.lr_finder(training_generator, end=0.1, n_epochs=8)\n\nEpoch: 0\nEpoch: 1\nEpoch: 2\nEpoch: 3\nEpoch: 4\nEpoch: 5\nEpoch: 6\nEpoch: 7\n\n\n\npd.DataFrame({'learning_rate': lrs, 'loss': losses}).plot(x='learning_rate', y='loss', logx=True);"
  },
  {
    "objectID": "api/training/core.html",
    "href": "api/training/core.html",
    "title": "Core",
    "section": "",
    "text": "source\n\nis_plateau\n\n is_plateau (array:numpy.ndarray, w_size:int=3, delta:float=0.01,\n             verbose:bool=True)\n\nDetect if a plateau is reached when array diffs between last w_size below delta\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\narray\nnp.ndarray\n\n1D array to test\n\n\nw_size\nint\n3\nLast elements to consider\n\n\ndelta\nfloat\n0.01\nThreshold indicating plateau\n\n\nverbose\nbool\nTrue\nDisplay last w_size array’s elements\n\n\n\nFor instance:\n\nloss = np.array([2.3, 2, 1, 0.5])\nprint(f'Array (loss): {loss}')\nprint(f'Has reached a plateau? {is_plateau(loss, delta=0.01)}')\npd.DataFrame(loss).plot();\n\nArray (loss): [2.3 2.  1.  0.5]\nLast pairs diff: [0.3 1.  0.5]\nHas reached a plateau? False\n\n\n\n\n\n\nloss = np.array([2.3, 2, 1, 0.5, 0.4])\nprint(f'Array (loss): {loss}')\nprint(f'Has reached a plateau? {is_plateau(loss, delta=0.01)}')\npd.DataFrame(loss).plot();\n\nArray (loss): [2.3 2.  1.  0.5 0.4]\nLast pairs diff: [1.  0.5 0.1]\nHas reached a plateau? False\n\n\n\n\n\n\nloss = np.array([2.3, 2, 1, 0.5, 0.4, 0.38, 0.37])\nprint(f'Array (loss): {loss}')\nprint(f'Has reached a plateau? {is_plateau(loss, delta=0.015)}')\npd.DataFrame(loss).plot();\n\nArray (loss): [2.3  2.   1.   0.5  0.4  0.38 0.37]\nLast pairs diff: [0.1  0.02 0.01]\nHas reached a plateau? False\n\n\n\n\n\n\nloss = np.array([2.3, 2, 1, 0.5, 0.4, 0.38, 0.37, 0.369, 0.369])\nprint(f'Array (loss): {loss}')\nprint(f'Has reached a plateau? {is_plateau(loss, delta=0.015)}')\npd.DataFrame(loss).plot();\n\nArray (loss): [2.3   2.    1.    0.5   0.4   0.38  0.37  0.369 0.369]\nLast pairs diff: [0.01  0.001 0.   ]\nHas reached a plateau? True\n\n\n\n\n\n\nsource\n\n\nload_dumps\n\n load_dumps (src_dir)\n\nLoad all .pickle file in specified directory\n\ndumps = load_dumps(Path('test/lc'))\n\n\ntest_eq(len(dumps), 2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mirzai",
    "section": "",
    "text": "Prediction of Exchangeable Potassium in Soil through Mid-Infrared Spectroscopy and Deep Learning: from Prediction to Explainability, Albinet et al., 2022\nThe mirzai Python Package, the present documentation and associated notebooks ensure the reproducibility of the above-mentioned scientific paper."
  },
  {
    "objectID": "index.html#paper-with-code",
    "href": "index.html#paper-with-code",
    "title": "Mirzai",
    "section": "Paper with code",
    "text": "Paper with code\n\nExploratory Data Analysis (Fig. 1)\nData selection and transformation\nBaseline model (PLSR):\n\nLearning curve\nTraining & evaluation\n\nConvolutional Neural Network (CNN):\n\nLearning rate finder\nLearning curve\nTraining & evaluation\nValidation curve by Soil Taxonomy Orders (Fig. 5)\n\nPLSR vs. CNN figures:\n\nLearning curves (Fig. 3)\nObserved vs. predicted scatterplots (Fig. 4)\nGlobal vs. local modelling (Fig. 6)\n\nInterpretability\n\nGradientShap values (Fig. 7))\nGradientShap values correlation (Fig. 8)"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Mirzai",
    "section": "Setup",
    "text": "Setup\n\nGetting the data\nA zipped archive of the data used in this study are available for download at the following link: https://drive.google.com/drive/folders/1VGfrBexMPCFvoUa1VW26n-zO9v5WHeFh?usp=sharing\n\n\nIn a local environment\nThe preferred way it to use Mamba. Mamba is a fast, robust, and cross-platform package manager.\nTo install the required dependency and proper Python version:\n\nClone git clone git@github.com:franckalbinet/mirzai.git or download the https://github.com/franckalbinet/mirzai into your local environement\nIn mirzai/ root folder, execute the following Mamba command mamba env create -f environment.yml\n\nHere below the content of mirzai/environment.yml file listing required Python version and packages:\nname: mirzai\nchannels:\n  - conda-forge\n  - fastchan\n  - pytorch\ndependencies:\n  - python=3.8\n  - nbdev\n  - jupyterlab\n  - numpy\n  - scipy\n  - matplotlib=3.5.1\n  - scikit-learn\n  - pytorch\n  - torchvision=0.12.0\n  - tqdm\n  - captum\n\nThen activate the Python environement generated: mamba activate mirzai\nAnd finally launch jupyter notebook\n\n\n\nIn Google Colab\nGoogle Colab has been used to perform the experiments described in the above mentioned paper. The main advantage of Colab is to give access to a GPU (Graphical Processing Unit) which allows to train Deep Learning model in a fair amount of time. Please refer to Colab FAQ for further information.\nEach notebook listed above includes a  link to load it in the Google Colab environment. When clicking on those links, the notebook will get loaded in Google Colab.\nThen, once open in Colab, you will need:\n\nTo mount Google drive to access the data uploaded\nTo install the mirzai Python package\n\nThese two steps are already included on top of each notebook and will be executed if on Colab:\nif 'google.colab' in str(get_ipython()):\n    from google.colab import drive\n    drive.mount('/content/drive',  force_remount=False)\n    !pip install mirzai\nTo locate the Google Drive folder where you uploaded the data, follow the 3 steps shown below:\n\nClick the “Files” icon in the Colab left panel then click on “MyDrive”\nNavigate to the directory containing the data then click on the ” 3 vertical dots” icon\nClick on “Copy path” to copy the full path (for instance /content/drive/MyDrive/research/predict-k-mirs-dl/data)\n\n\nLast, when a GPU is required (e.g when training the Convolutional Neural Network or computing the GradientShap values), change runtime type in Colab top menu: Runtime ▶ Change runtime type ▶ and select GPU in the “Hardware accelerator” select box."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Mirzai",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was carried out in the context of the IAEA funded Coordinated Research Project (CRP D1.50.19) titled “Remediation of Radioactive Contaminated Agricultural Land”, under IAEA Technical Contract n°23685.\nWe also thank Richard Ferguson from Kellogg Soil Survey Laboratory for providing access to the USDA MIR soil spectra library and the r equired training sessions for its operation."
  },
  {
    "objectID": "index.html#others",
    "href": "index.html#others",
    "title": "Mirzai",
    "section": "Others",
    "text": "Others\nThe name mirzai comes from Mid-InfraRed Spectroscopy with AI but also is a way to pay tribute to Nino Ferrer’s song “Mirza”"
  },
  {
    "objectID": "paper/figures_learning_curves.html#utilities",
    "href": "paper/figures_learning_curves.html#utilities",
    "title": "5.1. PLSR vs. CNN Learning Curves",
    "section": "Utilities",
    "text": "Utilities\n\ndef reduce(dumps):\n    df = pd.concat([pd.DataFrame(perf) for perf in dumps])\n    return df.groupby(['n_samples']).agg({'test_score':['mean','std']})"
  },
  {
    "objectID": "paper/figures_learning_curves.html#input-data",
    "href": "paper/figures_learning_curves.html#input-data",
    "title": "5.1. PLSR vs. CNN Learning Curves",
    "section": "Input data",
    "text": "Input data\nTo generate the learning curves for both the PLSR and CNN models, run the following notebooks: * PLSR Learning curve * CNN Learning curve\nInstead, we load already generated and saved data: history_pls_learning_curve.pickle and history_cnn_learning_curve.pickle.\n\nPLSR\n\nsrc_dir = Path('dumps/plsr/learning_curve')\n\n\ndumps = load_dumps(src_dir)\n\n\ndf_plsr = reduce(dumps); df_plsr\n\n\n\n\n\n  \n    \n      \n      test_score\n    \n    \n      \n      mean\n      std\n    \n    \n      n_samples\n      \n      \n    \n  \n  \n    \n      500\n      0.357241\n      0.168292\n    \n    \n      1000\n      0.446311\n      0.064118\n    \n    \n      2000\n      0.484049\n      0.066392\n    \n    \n      5000\n      0.563827\n      0.078549\n    \n    \n      10000\n      0.619452\n      0.030407\n    \n    \n      20000\n      0.634474\n      0.016305\n    \n    \n      30000\n      0.633108\n      0.013739\n    \n    \n      40132\n      0.639170\n      0.012540\n    \n  \n\n\n\n\n\n\nCNN\n\nsrc_dir = Path('dumps/cnn/learning_curve')\n\n\ndumps = load_dumps(src_dir)\n\n\ndf_cnn = reduce(dumps); df_cnn.head()\n\n\n\n\n\n  \n    \n      \n      test_score\n    \n    \n      \n      mean\n      std\n    \n    \n      n_samples\n      \n      \n    \n  \n  \n    \n      500\n      0.419518\n      0.198391\n    \n    \n      1000\n      0.453100\n      0.086281\n    \n    \n      2000\n      0.533940\n      0.054055\n    \n    \n      5000\n      0.622144\n      0.034591\n    \n    \n      10000\n      0.679383\n      0.024025\n    \n  \n\n\n\n\n\nhistory_plsr, history_cnn = [{'nb_samples': df.index.to_numpy(),\n                              'r2_mean': df[('test_score', 'mean')].to_numpy(),\n                              'r2_std': df[('test_score', 'std')].to_numpy()\n                             }\n                             for df in [df_plsr, df_cnn]]\n\n\nhistory_plsr\n\n{'nb_samples': array([  500,  1000,  2000,  5000, 10000, 20000, 30000, 40132]),\n 'r2_mean': array([0.35724121, 0.44631125, 0.48404897, 0.56382695, 0.61945173,\n        0.63447414, 0.63310825, 0.63917001]),\n 'r2_std': array([0.16829222, 0.06411818, 0.06639178, 0.0785487 , 0.03040741,\n        0.01630533, 0.01373926, 0.0125405 ])}\n\n\n\nhistory_cnn\n\n{'nb_samples': array([  500,  1000,  2000,  5000, 10000, 20000, 30000, 40132]),\n 'r2_mean': array([0.41951778, 0.45309992, 0.53394023, 0.6221441 , 0.67938348,\n        0.72648183, 0.75480461, 0.77354623]),\n 'r2_std': array([0.19839095, 0.08628061, 0.05405458, 0.03459126, 0.02402495,\n        0.02137746, 0.02162855, 0.01649216])}"
  },
  {
    "objectID": "paper/figures_learning_curves.html#plot",
    "href": "paper/figures_learning_curves.html#plot",
    "title": "5.1. PLSR vs. CNN Learning Curves",
    "section": "Plot",
    "text": "Plot\n\ndef plot_learning_curve(x, losses_train, losses_valid, ax=None,  train_kwargs={}, valid_kwargs={}):\n    if ax is None:\n        ax = plt.gca()\n    ax.plot(x, losses_train, label='Training', **train_kwargs) \n    #ax.plot(x, losses_valid, label='Validation', **valid_kwargs) \n    ax.set_yscale('log')\n    ax.set_xscale('log')\n    return(ax)\n\n\ndef plot_learning_curves(history_pls, history_cnn,\n                         figsize=(10*centimeter,8*centimeter), dpi=600):\n    # Layout \n    fig = plt.figure(figsize=figsize, dpi=600)\n\n    gs = GridSpec(nrows=1, ncols=1)\n    ax = fig.add_subplot(gs[0, 0])\n    # Plots\n    params = {'marker': 'o', 'mfc':'w', 'ms':3}\n    x, mean, std = history_plsr.values()\n    fill_between_params = {'facecolor': 'C0', 'alpha': 0.15, 'zorder': 1}\n    ax.fill_between(x, mean + std, mean - std, **fill_between_params)\n    ax.plot(x, mean, label='PLSR', c='C0', **params)\n    \n    x, mean, std = history_cnn.values()\n    fill_between_params = {'facecolor': 'C1', 'alpha': 0.15, 'zorder': 1}\n    ax.fill_between(x, mean + std, mean - std, **fill_between_params)\n    ax.plot(x, mean, label='CNN', c='C1', **params)\n    \n    ax.set_xscale('log')\n    \n    # Ornaments\n    ax.legend(loc='best', frameon=False) \n    ax.set_ylabel('$R^2$ →', loc='top')\n\n    ax.set_xlabel('Data size →', loc='right')\n    ax.grid(True, \"minor\", color=\"0.85\", linewidth=0.2, zorder=-20)\n    ax.grid(True, \"major\", color=\"0.65\", linewidth=0.4, zorder=-10) \n\n    plt.tight_layout()\n\n\nFIG_PATH = Path('images/')\nset_style(DEFAULT_STYLE)\nplot_learning_curves(history_plsr, history_cnn)\n\n# To save/export it\nplt.savefig(FIG_PATH/'learning-curves.png', dpi=600, transparent=True, format='png')"
  },
  {
    "objectID": "paper/cnn_lr_finder.html#load-and-transform",
    "href": "paper/cnn_lr_finder.html#load-and-transform",
    "title": "4.1 CNN learning rate finder",
    "section": "1. Load and transform",
    "text": "1. Load and transform\n\nLoad data\n\nsrc_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\n\ndata = X, y, X_id, depth_order\nX, y, X_id, depth_order = compose(*transforms)(data)\nprint(X.shape)\n\n(40132, 1764)\n\n\n\n\nCreate data loaders\n\nsplit_ratio = 0.1\n\n# Train/test split\nX_train, X_test, y_train, y_test, tax_order_train, tax_order_test = train_test_split(X, \n                                                                                     y, \n                                                                                     depth_order[:,1], \n                                                                                     test_size=split_ratio,\n                                                                                     random_state=42)\n\n# Further train/valid split\nX_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = train_test_split(X_train, \n                                                                                      y_train,\n                                                                                      tax_order_train, \n                                                                                      test_size=split_ratio, \n                                                                                      random_state=42)\n\n\ndls = DataLoaders((X_train, y_train, tax_order_train), \n                  (X_valid, y_valid, tax_order_valid),\n                  (X_test, y_test, tax_order_test), \n                  transform=SNV_transform())\ntraining_generator, validation_generator, test_generator = dls.loaders()"
  },
  {
    "objectID": "paper/cnn_lr_finder.html#setup",
    "href": "paper/cnn_lr_finder.html#setup",
    "title": "4.1 CNN learning rate finder",
    "section": "2. Setup",
    "text": "2. Setup\n\n# Is a GPU available?\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(f'Runtime is: {device}')\n\nn_epochs = 20\nstep_size_up = 5\ncriterion = MSELoss() # Mean Squared Error loss\n\nRuntime is: cuda:0\n\n\n\n4. Train\n\n## LR finder\nmodel = Model(X.shape[1], out_channel=16).to(device)\n\nopt = Adam(model.parameters(), lr=1e-4)\nmodel = model.apply(weights_init)\n\nlearner = Learner(model, criterion, opt, n_epochs=n_epochs, \n                  scheduler=None, early_stopper=None,\n                  tax_lookup=tax_lookup.values(), verbose=True)\n\nlrs, losses = learner.lr_finder(training_generator, end=0.1, n_epochs=8)\n\nEpoch: 0\nEpoch: 1\nEpoch: 2\nEpoch: 3\nEpoch: 4\nEpoch: 5\nEpoch: 6\nEpoch: 7\n\n\n\n\n5. Save & load\n\ndest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/lr_finder')\nwith open(dest_dir/f'cnn-lr.pickle', 'wb') as f: \n    pickle.dump([lrs, losses], f)\n\n\nsrc_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/lr_finder')\nwith open(src_dir/f'cnn-lr.pickle', 'rb') as f: \n    lrs, losses = pickle.load(f)\n\n\npd.DataFrame({'learning_rate': lrs, 'loss': losses}).plot(x='learning_rate', y='loss', logx=True);"
  },
  {
    "objectID": "paper/cnn_train_eval.html#load-and-transform",
    "href": "paper/cnn_train_eval.html#load-and-transform",
    "title": "4.3. Train & evaluate (CNN)",
    "section": "1. Load and transform",
    "text": "1. Load and transform\n\nLoad data\n\n# For testing purpose\n#src_dir = 'test'\n#fnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n#          'depth-order-smp.npy', 'target-smp.npy', \n#          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n\n\n# or with all data\nsrc_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\ndata = X, y, X_id, depth_order\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)"
  },
  {
    "objectID": "paper/cnn_train_eval.html#experiment",
    "href": "paper/cnn_train_eval.html#experiment",
    "title": "4.3. Train & evaluate (CNN)",
    "section": "Experiment",
    "text": "Experiment\n\nSetup\n\n# Is a GPU available?\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(f'Runtime is: {device}')\n\nparams_scheduler = {\n    'base_lr': 3e-5,\n    'max_lr': 1e-3,\n    'step_size_up': 5,\n    'mode': 'triangular',\n    'cycle_momentum': False\n}\n\nn_epochs = 201\nseeds = range(20)\nseeds = range(15, 20)\n\nRuntime is: cuda:0\n\n\n\n\nTrain on all Soil Taxonomic Orders\n\n# Replace following Paths with yours\ndest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/losses')\ndest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nlearners.train((X, y, depth_order[:, -1]), \n               dest_dir_loss=dest_dir_loss,\n               dest_dir_model=dest_dir_model,\n               n_epochs=n_epochs,\n               sc_kwargs=params_scheduler)\n\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.21006273476802928 | Validation loss: 0.16685934806555774\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 1\nTraining loss: 0.10366790779631614 | Validation loss: 0.12193218717532875\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 2\nTraining loss: 0.08515885396031883 | Validation loss: 0.07348759699843627\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 3\nTraining loss: 0.07681338573754655 | Validation loss: 0.0775093322174739\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 4\nTraining loss: 0.07012089212630385 | Validation loss: 0.06804058407392122\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 5\nTraining loss: 0.06663906943111673 | Validation loss: 0.06383055649631847\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 6\nTraining loss: 0.06138754077439057 | Validation loss: 0.05653325315004429\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 7\nTraining loss: 0.057238537138034624 | Validation loss: 0.05207979377458053\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 8\nTraining loss: 0.053663239335846595 | Validation loss: 0.04987848757774429\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 9\nTraining loss: 0.05063598560660667 | Validation loss: 0.04656141813415869\nValidation loss (ends of cycles): [0.16685935]\n------------------------------\nEpoch: 10\nTraining loss: 0.048641224010429515 | Validation loss: 0.045020436414772956\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 11\nTraining loss: 0.04944960257005474 | Validation loss: 0.04620409672832595\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 12\nTraining loss: 0.05068350452599739 | Validation loss: 0.0466461240322189\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 13\nTraining loss: 0.05165666111700941 | Validation loss: 0.048602522936015004\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 14\nTraining loss: 0.05240861789184058 | Validation loss: 0.04709551842734877\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 15\nTraining loss: 0.053400266255567395 | Validation loss: 0.05373546291571275\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 16\nTraining loss: 0.050950179472345654 | Validation loss: 0.046860707412778806\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 17\nTraining loss: 0.04810693795888091 | Validation loss: 0.049740820958287316\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 18\nTraining loss: 0.045981014085068245 | Validation loss: 0.04217774676472212\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 19\nTraining loss: 0.04369535342254859 | Validation loss: 0.04023268423249236\nValidation loss (ends of cycles): [0.16685935 0.04502044]\n------------------------------\nEpoch: 20\nTraining loss: 0.04157303913503768 | Validation loss: 0.03929079252303438\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 21\nTraining loss: 0.04292749061604859 | Validation loss: 0.03935050310781308\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 22\nTraining loss: 0.04409183335387859 | Validation loss: 0.04125337862302508\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 23\nTraining loss: 0.04542167893222233 | Validation loss: 0.04289548397393881\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 24\nTraining loss: 0.04644744235868236 | Validation loss: 0.04298555405101681\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 25\nTraining loss: 0.04782074668232029 | Validation loss: 0.04539380502779927\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 26\nTraining loss: 0.04569561580951348 | Validation loss: 0.04396933422679395\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 27\nTraining loss: 0.04414120492503399 | Validation loss: 0.04126848547463923\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 28\nTraining loss: 0.041882850408488076 | Validation loss: 0.03920497682637873\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 29\nTraining loss: 0.03984382633563483 | Validation loss: 0.03720222598156043\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n------------------------------\nEpoch: 30\nTraining loss: 0.03824256880274849 | Validation loss: 0.03624860502252009\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 31\nTraining loss: 0.039079163438002544 | Validation loss: 0.036830263261773945\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 32\nTraining loss: 0.04015510225774881 | Validation loss: 0.037671556770471876\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 33\nTraining loss: 0.04150084875373712 | Validation loss: 0.040750166566102905\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 34\nTraining loss: 0.0429485932956853 | Validation loss: 0.04076049813127096\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 35\nTraining loss: 0.044326993446122474 | Validation loss: 0.04463050882043564\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 36\nTraining loss: 0.042592402374349886 | Validation loss: 0.04191957023489264\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 37\nTraining loss: 0.04073864440012019 | Validation loss: 0.039296620451243575\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 38\nTraining loss: 0.0388192206421339 | Validation loss: 0.040280445091492305\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 39\nTraining loss: 0.03695870364381836 | Validation loss: 0.03545188218915621\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n------------------------------\nEpoch: 40\nTraining loss: 0.03567687908414839 | Validation loss: 0.03444945969643582\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 41\nTraining loss: 0.03621109440926404 | Validation loss: 0.03518754746600063\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 42\nTraining loss: 0.03740448744116923 | Validation loss: 0.035378930587486354\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 43\nTraining loss: 0.038526631041987267 | Validation loss: 0.037030073467938775\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 44\nTraining loss: 0.04034058869666031 | Validation loss: 0.03999876431230687\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 45\nTraining loss: 0.041215690155330255 | Validation loss: 0.041170561809785074\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 46\nTraining loss: 0.040143450241770566 | Validation loss: 0.03838985987765863\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 47\nTraining loss: 0.03800150607052574 | Validation loss: 0.04004549053786075\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 48\nTraining loss: 0.03671635882598971 | Validation loss: 0.03537545478449459\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 49\nTraining loss: 0.0347628424257612 | Validation loss: 0.034168427686446005\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n------------------------------\nEpoch: 50\nTraining loss: 0.03355114189009586 | Validation loss: 0.03320952699379583\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 51\nTraining loss: 0.034235434844085255 | Validation loss: 0.033966143739170736\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 52\nTraining loss: 0.03527097588709343 | Validation loss: 0.03463689532889202\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 53\nTraining loss: 0.036384562783925906 | Validation loss: 0.043450314378514224\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 54\nTraining loss: 0.037796340584864946 | Validation loss: 0.037529862032527415\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 55\nTraining loss: 0.040052222718638696 | Validation loss: 0.037125598194193\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 56\nTraining loss: 0.037771799428366476 | Validation loss: 0.03799022023722661\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 57\nTraining loss: 0.03619785493654089 | Validation loss: 0.037243562148867454\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 58\nTraining loss: 0.03466686871229106 | Validation loss: 0.036433838901266585\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 59\nTraining loss: 0.03298729962181652 | Validation loss: 0.03261748232375995\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n------------------------------\nEpoch: 60\nTraining loss: 0.0318513594633775 | Validation loss: 0.03203852672492508\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 61\nTraining loss: 0.03245821607750144 | Validation loss: 0.03227031828456484\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 62\nTraining loss: 0.033735571010221586 | Validation loss: 0.036995998938131124\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 63\nTraining loss: 0.03462631959838455 | Validation loss: 0.03523477534475052\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 64\nTraining loss: 0.03623737948744991 | Validation loss: 0.03887613358355201\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 65\nTraining loss: 0.037692032123761855 | Validation loss: 0.0393562819314214\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 66\nTraining loss: 0.03629782138663659 | Validation loss: 0.03982825133850617\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 67\nTraining loss: 0.034753578830525045 | Validation loss: 0.035794618621930084\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 68\nTraining loss: 0.03335795502130705 | Validation loss: 0.03471514085653873\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 69\nTraining loss: 0.031571629029097346 | Validation loss: 0.03225022736541201\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853]\n------------------------------\nEpoch: 70\nTraining loss: 0.03022792704007405 | Validation loss: 0.03138879530824128\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 71\nTraining loss: 0.03081579728044687 | Validation loss: 0.0321810659581581\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 72\nTraining loss: 0.031975348947540394 | Validation loss: 0.03330272281196265\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 73\nTraining loss: 0.03338813357143186 | Validation loss: 0.03498803695614359\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 74\nTraining loss: 0.03510964766335417 | Validation loss: 0.03675926781663325\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 75\nTraining loss: 0.03621827504931267 | Validation loss: 0.04031967806868848\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 76\nTraining loss: 0.03502747318872405 | Validation loss: 0.03644606344138099\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 77\nTraining loss: 0.033388999948923394 | Validation loss: 0.03493262910638499\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 78\nTraining loss: 0.031588033860087336 | Validation loss: 0.03346493154500438\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 79\nTraining loss: 0.030269897492477802 | Validation loss: 0.031828868301766636\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888 ]\n------------------------------\nEpoch: 80\nTraining loss: 0.029260817702536978 | Validation loss: 0.030966343737281528\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 81\nTraining loss: 0.02962339904496637 | Validation loss: 0.03175661460686047\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 82\nTraining loss: 0.030601339510964654 | Validation loss: 0.03278937654784032\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 83\nTraining loss: 0.03195925210107587 | Validation loss: 0.034539599556772584\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 84\nTraining loss: 0.03342047210885432 | Validation loss: 0.03554835073254277\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 85\nTraining loss: 0.03485352619034128 | Validation loss: 0.03597379817866382\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 86\nTraining loss: 0.03383251438632343 | Validation loss: 0.03613145378278156\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 87\nTraining loss: 0.032122112861027806 | Validation loss: 0.03380987030841344\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 88\nTraining loss: 0.030471733455018208 | Validation loss: 0.03271121671835406\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 89\nTraining loss: 0.029372684216662125 | Validation loss: 0.031361686634476735\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634]\n------------------------------\nEpoch: 90\nTraining loss: 0.028345677224317873 | Validation loss: 0.03078324074931113\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 91\nTraining loss: 0.02855107771859717 | Validation loss: 0.03187606106223786\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 92\nTraining loss: 0.029404923101652443 | Validation loss: 0.03323598527473159\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 93\nTraining loss: 0.03089467578340234 | Validation loss: 0.03677618085711667\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 94\nTraining loss: 0.032298622542889566 | Validation loss: 0.03654344159846021\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 95\nTraining loss: 0.033855747049271065 | Validation loss: 0.03389050771144375\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 96\nTraining loss: 0.032686932765013416 | Validation loss: 0.03563051058424521\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 97\nTraining loss: 0.03117338846085637 | Validation loss: 0.036191234661810165\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 98\nTraining loss: 0.029448336268973162 | Validation loss: 0.03330194152298227\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 99\nTraining loss: 0.028292122928565002 | Validation loss: 0.03124773993560698\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324]\n------------------------------\nEpoch: 100\nTraining loss: 0.027255275193235184 | Validation loss: 0.030370980245679354\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 101\nTraining loss: 0.027862145766197874 | Validation loss: 0.031165265132038468\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 102\nTraining loss: 0.028608309474451043 | Validation loss: 0.03325484803490407\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 103\nTraining loss: 0.02942115707344955 | Validation loss: 0.03360139277755423\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 104\nTraining loss: 0.031027071798535606 | Validation loss: 0.035984445478668255\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 105\nTraining loss: 0.03268752752080941 | Validation loss: 0.04319226613218805\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 106\nTraining loss: 0.03163052153018281 | Validation loss: 0.033679972509894754\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 107\nTraining loss: 0.030197662804605747 | Validation loss: 0.03366826350215526\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 108\nTraining loss: 0.028440817642414313 | Validation loss: 0.03247853203684883\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 109\nTraining loss: 0.02726938468074118 | Validation loss: 0.031231511301830807\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n------------------------------\nEpoch: 110\nTraining loss: 0.026660842424089923 | Validation loss: 0.030472978850645302\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 111\nTraining loss: 0.026930124313739225 | Validation loss: 0.030931837073799255\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 112\nTraining loss: 0.027782215082313953 | Validation loss: 0.03232833159576475\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 113\nTraining loss: 0.02897368960804856 | Validation loss: 0.03295310706196897\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 114\nTraining loss: 0.030728193830449398 | Validation loss: 0.034547940753729994\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 115\nTraining loss: 0.03180026859100028 | Validation loss: 0.03586612556096727\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 116\nTraining loss: 0.030403358757305217 | Validation loss: 0.036287696990885035\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 117\nTraining loss: 0.029338854772962747 | Validation loss: 0.033448813400701084\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 118\nTraining loss: 0.027786362097657277 | Validation loss: 0.03349754812640954\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 119\nTraining loss: 0.02651610360645843 | Validation loss: 0.03075202837982009\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n------------------------------\nEpoch: 120\nTraining loss: 0.025863564623644444 | Validation loss: 0.030081888875075145\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 121\nTraining loss: 0.025841036838054013 | Validation loss: 0.030718148438737984\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 122\nTraining loss: 0.026887093637666597 | Validation loss: 0.03189418640449247\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 123\nTraining loss: 0.0282036214269419 | Validation loss: 0.033052585845961505\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 124\nTraining loss: 0.02943545867190293 | Validation loss: 0.03484091803068872\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 125\nTraining loss: 0.031106999646285622 | Validation loss: 0.03587077220127118\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 126\nTraining loss: 0.029798714455041127 | Validation loss: 0.03478194975945274\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 127\nTraining loss: 0.028319378263375713 | Validation loss: 0.03545742583320995\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 128\nTraining loss: 0.02707512585783568 | Validation loss: 0.031950104305833844\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 129\nTraining loss: 0.025894583061733468 | Validation loss: 0.03058983964960923\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189]\n------------------------------\nEpoch: 130\nTraining loss: 0.02535024630070818 | Validation loss: 0.030067410971145188\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 131\nTraining loss: 0.025085657799715454 | Validation loss: 0.03055939608278264\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 132\nTraining loss: 0.026115530444580447 | Validation loss: 0.03141266045745759\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 133\nTraining loss: 0.027371244338216392 | Validation loss: 0.03283754703217903\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 134\nTraining loss: 0.028676321570061176 | Validation loss: 0.0335505915624378\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 135\nTraining loss: 0.030171366977710716 | Validation loss: 0.036135951370264575\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 136\nTraining loss: 0.028916694708791834 | Validation loss: 0.03220553534616411\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 137\nTraining loss: 0.02758560871511082 | Validation loss: 0.034407987646692624\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 138\nTraining loss: 0.026476486200580417 | Validation loss: 0.032068598825913085\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 139\nTraining loss: 0.025373654041299962 | Validation loss: 0.03088633716930594\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741]\n------------------------------\nEpoch: 140\nTraining loss: 0.02446029111412977 | Validation loss: 0.029658473261623783\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 141\nTraining loss: 0.024613056748951454 | Validation loss: 0.030765600189302876\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 142\nTraining loss: 0.02563061149273889 | Validation loss: 0.031055306257531706\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 143\nTraining loss: 0.026793520601546963 | Validation loss: 0.03368187746016589\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 144\nTraining loss: 0.028121095472455435 | Validation loss: 0.034668188031136464\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 145\nTraining loss: 0.02954825781392095 | Validation loss: 0.036907955427217275\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 146\nTraining loss: 0.02867951834992337 | Validation loss: 0.03384090841343972\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 147\nTraining loss: 0.027033594139030306 | Validation loss: 0.03186083252055455\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 148\nTraining loss: 0.02586826024298358 | Validation loss: 0.03187882715621881\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 149\nTraining loss: 0.02469652157055038 | Validation loss: 0.030002151368663903\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847]\n------------------------------\nEpoch: 150\nTraining loss: 0.024060468244417682 | Validation loss: 0.02966953616226669\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 151\nTraining loss: 0.02407615681684862 | Validation loss: 0.030171342457817718\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 152\nTraining loss: 0.025213620124942087 | Validation loss: 0.03115276776386046\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 153\nTraining loss: 0.02618761574271993 | Validation loss: 0.03224841546498041\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 154\nTraining loss: 0.02742909563532994 | Validation loss: 0.033071257239949386\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 155\nTraining loss: 0.028960566331566438 | Validation loss: 0.03520545495294892\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 156\nTraining loss: 0.027878794489578734 | Validation loss: 0.03331690206629249\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 157\nTraining loss: 0.026770222055777088 | Validation loss: 0.03253524844427552\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 158\nTraining loss: 0.025255439297926768 | Validation loss: 0.031415996751624396\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 159\nTraining loss: 0.023955525235478274 | Validation loss: 0.030671831624236253\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954]\n------------------------------\nEpoch: 160\nTraining loss: 0.023950381831936977 | Validation loss: 0.02982574759001753\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 161\nTraining loss: 0.023566104945594694 | Validation loss: 0.030525477438242035\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 162\nTraining loss: 0.02436839387015124 | Validation loss: 0.03077274331806508\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 163\nTraining loss: 0.025670240620356492 | Validation loss: 0.03336958942390912\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 164\nTraining loss: 0.026950350978162403 | Validation loss: 0.03207134384563012\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 165\nTraining loss: 0.02825807590371247 | Validation loss: 0.03577499762507139\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 166\nTraining loss: 0.027466312799727412 | Validation loss: 0.034949956346402127\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 167\nTraining loss: 0.025565710374315483 | Validation loss: 0.03352450120336978\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 168\nTraining loss: 0.024536826447987065 | Validation loss: 0.030833024449184933\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 169\nTraining loss: 0.023544609255164745 | Validation loss: 0.030002048099001425\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n------------------------------\nEpoch: 170\nTraining loss: 0.02321960965692117 | Validation loss: 0.02953936348347801\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 171\nTraining loss: 0.02307436841917701 | Validation loss: 0.030252535151630903\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 172\nTraining loss: 0.02397868466677188 | Validation loss: 0.031158533524227355\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 173\nTraining loss: 0.025020400550781096 | Validation loss: 0.032842995162097224\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 174\nTraining loss: 0.026263095045693248 | Validation loss: 0.033730272541597356\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 175\nTraining loss: 0.02800633968862404 | Validation loss: 0.03604198408733427\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 176\nTraining loss: 0.02651089855550429 | Validation loss: 0.03513341484410045\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 177\nTraining loss: 0.02560398147412114 | Validation loss: 0.0321963859095642\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 178\nTraining loss: 0.02422986511053063 | Validation loss: 0.03157056310106959\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 179\nTraining loss: 0.023086803899271282 | Validation loss: 0.030515983684268673\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n------------------------------\nEpoch: 180\nTraining loss: 0.022762703376558176 | Validation loss: 0.02935167956701686\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 181\nTraining loss: 0.022699793172343892 | Validation loss: 0.030329733221602123\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 182\nTraining loss: 0.023518949007344116 | Validation loss: 0.030502196112894907\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 183\nTraining loss: 0.024388733594935007 | Validation loss: 0.03174259513616562\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 184\nTraining loss: 0.025955424753135056 | Validation loss: 0.03257236281096672\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 185\nTraining loss: 0.027296889776833297 | Validation loss: 0.03400973833899582\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 186\nTraining loss: 0.026026336777786627 | Validation loss: 0.03210558142282267\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 187\nTraining loss: 0.025013451092311834 | Validation loss: 0.03260836849170449\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 188\nTraining loss: 0.02365047061141199 | Validation loss: 0.03127120152544395\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 189\nTraining loss: 0.022748118945705432 | Validation loss: 0.030008847141160373\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168]\n------------------------------\nEpoch: 190\nTraining loss: 0.022331331235972623 | Validation loss: 0.029065387826248082\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 191\nTraining loss: 0.022117540770114994 | Validation loss: 0.029946955860452314\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 192\nTraining loss: 0.02293831821417392 | Validation loss: 0.030338736745504153\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 193\nTraining loss: 0.024056396900408526 | Validation loss: 0.03252197858935173\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 194\nTraining loss: 0.02526241676986408 | Validation loss: 0.032178846088988064\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 195\nTraining loss: 0.026862020887559148 | Validation loss: 0.0375595488214651\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 196\nTraining loss: 0.025642202743585772 | Validation loss: 0.03363440044850639\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 197\nTraining loss: 0.024161111101907804 | Validation loss: 0.032072389045054404\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 198\nTraining loss: 0.02327819517813623 | Validation loss: 0.030471804131448798\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 199\nTraining loss: 0.022183483882667863 | Validation loss: 0.02956509131905252\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539]\n------------------------------\nEpoch: 200\nTraining loss: 0.022008079754951313 | Validation loss: 0.029146107779074032\nValidation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n 0.02935168 0.02906539 0.02914611]\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2737787661369978 | Validation loss: 0.20023219089592453\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 1\nTraining loss: 0.10422682451216254 | Validation loss: 0.09987394712799419\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 2\nTraining loss: 0.08268818595468647 | Validation loss: 0.07145754689663912\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 3\nTraining loss: 0.07512161015041584 | Validation loss: 0.06633957702897292\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 4\nTraining loss: 0.07036189057893581 | Validation loss: 0.06764761305752054\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 5\nTraining loss: 0.06704740458366087 | Validation loss: 0.06609862109860487\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 6\nTraining loss: 0.06204092073965612 | Validation loss: 0.05567961194412371\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 7\nTraining loss: 0.05805403113504505 | Validation loss: 0.05421012262525284\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 8\nTraining loss: 0.054467795774208634 | Validation loss: 0.05056867451794379\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 9\nTraining loss: 0.05140738620965441 | Validation loss: 0.04763612818731144\nValidation loss (ends of cycles): [0.20023219]\n------------------------------\nEpoch: 10\nTraining loss: 0.049082192368658746 | Validation loss: 0.04559225025298321\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 11\nTraining loss: 0.04975575218380495 | Validation loss: 0.046683357748310124\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 12\nTraining loss: 0.05101741386364823 | Validation loss: 0.047293581431154655\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 13\nTraining loss: 0.05192214983339444 | Validation loss: 0.0476417343206374\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 14\nTraining loss: 0.053264485360453216 | Validation loss: 0.05344951939068537\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 15\nTraining loss: 0.05388570320975827 | Validation loss: 0.05068364296009583\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 16\nTraining loss: 0.051310861563882024 | Validation loss: 0.04789036223912133\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 17\nTraining loss: 0.04877522815290222 | Validation loss: 0.051986201707504495\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 18\nTraining loss: 0.04632410562347885 | Validation loss: 0.043588970103754406\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 19\nTraining loss: 0.04381964350573894 | Validation loss: 0.042146987932841336\nValidation loss (ends of cycles): [0.20023219 0.04559225]\n------------------------------\nEpoch: 20\nTraining loss: 0.04200194686500898 | Validation loss: 0.03990887502719343\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 21\nTraining loss: 0.04261076600862828 | Validation loss: 0.041349465662069554\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 22\nTraining loss: 0.044018229102994515 | Validation loss: 0.044328473145719124\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 23\nTraining loss: 0.04533218726970431 | Validation loss: 0.04635683298770305\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 24\nTraining loss: 0.04664624337194942 | Validation loss: 0.05748243245864864\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 25\nTraining loss: 0.04778335881456146 | Validation loss: 0.04696861471552237\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 26\nTraining loss: 0.045971616663544726 | Validation loss: 0.04174737498757586\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 27\nTraining loss: 0.04344583072655607 | Validation loss: 0.04394390559301967\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 28\nTraining loss: 0.04164245962000298 | Validation loss: 0.041117376507779135\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 29\nTraining loss: 0.0392682946013932 | Validation loss: 0.03982357207718676\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n------------------------------\nEpoch: 30\nTraining loss: 0.037741293180110595 | Validation loss: 0.036664327384030394\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 31\nTraining loss: 0.038508784161572614 | Validation loss: 0.03875069599598646\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 32\nTraining loss: 0.03946622626353642 | Validation loss: 0.03949111793190241\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 33\nTraining loss: 0.04122776931929072 | Validation loss: 0.043168582302411045\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 34\nTraining loss: 0.04259670557505561 | Validation loss: 0.042725293262474304\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 35\nTraining loss: 0.04403580553411675 | Validation loss: 0.04463229184986743\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 36\nTraining loss: 0.04248637171447512 | Validation loss: 0.040492692583166394\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 37\nTraining loss: 0.040044852228116566 | Validation loss: 0.038837947511831214\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 38\nTraining loss: 0.0383974302965253 | Validation loss: 0.03937832881754215\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 39\nTraining loss: 0.03638932610473295 | Validation loss: 0.03644921438587186\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n------------------------------\nEpoch: 40\nTraining loss: 0.03480823439413931 | Validation loss: 0.03473438378588288\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 41\nTraining loss: 0.03557032718246172 | Validation loss: 0.036001175196956746\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 42\nTraining loss: 0.036576151781840115 | Validation loss: 0.037461821863477206\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 43\nTraining loss: 0.038131684358195056 | Validation loss: 0.03910830907062092\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 44\nTraining loss: 0.03957182635905177 | Validation loss: 0.04135864841199554\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 45\nTraining loss: 0.04120923336925293 | Validation loss: 0.04687123353370523\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 46\nTraining loss: 0.03937091541890876 | Validation loss: 0.04366569883659878\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 47\nTraining loss: 0.03759949713465061 | Validation loss: 0.03793287259913915\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 48\nTraining loss: 0.035793748596228483 | Validation loss: 0.036690053769049394\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 49\nTraining loss: 0.034185784380318315 | Validation loss: 0.0347253294348453\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n------------------------------\nEpoch: 50\nTraining loss: 0.03280941256061636 | Validation loss: 0.033420166544682155\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 51\nTraining loss: 0.03306701497768792 | Validation loss: 0.03450813103236456\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 52\nTraining loss: 0.03430615315089134 | Validation loss: 0.03600950290209952\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 53\nTraining loss: 0.03589855856720475 | Validation loss: 0.037268991961218086\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 54\nTraining loss: 0.037301842755762725 | Validation loss: 0.04395043955440015\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 55\nTraining loss: 0.038821042116592076 | Validation loss: 0.043963905704100574\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 56\nTraining loss: 0.03712832113071601 | Validation loss: 0.040459541430845194\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 57\nTraining loss: 0.035522612095920475 | Validation loss: 0.038693558564821706\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 58\nTraining loss: 0.03354250567352883 | Validation loss: 0.03509865964935944\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 59\nTraining loss: 0.03192239735774168 | Validation loss: 0.03395114307481367\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n------------------------------\nEpoch: 60\nTraining loss: 0.030853263973295103 | Validation loss: 0.03248945271711698\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 61\nTraining loss: 0.03124623723130116 | Validation loss: 0.03348119809926875\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 62\nTraining loss: 0.032311980573159794 | Validation loss: 0.035267103158465\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 63\nTraining loss: 0.033898110482218406 | Validation loss: 0.03626691218696337\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 64\nTraining loss: 0.03541915445023869 | Validation loss: 0.03732870159819063\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 65\nTraining loss: 0.03680497418546418 | Validation loss: 0.04455761301570234\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 66\nTraining loss: 0.03531477025592714 | Validation loss: 0.0376130314873515\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 67\nTraining loss: 0.033475336373057656 | Validation loss: 0.03557608191715141\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 68\nTraining loss: 0.03201586188103153 | Validation loss: 0.03565459449536505\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 69\nTraining loss: 0.030342980738047247 | Validation loss: 0.033357611354605285\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945]\n------------------------------\nEpoch: 70\nTraining loss: 0.029202395655933445 | Validation loss: 0.03172996544600588\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 71\nTraining loss: 0.029675779442530213 | Validation loss: 0.03339542720499819\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 72\nTraining loss: 0.030540388210412613 | Validation loss: 0.03487979933059057\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 73\nTraining loss: 0.03213733445218669 | Validation loss: 0.03967391237187966\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 74\nTraining loss: 0.03372008166198919 | Validation loss: 0.04293370014468653\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 75\nTraining loss: 0.03520092460983176 | Validation loss: 0.04376732475594082\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 76\nTraining loss: 0.033938281122580405 | Validation loss: 0.03703245200810179\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 77\nTraining loss: 0.032296309855355934 | Validation loss: 0.03492208946067675\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 78\nTraining loss: 0.030508256989428554 | Validation loss: 0.035185562314844765\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 79\nTraining loss: 0.02909078923380369 | Validation loss: 0.03294597795249614\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997]\n------------------------------\nEpoch: 80\nTraining loss: 0.02803419754514104 | Validation loss: 0.031490433136973764\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 81\nTraining loss: 0.028328925535251483 | Validation loss: 0.032534939574852456\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 82\nTraining loss: 0.02926808329002096 | Validation loss: 0.03435256992326637\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 83\nTraining loss: 0.03089131600513145 | Validation loss: 0.035728449348063594\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 84\nTraining loss: 0.032449012219825185 | Validation loss: 0.037415493403322404\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 85\nTraining loss: 0.03403688801679848 | Validation loss: 0.04564265021642225\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 86\nTraining loss: 0.03234556234463697 | Validation loss: 0.041697077766324565\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 87\nTraining loss: 0.03099816353665298 | Validation loss: 0.03541089097384067\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 88\nTraining loss: 0.029276077776751296 | Validation loss: 0.03348477607576457\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 89\nTraining loss: 0.027866539251110628 | Validation loss: 0.032401630176907094\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043]\n------------------------------\nEpoch: 90\nTraining loss: 0.026741057555521214 | Validation loss: 0.030844259166480165\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 91\nTraining loss: 0.02716905342902636 | Validation loss: 0.03190479325377835\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 92\nTraining loss: 0.02814049190863615 | Validation loss: 0.03378076414552938\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 93\nTraining loss: 0.02952301814257512 | Validation loss: 0.034746238747529225\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 94\nTraining loss: 0.031075982355850008 | Validation loss: 0.03731691609837313\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 95\nTraining loss: 0.03271141126256905 | Validation loss: 0.060955908380251014\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 96\nTraining loss: 0.03134089423614869 | Validation loss: 0.03622718771045978\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 97\nTraining loss: 0.029479217560646514 | Validation loss: 0.033980404232851175\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 98\nTraining loss: 0.028381742420606315 | Validation loss: 0.03344887570865386\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 99\nTraining loss: 0.02674665158214854 | Validation loss: 0.03224219867250824\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426]\n------------------------------\nEpoch: 100\nTraining loss: 0.0260751176383493 | Validation loss: 0.030910984786078993\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 101\nTraining loss: 0.02635237188665123 | Validation loss: 0.03226050084242515\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 102\nTraining loss: 0.027261614670251004 | Validation loss: 0.033868492984798104\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 103\nTraining loss: 0.028737410438232358 | Validation loss: 0.03410222200033939\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 104\nTraining loss: 0.03006675193823258 | Validation loss: 0.056256531152577526\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 105\nTraining loss: 0.03191802492016912 | Validation loss: 0.048147553182413094\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 106\nTraining loss: 0.030185575500739314 | Validation loss: 0.03620794531683215\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 107\nTraining loss: 0.029039505783901146 | Validation loss: 0.03526321702958208\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 108\nTraining loss: 0.027365574273134134 | Validation loss: 0.03291359628690819\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 109\nTraining loss: 0.026011773122027224 | Validation loss: 0.03202740053555607\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n------------------------------\nEpoch: 110\nTraining loss: 0.02524538153294826 | Validation loss: 0.030587680316406542\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 111\nTraining loss: 0.02533113995959293 | Validation loss: 0.032261734877803684\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 112\nTraining loss: 0.02640439592041515 | Validation loss: 0.03358038018696603\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 113\nTraining loss: 0.027467445776555834 | Validation loss: 0.03410669124429732\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 114\nTraining loss: 0.029203762646808518 | Validation loss: 0.03419871860703008\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 115\nTraining loss: 0.030872044258010316 | Validation loss: 0.035971489265165496\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 116\nTraining loss: 0.029417167569869968 | Validation loss: 0.03570391744605999\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 117\nTraining loss: 0.0279873248426185 | Validation loss: 0.03376021398544575\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 118\nTraining loss: 0.02645933153804421 | Validation loss: 0.032196724564826064\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 119\nTraining loss: 0.025299409457563062 | Validation loss: 0.03130617110569656\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n------------------------------\nEpoch: 120\nTraining loss: 0.024414063379989834 | Validation loss: 0.030250687490060792\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 121\nTraining loss: 0.024591018528747188 | Validation loss: 0.03162372223538371\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 122\nTraining loss: 0.025364326536802092 | Validation loss: 0.03282762849621013\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 123\nTraining loss: 0.026572306790777784 | Validation loss: 0.03422165076058786\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 124\nTraining loss: 0.028306469948702 | Validation loss: 0.035084132006210564\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 125\nTraining loss: 0.030099466125898824 | Validation loss: 0.04039957774061281\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 126\nTraining loss: 0.028633888242546264 | Validation loss: 0.037205871458338424\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 127\nTraining loss: 0.027315277815246442 | Validation loss: 0.03901227530652443\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 128\nTraining loss: 0.025800537531113794 | Validation loss: 0.03346189024108174\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 129\nTraining loss: 0.024668992970021456 | Validation loss: 0.031561887114251085\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069]\n------------------------------\nEpoch: 130\nTraining loss: 0.02404350608699097 | Validation loss: 0.030343229462087683\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 131\nTraining loss: 0.02407458088888721 | Validation loss: 0.031308728204298336\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 132\nTraining loss: 0.024918176032104125 | Validation loss: 0.03360111465825971\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 133\nTraining loss: 0.026058742077104513 | Validation loss: 0.03443116572710265\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 134\nTraining loss: 0.027624107491203004 | Validation loss: 0.03449264597312539\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 135\nTraining loss: 0.02928206292494369 | Validation loss: 0.03776389098338849\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 136\nTraining loss: 0.028131398687681813 | Validation loss: 0.03479092995083965\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 137\nTraining loss: 0.026286100760637598 | Validation loss: 0.0348371911167571\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 138\nTraining loss: 0.0250626021893475 | Validation loss: 0.03248374907514163\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 139\nTraining loss: 0.024081416701252654 | Validation loss: 0.03174565414345897\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323]\n------------------------------\nEpoch: 140\nTraining loss: 0.02352582935544508 | Validation loss: 0.030051779883823036\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 141\nTraining loss: 0.023359468449677127 | Validation loss: 0.0312756019174657\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 142\nTraining loss: 0.02415206788408416 | Validation loss: 0.03251146805365529\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 143\nTraining loss: 0.02550879377490423 | Validation loss: 0.03490573369724825\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 144\nTraining loss: 0.026806120814821557 | Validation loss: 0.0345633335873089\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 145\nTraining loss: 0.0286723116860087 | Validation loss: 0.03890983096892591\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 146\nTraining loss: 0.027140730675713227 | Validation loss: 0.03415699658371442\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 147\nTraining loss: 0.02588838122154432 | Validation loss: 0.03404608176015647\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 148\nTraining loss: 0.024426499718074195 | Validation loss: 0.03193692797053177\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 149\nTraining loss: 0.023166143655314983 | Validation loss: 0.03149128182733481\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178]\n------------------------------\nEpoch: 150\nTraining loss: 0.0229603738123441 | Validation loss: 0.029910476128282272\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 151\nTraining loss: 0.02284284436792927 | Validation loss: 0.030855711153914445\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 152\nTraining loss: 0.02365900123194887 | Validation loss: 0.03222047771632144\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 153\nTraining loss: 0.024800017015003843 | Validation loss: 0.03301934850921409\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 154\nTraining loss: 0.026273147880437043 | Validation loss: 0.03420751300368425\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 155\nTraining loss: 0.027664226583410494 | Validation loss: 0.0349949984110694\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 156\nTraining loss: 0.026663722442444707 | Validation loss: 0.033630230348656136\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 157\nTraining loss: 0.025194934456550525 | Validation loss: 0.03472642444766465\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 158\nTraining loss: 0.023837915450283215 | Validation loss: 0.032156074790498326\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 159\nTraining loss: 0.023005821732950845 | Validation loss: 0.031500728331110645\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048]\n------------------------------\nEpoch: 160\nTraining loss: 0.02235974420061863 | Validation loss: 0.0296877749115888\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 161\nTraining loss: 0.022434936120911818 | Validation loss: 0.030912983049927033\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 162\nTraining loss: 0.02321441728148727 | Validation loss: 0.03159520993487234\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 163\nTraining loss: 0.024087414784378952 | Validation loss: 0.03426515600289655\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 164\nTraining loss: 0.02584073524720468 | Validation loss: 0.03549671795351052\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 165\nTraining loss: 0.027481816246130275 | Validation loss: 0.04569036421258893\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 166\nTraining loss: 0.025911257378610334 | Validation loss: 0.03599856464208755\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 167\nTraining loss: 0.0245359625546335 | Validation loss: 0.03235892685278591\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 168\nTraining loss: 0.02326479062573676 | Validation loss: 0.03178735901560403\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 169\nTraining loss: 0.02218883088652382 | Validation loss: 0.03116621243544912\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n------------------------------\nEpoch: 170\nTraining loss: 0.021824004793609308 | Validation loss: 0.029547290860024174\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 171\nTraining loss: 0.021793495756885637 | Validation loss: 0.0311219496570066\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 172\nTraining loss: 0.022444999180954155 | Validation loss: 0.03314602608273251\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 173\nTraining loss: 0.023874418106018088 | Validation loss: 0.033780165544125884\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 174\nTraining loss: 0.025131154348927102 | Validation loss: 0.03473723620440053\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 175\nTraining loss: 0.0267372918269399 | Validation loss: 0.03518312112881546\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 176\nTraining loss: 0.02542390567012219 | Validation loss: 0.03591320638965189\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 177\nTraining loss: 0.02414658128038254 | Validation loss: 0.03431402006705778\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 178\nTraining loss: 0.02274914544788886 | Validation loss: 0.03193719099910386\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 179\nTraining loss: 0.021763978673577836 | Validation loss: 0.030923344120712935\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n------------------------------\nEpoch: 180\nTraining loss: 0.02152854327009771 | Validation loss: 0.029603901574701334\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 181\nTraining loss: 0.021365951617912014 | Validation loss: 0.03074364638598883\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 182\nTraining loss: 0.021945311958300257 | Validation loss: 0.035094125188095904\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 183\nTraining loss: 0.02318344924848263 | Validation loss: 0.03233091902416364\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 184\nTraining loss: 0.0248239246356543 | Validation loss: 0.03261617678258799\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 185\nTraining loss: 0.026298621777577077 | Validation loss: 0.03375745768271453\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 186\nTraining loss: 0.024853772596514775 | Validation loss: 0.03509717840965079\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 187\nTraining loss: 0.023714199431211225 | Validation loss: 0.03329228342766256\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 188\nTraining loss: 0.022584474363224947 | Validation loss: 0.032880636723299994\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 189\nTraining loss: 0.02153710593188944 | Validation loss: 0.03080118345343961\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039 ]\n------------------------------\nEpoch: 190\nTraining loss: 0.02122366759563085 | Validation loss: 0.029605589848242502\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 191\nTraining loss: 0.020808289958692088 | Validation loss: 0.030839143395094217\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 192\nTraining loss: 0.021608949259407526 | Validation loss: 0.03275987589979066\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 193\nTraining loss: 0.022862843535730513 | Validation loss: 0.03243703589634558\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 194\nTraining loss: 0.024137301308168903 | Validation loss: 0.034653909075840386\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 195\nTraining loss: 0.02555413278989406 | Validation loss: 0.03679713180733729\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 196\nTraining loss: 0.024435840169422504 | Validation loss: 0.03782723306686477\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 197\nTraining loss: 0.02327967907932741 | Validation loss: 0.03321316239910316\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 198\nTraining loss: 0.022098688396815884 | Validation loss: 0.03147691657222741\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 199\nTraining loss: 0.02113150341131937 | Validation loss: 0.03040765568745875\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559]\n------------------------------\nEpoch: 200\nTraining loss: 0.020685331293704413 | Validation loss: 0.029657005612631286\nValidation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n 0.0296039  0.02960559 0.02965701]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.19528153301839987 | Validation loss: 0.15108045474090406\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 1\nTraining loss: 0.09630404248481661 | Validation loss: 0.07499229344250881\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 2\nTraining loss: 0.0770322958220119 | Validation loss: 0.0657210370476267\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 3\nTraining loss: 0.07153833540721144 | Validation loss: 0.06682331098523815\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 4\nTraining loss: 0.06794721161301329 | Validation loss: 0.058392948561669454\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 5\nTraining loss: 0.06519884280823758 | Validation loss: 0.05638599817731739\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 6\nTraining loss: 0.06049888590204082 | Validation loss: 0.062485484785474506\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 7\nTraining loss: 0.056857929563179144 | Validation loss: 0.04876981789361587\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 8\nTraining loss: 0.053296953524044885 | Validation loss: 0.04630405498685035\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 9\nTraining loss: 0.05026454213588667 | Validation loss: 0.04334998397832423\nValidation loss (ends of cycles): [0.15108045]\n------------------------------\nEpoch: 10\nTraining loss: 0.04759277868282607 | Validation loss: 0.04201392273921355\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 11\nTraining loss: 0.04893287182528907 | Validation loss: 0.042212111397390874\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 12\nTraining loss: 0.0500142074952738 | Validation loss: 0.04350175510729309\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 13\nTraining loss: 0.05130318975384076 | Validation loss: 0.04694888262753993\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 14\nTraining loss: 0.05227278390583971 | Validation loss: 0.048200364991099434\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 15\nTraining loss: 0.05314655587678467 | Validation loss: 0.05896381667889325\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 16\nTraining loss: 0.05050081228030714 | Validation loss: 0.04554605564778357\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 17\nTraining loss: 0.048094492702649566 | Validation loss: 0.0431616822735662\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 18\nTraining loss: 0.04548985188584921 | Validation loss: 0.04048775626033281\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 19\nTraining loss: 0.04332584545280286 | Validation loss: 0.03895747074774936\nValidation loss (ends of cycles): [0.15108045 0.04201392]\n------------------------------\nEpoch: 20\nTraining loss: 0.04135821033525831 | Validation loss: 0.037356803407975\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.04213623469237121 | Validation loss: 0.038663210462679905\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.04348462626730334 | Validation loss: 0.04059584674516083\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.044896001835889 | Validation loss: 0.0408192587175728\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.04613425673396275 | Validation loss: 0.040819176052392055\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.04764102466081775 | Validation loss: 0.04785382432813665\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.04551267926581204 | Validation loss: 0.04141203267911894\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.04350365237809542 | Validation loss: 0.038893470408774056\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.041088982838147325 | Validation loss: 0.03846450380783165\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.039170486238367094 | Validation loss: 0.035727420409696294\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.03741839054172609 | Validation loss: 0.03534357691496874\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 31\nTraining loss: 0.038331123063428785 | Validation loss: 0.03712475580056157\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 32\nTraining loss: 0.03958883121752686 | Validation loss: 0.03590556875332794\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 33\nTraining loss: 0.04092563144536322 | Validation loss: 0.03797215149136244\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 34\nTraining loss: 0.042431336150167316 | Validation loss: 0.03862651510048756\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 35\nTraining loss: 0.04385021059612531 | Validation loss: 0.04650221689216859\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 36\nTraining loss: 0.042333089369789176 | Validation loss: 0.04138898033549828\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 37\nTraining loss: 0.040143472958341475 | Validation loss: 0.04083230859080775\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 38\nTraining loss: 0.038212460662810296 | Validation loss: 0.03604175413307627\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 39\nTraining loss: 0.03621550740795137 | Validation loss: 0.03490787063750018\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n------------------------------\nEpoch: 40\nTraining loss: 0.0347418464531418 | Validation loss: 0.033902818650270984\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 41\nTraining loss: 0.03551816663844144 | Validation loss: 0.0339503469918154\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 42\nTraining loss: 0.03664073076623484 | Validation loss: 0.036111429589182405\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 43\nTraining loss: 0.03794130406940106 | Validation loss: 0.038581867809448625\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 44\nTraining loss: 0.039544879462511284 | Validation loss: 0.03753844731017551\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 45\nTraining loss: 0.04120968023817399 | Validation loss: 0.040164445156017234\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 46\nTraining loss: 0.03963290285832417 | Validation loss: 0.03767492653455882\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 47\nTraining loss: 0.037655649592139295 | Validation loss: 0.03940686863739934\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 48\nTraining loss: 0.0360215248523544 | Validation loss: 0.03606964662605155\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 49\nTraining loss: 0.03379709184294435 | Validation loss: 0.03300323327426362\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n------------------------------\nEpoch: 50\nTraining loss: 0.032713453326645624 | Validation loss: 0.032592986331247124\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 51\nTraining loss: 0.03327064899779035 | Validation loss: 0.0332814001074407\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 52\nTraining loss: 0.03456818787784382 | Validation loss: 0.03814794470976412\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 53\nTraining loss: 0.035899267839011186 | Validation loss: 0.03423762608286554\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 54\nTraining loss: 0.03718895989829513 | Validation loss: 0.037633575235320404\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 55\nTraining loss: 0.038761256454241146 | Validation loss: 0.0388596230145313\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 56\nTraining loss: 0.03721329200770852 | Validation loss: 0.03893185608023036\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 57\nTraining loss: 0.035715234900392065 | Validation loss: 0.03628284927556472\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 58\nTraining loss: 0.03370676121645145 | Validation loss: 0.033211706354554775\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 59\nTraining loss: 0.0323047479999611 | Validation loss: 0.03253601488978725\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n------------------------------\nEpoch: 60\nTraining loss: 0.030942986885225034 | Validation loss: 0.03169571242369382\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 61\nTraining loss: 0.03151171489281389 | Validation loss: 0.03195847763754098\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 62\nTraining loss: 0.0326535613743559 | Validation loss: 0.03367336982020498\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 63\nTraining loss: 0.03397060671223577 | Validation loss: 0.0392299928265599\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 64\nTraining loss: 0.03557851326406208 | Validation loss: 0.036114410315928734\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 65\nTraining loss: 0.036995837769258445 | Validation loss: 0.046228273697527106\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 66\nTraining loss: 0.03573583279219346 | Validation loss: 0.035999696652314304\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 67\nTraining loss: 0.03398162927240221 | Validation loss: 0.033870757285472564\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 68\nTraining loss: 0.032125317916919395 | Validation loss: 0.03275708983594601\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 69\nTraining loss: 0.030659903030600545 | Validation loss: 0.030907584725162095\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571]\n------------------------------\nEpoch: 70\nTraining loss: 0.029677720091445006 | Validation loss: 0.030892113479167486\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 71\nTraining loss: 0.030018180326419317 | Validation loss: 0.030626152551411528\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 72\nTraining loss: 0.03106435915953883 | Validation loss: 0.033222900274976166\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 73\nTraining loss: 0.032380191656929534 | Validation loss: 0.03328017588803726\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 74\nTraining loss: 0.03383174836525591 | Validation loss: 0.03539824518745979\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 75\nTraining loss: 0.03549576116777135 | Validation loss: 0.03629769839807949\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 76\nTraining loss: 0.034087463657877695 | Validation loss: 0.03376856591321726\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 77\nTraining loss: 0.03234960998346164 | Validation loss: 0.03851411953172852\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 78\nTraining loss: 0.03068069219369236 | Validation loss: 0.0314846167884833\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 79\nTraining loss: 0.029369526592222608 | Validation loss: 0.030833860068062765\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211]\n------------------------------\nEpoch: 80\nTraining loss: 0.02836777383780591 | Validation loss: 0.030553860598103662\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 81\nTraining loss: 0.028889779986879662 | Validation loss: 0.031743324707896835\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 82\nTraining loss: 0.029690888467406137 | Validation loss: 0.031142891442353746\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 83\nTraining loss: 0.031053277377889852 | Validation loss: 0.03351418073165469\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 84\nTraining loss: 0.03267707209850801 | Validation loss: 0.042135526879435094\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 85\nTraining loss: 0.03411234047458192 | Validation loss: 0.0370925584132165\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 86\nTraining loss: 0.03266451155994175 | Validation loss: 0.03274107290909881\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 87\nTraining loss: 0.031198181013034027 | Validation loss: 0.0340254512013851\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 88\nTraining loss: 0.029595627138706466 | Validation loss: 0.031916605308651924\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 89\nTraining loss: 0.028043366407638225 | Validation loss: 0.030369010976458017\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386]\n------------------------------\nEpoch: 90\nTraining loss: 0.027346802133824823 | Validation loss: 0.030075445589897908\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 91\nTraining loss: 0.027608434050013935 | Validation loss: 0.030171571629105415\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 92\nTraining loss: 0.028327987452338294 | Validation loss: 0.03024076547605538\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 93\nTraining loss: 0.029624219871984107 | Validation loss: 0.03404002234471583\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 94\nTraining loss: 0.03148827917679528 | Validation loss: 0.03371854062167417\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 95\nTraining loss: 0.03307377927529618 | Validation loss: 0.04036632132411531\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 96\nTraining loss: 0.031799388683665046 | Validation loss: 0.03298156123311646\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 97\nTraining loss: 0.03007377838939575 | Validation loss: 0.033205109755549814\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 98\nTraining loss: 0.028580836875650183 | Validation loss: 0.030641627497971058\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 99\nTraining loss: 0.027427082016008106 | Validation loss: 0.0313515687625097\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545]\n------------------------------\nEpoch: 100\nTraining loss: 0.026483976918338672 | Validation loss: 0.030348778084184215\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 101\nTraining loss: 0.026674966022599576 | Validation loss: 0.03149610185023166\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 102\nTraining loss: 0.02751217969953574 | Validation loss: 0.03084385859887157\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 103\nTraining loss: 0.028731230297125876 | Validation loss: 0.031040501080255592\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 104\nTraining loss: 0.030062061833496405 | Validation loss: 0.03604654427123281\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 105\nTraining loss: 0.0318882070439611 | Validation loss: 0.039417957194742906\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 106\nTraining loss: 0.030514564989391334 | Validation loss: 0.03316041232144411\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 107\nTraining loss: 0.028916336958091267 | Validation loss: 0.035398528771062865\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 108\nTraining loss: 0.02764455142969955 | Validation loss: 0.031008046623154553\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 109\nTraining loss: 0.02622884780894525 | Validation loss: 0.031359491927499795\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n------------------------------\nEpoch: 110\nTraining loss: 0.02562284080117325 | Validation loss: 0.030416654530022524\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 111\nTraining loss: 0.02577647100859065 | Validation loss: 0.03090014162811294\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 112\nTraining loss: 0.026568011741894555 | Validation loss: 0.030865917913615704\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 113\nTraining loss: 0.028003387864473768 | Validation loss: 0.03234168109937315\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 114\nTraining loss: 0.029407237495999695 | Validation loss: 0.032332020902396306\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 115\nTraining loss: 0.03110336661650469 | Validation loss: 0.03286148103333152\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 116\nTraining loss: 0.02993302761877733 | Validation loss: 0.035776853462499855\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 117\nTraining loss: 0.02837571760045555 | Validation loss: 0.03131979187966975\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 118\nTraining loss: 0.02665195077214038 | Validation loss: 0.031021781488264028\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 119\nTraining loss: 0.02563444459370858 | Validation loss: 0.030956934082560835\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n------------------------------\nEpoch: 120\nTraining loss: 0.024944440375529522 | Validation loss: 0.030078876598746376\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 121\nTraining loss: 0.02519254589051844 | Validation loss: 0.03037429168259939\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 122\nTraining loss: 0.02574926438213392 | Validation loss: 0.030591894485122336\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 123\nTraining loss: 0.027375541950070012 | Validation loss: 0.03259252623316988\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 124\nTraining loss: 0.028651660226659454 | Validation loss: 0.03243902798709089\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 125\nTraining loss: 0.030220652338278694 | Validation loss: 0.03472108380720679\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 126\nTraining loss: 0.02893120789346762 | Validation loss: 0.0334128510635511\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 127\nTraining loss: 0.02742837137619664 | Validation loss: 0.03295834002340526\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 128\nTraining loss: 0.02586745542719755 | Validation loss: 0.03359922730421598\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 129\nTraining loss: 0.02518866923601022 | Validation loss: 0.030972652326840742\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888]\n------------------------------\nEpoch: 130\nTraining loss: 0.024370399135069585 | Validation loss: 0.029321946928986407\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 131\nTraining loss: 0.024452643489142455 | Validation loss: 0.03170669283223363\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 132\nTraining loss: 0.02514903414646149 | Validation loss: 0.02934865020545183\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 133\nTraining loss: 0.026473972576076355 | Validation loss: 0.03256165103541803\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 134\nTraining loss: 0.027980016625126985 | Validation loss: 0.037934470789886154\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 135\nTraining loss: 0.02950241145545866 | Validation loss: 0.033272428256748\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 136\nTraining loss: 0.02817217055726503 | Validation loss: 0.03544298366569312\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 137\nTraining loss: 0.02676052859971505 | Validation loss: 0.03308902620708784\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 138\nTraining loss: 0.025434287322692456 | Validation loss: 0.0314219071554531\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 139\nTraining loss: 0.024347204041271286 | Validation loss: 0.03068882338208171\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195]\n------------------------------\nEpoch: 140\nTraining loss: 0.023864611201391623 | Validation loss: 0.029601984030970956\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 141\nTraining loss: 0.023795595388918175 | Validation loss: 0.032649262017051206\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 142\nTraining loss: 0.0245595584966345 | Validation loss: 0.03269626537288448\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 143\nTraining loss: 0.025921545380095796 | Validation loss: 0.03559081084136151\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 144\nTraining loss: 0.02733462275735535 | Validation loss: 0.033264496363699436\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 145\nTraining loss: 0.029037256825964634 | Validation loss: 0.0364198326833744\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 146\nTraining loss: 0.027521122688808897 | Validation loss: 0.03114709215222207\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 147\nTraining loss: 0.026036638752777334 | Validation loss: 0.03610302006776354\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 148\nTraining loss: 0.024805288894769302 | Validation loss: 0.030778389053204947\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 149\nTraining loss: 0.023793241487610646 | Validation loss: 0.030905219281207673\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198]\n------------------------------\nEpoch: 150\nTraining loss: 0.02341242203824442 | Validation loss: 0.029345920012719864\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 151\nTraining loss: 0.023260569254919067 | Validation loss: 0.030717732623812898\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 152\nTraining loss: 0.023884564654637626 | Validation loss: 0.03010071575872402\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 153\nTraining loss: 0.025170819309547426 | Validation loss: 0.029870342207759356\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 154\nTraining loss: 0.026484384465463987 | Validation loss: 0.04114546122408546\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 155\nTraining loss: 0.028278595922830125 | Validation loss: 0.037228376563407675\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 156\nTraining loss: 0.026891657869023543 | Validation loss: 0.035947980639417614\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 157\nTraining loss: 0.025646816195707446 | Validation loss: 0.03072655048485087\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 158\nTraining loss: 0.024305647449003254 | Validation loss: 0.03094838262922996\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 159\nTraining loss: 0.0235144635589104 | Validation loss: 0.03033747435011695\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592]\n------------------------------\nEpoch: 160\nTraining loss: 0.02274166181863205 | Validation loss: 0.028855146320981788\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 161\nTraining loss: 0.022850188836418678 | Validation loss: 0.030827936905938966\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 162\nTraining loss: 0.023490403991056914 | Validation loss: 0.03056321562622237\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 163\nTraining loss: 0.024536157205634877 | Validation loss: 0.030726340179026656\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 164\nTraining loss: 0.025968177275233498 | Validation loss: 0.03182310807164264\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 165\nTraining loss: 0.02745464058156587 | Validation loss: 0.03240670689043745\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 166\nTraining loss: 0.02638791023944248 | Validation loss: 0.03207409606867396\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 167\nTraining loss: 0.025040658959004237 | Validation loss: 0.03346892037486608\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 168\nTraining loss: 0.023840111028586373 | Validation loss: 0.03250710043924308\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 169\nTraining loss: 0.022957657927676567 | Validation loss: 0.03261066463866592\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n------------------------------\nEpoch: 170\nTraining loss: 0.02253573403536071 | Validation loss: 0.028775880929181534\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 171\nTraining loss: 0.022440487484990317 | Validation loss: 0.030062347749429466\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 172\nTraining loss: 0.022867488988315848 | Validation loss: 0.03976280561986223\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 173\nTraining loss: 0.02382546374629303 | Validation loss: 0.03142198853491418\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 174\nTraining loss: 0.025262574162004208 | Validation loss: 0.03314230741059358\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 175\nTraining loss: 0.02710123380965106 | Validation loss: 0.03711360927044818\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 176\nTraining loss: 0.025813494960220135 | Validation loss: 0.034733611777161076\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 177\nTraining loss: 0.0244471754999758 | Validation loss: 0.030967152215936017\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 178\nTraining loss: 0.023251139880598 | Validation loss: 0.03004695099275724\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 179\nTraining loss: 0.0224692894349344 | Validation loss: 0.03162622818542001\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n------------------------------\nEpoch: 180\nTraining loss: 0.021959586049738068 | Validation loss: 0.0287618160874179\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 181\nTraining loss: 0.021884380696766723 | Validation loss: 0.03256968013216964\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 182\nTraining loss: 0.02237591364670281 | Validation loss: 0.03341920127416343\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 183\nTraining loss: 0.023713711671179204 | Validation loss: 0.029729280975210454\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 184\nTraining loss: 0.025079944019763194 | Validation loss: 0.03119673509051842\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 185\nTraining loss: 0.02635635260866559 | Validation loss: 0.03225741138170778\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 186\nTraining loss: 0.025441277809771085 | Validation loss: 0.03134904677394481\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 187\nTraining loss: 0.024010141896208616 | Validation loss: 0.031274090043013604\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 188\nTraining loss: 0.023025057314552264 | Validation loss: 0.03273896375193005\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 189\nTraining loss: 0.022124788933800255 | Validation loss: 0.03352637900517578\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182]\n------------------------------\nEpoch: 190\nTraining loss: 0.02186805220626984 | Validation loss: 0.029281317882767294\nValidation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n 0.02876182 0.02928132]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.14745687956987755 | Validation loss: 0.11181736810017476\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 1\nTraining loss: 0.09623743342162352 | Validation loss: 0.08580441937773628\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 2\nTraining loss: 0.08750657575556964 | Validation loss: 0.08224849567740364\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 3\nTraining loss: 0.08265806757294991 | Validation loss: 0.07812433627195063\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 4\nTraining loss: 0.07909638743011618 | Validation loss: 0.10523036336608693\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 5\nTraining loss: 0.07519952726997728 | Validation loss: 0.07433472433646696\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 6\nTraining loss: 0.06963092798191145 | Validation loss: 0.0681168861621249\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 7\nTraining loss: 0.0647309234413487 | Validation loss: 0.061909659196977065\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 8\nTraining loss: 0.0603061709351339 | Validation loss: 0.06635272014985043\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 9\nTraining loss: 0.056970927845980006 | Validation loss: 0.053171325606846176\nValidation loss (ends of cycles): [0.11181737]\n------------------------------\nEpoch: 10\nTraining loss: 0.054451444431934067 | Validation loss: 0.050428496311064316\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.05530370387989353 | Validation loss: 0.05998936325179792\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.0562868577292497 | Validation loss: 0.08638219957330585\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.0572645927277049 | Validation loss: 0.06002994229506075\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.05791082459604588 | Validation loss: 0.07916613048420543\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.058337017147633154 | Validation loss: 0.06136379625021884\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.0556387303206395 | Validation loss: 0.05869993445488204\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.05294741382895727 | Validation loss: 0.05184250018369835\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.05023024163912894 | Validation loss: 0.05074060479162541\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.047944747648517215 | Validation loss: 0.04994288424804675\nValidation loss (ends of cycles): [0.11181737 0.0504285 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.045913187161777315 | Validation loss: 0.04389677417621148\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 21\nTraining loss: 0.047116746218493724 | Validation loss: 0.04913257856943966\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 22\nTraining loss: 0.048278160490403614 | Validation loss: 0.04979463033707796\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 23\nTraining loss: 0.04970042983906006 | Validation loss: 0.05419276432191904\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 24\nTraining loss: 0.05063182357918677 | Validation loss: 0.05380100643146882\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 25\nTraining loss: 0.05186005225991757 | Validation loss: 0.05329234860178116\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 26\nTraining loss: 0.04985330287572436 | Validation loss: 0.054778525721182865\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 27\nTraining loss: 0.047700814294122804 | Validation loss: 0.04927220066959879\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 28\nTraining loss: 0.04578730350316275 | Validation loss: 0.04668996552318598\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 29\nTraining loss: 0.04332482842838494 | Validation loss: 0.04906922478261774\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n------------------------------\nEpoch: 30\nTraining loss: 0.04196088215130873 | Validation loss: 0.04077261508302351\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 31\nTraining loss: 0.042725179914575744 | Validation loss: 0.04192748965810886\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 32\nTraining loss: 0.043890829696723324 | Validation loss: 0.0423654904506639\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 33\nTraining loss: 0.04479648191188499 | Validation loss: 0.045411624986909135\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 34\nTraining loss: 0.046417209704765885 | Validation loss: 0.04752046215982564\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 35\nTraining loss: 0.04789944863548194 | Validation loss: 0.04866404211626644\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 36\nTraining loss: 0.04611892869886686 | Validation loss: 0.049108236747901\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 37\nTraining loss: 0.044292686737186504 | Validation loss: 0.049626477540726154\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 38\nTraining loss: 0.042089813981043896 | Validation loss: 0.05028616577650594\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 39\nTraining loss: 0.040471516651336074 | Validation loss: 0.04539883885104044\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n------------------------------\nEpoch: 40\nTraining loss: 0.038953275320772814 | Validation loss: 0.038260011332093086\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 41\nTraining loss: 0.039753880176700535 | Validation loss: 0.03953419919166945\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 42\nTraining loss: 0.04075378145267233 | Validation loss: 0.04775480268341777\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 43\nTraining loss: 0.04191550611716321 | Validation loss: 0.05315274405664047\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 44\nTraining loss: 0.04337615864333059 | Validation loss: 0.04678723331441922\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 45\nTraining loss: 0.0448561115373718 | Validation loss: 0.04518167654761171\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 46\nTraining loss: 0.04328001322276069 | Validation loss: 0.04366327006270928\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 47\nTraining loss: 0.04148955256348168 | Validation loss: 0.04564844665274156\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 48\nTraining loss: 0.03969575352251794 | Validation loss: 0.04270295615810736\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 49\nTraining loss: 0.03778248318328016 | Validation loss: 0.045216823161571426\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n------------------------------\nEpoch: 50\nTraining loss: 0.036728900827020114 | Validation loss: 0.036400728900216325\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 51\nTraining loss: 0.037182638656318656 | Validation loss: 0.039920183492049705\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 52\nTraining loss: 0.03852524513958066 | Validation loss: 0.04021004818182076\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 53\nTraining loss: 0.039626261542045224 | Validation loss: 0.039501024509030105\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 54\nTraining loss: 0.04119911792644072 | Validation loss: 0.04457332704842618\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 55\nTraining loss: 0.042616045866968245 | Validation loss: 0.045998089163836124\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 56\nTraining loss: 0.04113453015039756 | Validation loss: 0.050729175304285196\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 57\nTraining loss: 0.03976550421266868 | Validation loss: 0.04352686356390472\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 58\nTraining loss: 0.03769710026296428 | Validation loss: 0.0416220211283823\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 59\nTraining loss: 0.03594528460933819 | Validation loss: 0.041781470377360824\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n------------------------------\nEpoch: 60\nTraining loss: 0.034993118661507144 | Validation loss: 0.03542220634531922\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 61\nTraining loss: 0.03528238928649487 | Validation loss: 0.041782269765317966\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 62\nTraining loss: 0.036277946592565245 | Validation loss: 0.048255273973387955\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 63\nTraining loss: 0.037810007194570436 | Validation loss: 0.04529560640849899\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 64\nTraining loss: 0.039236139712349224 | Validation loss: 0.046266291455357475\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 65\nTraining loss: 0.040906800351318294 | Validation loss: 0.06313739067553419\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 66\nTraining loss: 0.03917124048407065 | Validation loss: 0.04793259138818336\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 67\nTraining loss: 0.037500780060205345 | Validation loss: 0.04579635729304457\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 68\nTraining loss: 0.0361412501221112 | Validation loss: 0.04605197491107789\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 69\nTraining loss: 0.03438679540280517 | Validation loss: 0.0456059823983011\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221]\n------------------------------\nEpoch: 70\nTraining loss: 0.033415974244314035 | Validation loss: 0.03462613776194311\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 71\nTraining loss: 0.03395316714611579 | Validation loss: 0.038469126250232216\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 72\nTraining loss: 0.03479183258353109 | Validation loss: 0.03594474876876426\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 73\nTraining loss: 0.03633535791401143 | Validation loss: 0.04774723141000862\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 74\nTraining loss: 0.0374829898621359 | Validation loss: 0.04101652198726625\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 75\nTraining loss: 0.0391945250370855 | Validation loss: 0.0401327273915563\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 76\nTraining loss: 0.03788902773430175 | Validation loss: 0.03995021490330717\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 77\nTraining loss: 0.03639131400176859 | Validation loss: 0.03722566730482916\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 78\nTraining loss: 0.0348304618996986 | Validation loss: 0.04006522394453002\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 79\nTraining loss: 0.03319978419834323 | Validation loss: 0.04412269613713817\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614]\n------------------------------\nEpoch: 80\nTraining loss: 0.03219999324468031 | Validation loss: 0.03391640984445019\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 81\nTraining loss: 0.032688427642244465 | Validation loss: 0.037923850531203555\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 82\nTraining loss: 0.03355343060890638 | Validation loss: 0.04907491460310674\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 83\nTraining loss: 0.03503366941531137 | Validation loss: 0.040808099231361285\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 84\nTraining loss: 0.036361888898023234 | Validation loss: 0.04570449341451172\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 85\nTraining loss: 0.037688679410159354 | Validation loss: 0.03851071792959639\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 86\nTraining loss: 0.036509045416225246 | Validation loss: 0.05178985225481797\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 87\nTraining loss: 0.03476176010243096 | Validation loss: 0.04965500327535963\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 88\nTraining loss: 0.033466790973274846 | Validation loss: 0.04419837694252487\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 89\nTraining loss: 0.03203875968768078 | Validation loss: 0.037252725571789574\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641]\n------------------------------\nEpoch: 90\nTraining loss: 0.0312534848193264 | Validation loss: 0.03305692382288718\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 91\nTraining loss: 0.031425257888072586 | Validation loss: 0.035670075104036164\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 92\nTraining loss: 0.032341907633189844 | Validation loss: 0.05314765029908281\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 93\nTraining loss: 0.03363316069544095 | Validation loss: 0.05042708106338978\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 94\nTraining loss: 0.03508961931472336 | Validation loss: 0.04901893657788766\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 95\nTraining loss: 0.036707183874146204 | Validation loss: 0.06835057373793252\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 96\nTraining loss: 0.0352418556319931 | Validation loss: 0.05205297728885064\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 97\nTraining loss: 0.03360738364271966 | Validation loss: 0.03979173940565206\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 98\nTraining loss: 0.032393003445426664 | Validation loss: 0.04202516406642652\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 99\nTraining loss: 0.030928450876729578 | Validation loss: 0.03461811653610352\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692]\n------------------------------\nEpoch: 100\nTraining loss: 0.030302010686503445 | Validation loss: 0.032624518037237954\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 101\nTraining loss: 0.030482625337610914 | Validation loss: 0.03657771112908304\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 102\nTraining loss: 0.031544560573877785 | Validation loss: 0.04379735982655424\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 103\nTraining loss: 0.03267700951289767 | Validation loss: 0.05044605840096431\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 104\nTraining loss: 0.033956850249151606 | Validation loss: 0.039384911364290565\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 105\nTraining loss: 0.035652619702228175 | Validation loss: 0.040526895561313205\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 106\nTraining loss: 0.0343283357183031 | Validation loss: 0.037462938774739746\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 107\nTraining loss: 0.0329232818115224 | Validation loss: 0.04072877669097048\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 108\nTraining loss: 0.03152846630963343 | Validation loss: 0.039494824257835875\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 109\nTraining loss: 0.030297402225574113 | Validation loss: 0.035708916085088145\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n------------------------------\nEpoch: 110\nTraining loss: 0.02962929017907815 | Validation loss: 0.032125119894611094\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 111\nTraining loss: 0.029658222022855025 | Validation loss: 0.03462422362207312\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 112\nTraining loss: 0.030390083743992637 | Validation loss: 0.037167447627381944\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 113\nTraining loss: 0.03176091816716307 | Validation loss: 0.035366525972443344\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 114\nTraining loss: 0.0330082182680792 | Validation loss: 0.03707461589864925\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 115\nTraining loss: 0.03469608666642943 | Validation loss: 0.05098501968700274\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 116\nTraining loss: 0.033389072871664316 | Validation loss: 0.05167623942272853\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 117\nTraining loss: 0.0319534340987026 | Validation loss: 0.03803503140807152\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 118\nTraining loss: 0.03054207461651444 | Validation loss: 0.037278614163530614\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 119\nTraining loss: 0.029319434328642712 | Validation loss: 0.03323572590551545\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n------------------------------\nEpoch: 120\nTraining loss: 0.028277252811762528 | Validation loss: 0.031856293105973606\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 121\nTraining loss: 0.028927232044396966 | Validation loss: 0.03381007744938926\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 122\nTraining loss: 0.029823541352488686 | Validation loss: 0.036562808286563483\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 123\nTraining loss: 0.030995890572410457 | Validation loss: 0.03735244338425387\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 124\nTraining loss: 0.03232222717754015 | Validation loss: 0.0393577231737101\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 125\nTraining loss: 0.033686123715024295 | Validation loss: 0.039191240626098835\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 126\nTraining loss: 0.0328642097200949 | Validation loss: 0.04119740207308689\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 127\nTraining loss: 0.03124009068563991 | Validation loss: 0.045967824147206494\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 128\nTraining loss: 0.029692287028833166 | Validation loss: 0.03530660837855751\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 129\nTraining loss: 0.028460362538380007 | Validation loss: 0.034659543915330306\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629]\n------------------------------\nEpoch: 130\nTraining loss: 0.02782241098296748 | Validation loss: 0.03151542620848766\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 131\nTraining loss: 0.028115404061203105 | Validation loss: 0.0364043337779235\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 132\nTraining loss: 0.028829176158563594 | Validation loss: 0.0347792377069065\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 133\nTraining loss: 0.030203928205727298 | Validation loss: 0.03419680051109959\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 134\nTraining loss: 0.03136087996760515 | Validation loss: 0.04011434100700163\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 135\nTraining loss: 0.033034575681429444 | Validation loss: 0.040510393481338974\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 136\nTraining loss: 0.03180332440336594 | Validation loss: 0.03888363813140751\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 137\nTraining loss: 0.030477852163667694 | Validation loss: 0.03862581931186461\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 138\nTraining loss: 0.0292505194876695 | Validation loss: 0.03288874167751158\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 139\nTraining loss: 0.027981118863727165 | Validation loss: 0.03388794129961623\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543]\n------------------------------\nEpoch: 140\nTraining loss: 0.027372297242386486 | Validation loss: 0.031212307529243748\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 141\nTraining loss: 0.0276586958958848 | Validation loss: 0.03246931614667441\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 142\nTraining loss: 0.028360963867808595 | Validation loss: 0.034810532453115536\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 143\nTraining loss: 0.029655160085393453 | Validation loss: 0.03974294517420034\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 144\nTraining loss: 0.030910991771168655 | Validation loss: 0.04399756938878414\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 145\nTraining loss: 0.0325270962652083 | Validation loss: 0.061640194658420785\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 146\nTraining loss: 0.03149095214683357 | Validation loss: 0.03760540243073375\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 147\nTraining loss: 0.029875266428039535 | Validation loss: 0.035697247552792584\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 148\nTraining loss: 0.028500897502783305 | Validation loss: 0.03381003710581402\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 149\nTraining loss: 0.027681679243045883 | Validation loss: 0.032980115103444695\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231]\n------------------------------\nEpoch: 150\nTraining loss: 0.026877311056509146 | Validation loss: 0.031143162226452765\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 151\nTraining loss: 0.026977618478832576 | Validation loss: 0.03266228110719044\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 152\nTraining loss: 0.027789666411818893 | Validation loss: 0.034961970028492204\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 153\nTraining loss: 0.02894545074526631 | Validation loss: 0.03532901065077929\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 154\nTraining loss: 0.030454562234464945 | Validation loss: 0.040225627527168364\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 155\nTraining loss: 0.03164946059806375 | Validation loss: 0.037975336828854234\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 156\nTraining loss: 0.03068030412000875 | Validation loss: 0.04220050962361614\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 157\nTraining loss: 0.029348198092527922 | Validation loss: 0.03511155945603299\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 158\nTraining loss: 0.028144483990769277 | Validation loss: 0.03515485296668732\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 159\nTraining loss: 0.027031416854432482 | Validation loss: 0.034209251535677276\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316]\n------------------------------\nEpoch: 160\nTraining loss: 0.026445907158373787 | Validation loss: 0.030806712303475467\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 161\nTraining loss: 0.026552394952801033 | Validation loss: 0.03321779652836576\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 162\nTraining loss: 0.027329204999108424 | Validation loss: 0.035172222956883166\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 163\nTraining loss: 0.028377699224813657 | Validation loss: 0.03415004107936294\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 164\nTraining loss: 0.02962424848998684 | Validation loss: 0.0380124523576382\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 165\nTraining loss: 0.031245342592386104 | Validation loss: 0.03870395737477636\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 166\nTraining loss: 0.03022429422295733 | Validation loss: 0.03497945312080921\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 167\nTraining loss: 0.02867190224232196 | Validation loss: 0.034943901974938615\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 168\nTraining loss: 0.02769064278008167 | Validation loss: 0.0328868226667421\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 169\nTraining loss: 0.026110646025255674 | Validation loss: 0.032032206835868084\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n------------------------------\nEpoch: 170\nTraining loss: 0.025901495435964463 | Validation loss: 0.03086405956244047\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 171\nTraining loss: 0.025832738417959734 | Validation loss: 0.03191902321161686\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 172\nTraining loss: 0.02658675519811855 | Validation loss: 0.032392649120131956\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 173\nTraining loss: 0.02785930638549864 | Validation loss: 0.04055770080272866\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 174\nTraining loss: 0.029304441305676724 | Validation loss: 0.03938528715707032\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 175\nTraining loss: 0.030693826810126817 | Validation loss: 0.042542108787899525\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 176\nTraining loss: 0.02968439479967154 | Validation loss: 0.03487302249182114\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 177\nTraining loss: 0.028509433252010934 | Validation loss: 0.035466368128834046\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 178\nTraining loss: 0.027170329408786133 | Validation loss: 0.03241105497180097\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 179\nTraining loss: 0.026051353732871964 | Validation loss: 0.031891154834127\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n------------------------------\nEpoch: 180\nTraining loss: 0.025475371672082546 | Validation loss: 0.030502695054541118\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 181\nTraining loss: 0.02571635264591644 | Validation loss: 0.03273352388852993\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 182\nTraining loss: 0.02636262705855761 | Validation loss: 0.03367737308968749\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 183\nTraining loss: 0.02711716690467392 | Validation loss: 0.03316663689416858\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 184\nTraining loss: 0.028923610236319737 | Validation loss: 0.03617072011451278\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 185\nTraining loss: 0.029695096961955915 | Validation loss: 0.03914791269771821\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 186\nTraining loss: 0.02894029467949629 | Validation loss: 0.033959697874312376\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 187\nTraining loss: 0.02806392555055392 | Validation loss: 0.032832851015295074\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 188\nTraining loss: 0.026699587195840348 | Validation loss: 0.03351602898663388\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 189\nTraining loss: 0.02575078903052105 | Validation loss: 0.032145418597599576\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027 ]\n------------------------------\nEpoch: 190\nTraining loss: 0.025277233614740293 | Validation loss: 0.030475220247377866\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 191\nTraining loss: 0.02529354150293351 | Validation loss: 0.031475040253944105\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 192\nTraining loss: 0.02586796419332024 | Validation loss: 0.03231935241811835\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 193\nTraining loss: 0.026974914326904503 | Validation loss: 0.03522281436068294\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 194\nTraining loss: 0.028522872728090294 | Validation loss: 0.03937122993896493\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 195\nTraining loss: 0.029662396922876224 | Validation loss: 0.04080926054940287\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 196\nTraining loss: 0.02874099444799624 | Validation loss: 0.03661016435815697\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 197\nTraining loss: 0.027630861122601144 | Validation loss: 0.03896229999321225\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 198\nTraining loss: 0.026198979643151515 | Validation loss: 0.03245780790900498\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 199\nTraining loss: 0.025587171060399806 | Validation loss: 0.0329771255332548\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522]\n------------------------------\nEpoch: 200\nTraining loss: 0.02489157276609399 | Validation loss: 0.030435980206965346\nValidation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n 0.0305027  0.03047522 0.03043598]\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.13013751572364662 | Validation loss: 0.10866411221502102\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 1\nTraining loss: 0.0962619811872123 | Validation loss: 0.08431469047425595\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 2\nTraining loss: 0.08808117919403502 | Validation loss: 0.08335764244594406\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 3\nTraining loss: 0.08320651522573583 | Validation loss: 0.07264648778446481\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 4\nTraining loss: 0.078821630328967 | Validation loss: 0.086400652452644\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 5\nTraining loss: 0.07551912005668081 | Validation loss: 0.07294148150666625\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 6\nTraining loss: 0.06994353534147789 | Validation loss: 0.07389080859061363\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 7\nTraining loss: 0.06581751633926815 | Validation loss: 0.05924121888799477\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 8\nTraining loss: 0.061769033043359324 | Validation loss: 0.05550177810730132\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 9\nTraining loss: 0.05849388224901411 | Validation loss: 0.05526111535398306\nValidation loss (ends of cycles): [0.10866411]\n------------------------------\nEpoch: 10\nTraining loss: 0.05599109683640477 | Validation loss: 0.05028300414240993\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 11\nTraining loss: 0.05651184237070792 | Validation loss: 0.05119481994317169\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 12\nTraining loss: 0.05796342328608799 | Validation loss: 0.05256845900970223\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 13\nTraining loss: 0.05864124310452227 | Validation loss: 0.05311432339053238\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 14\nTraining loss: 0.05971920895060216 | Validation loss: 0.053835257805422344\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 15\nTraining loss: 0.0600251086206564 | Validation loss: 0.05795030089804029\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 16\nTraining loss: 0.05733736497020041 | Validation loss: 0.05345632078175524\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 17\nTraining loss: 0.05464613109489712 | Validation loss: 0.05284101431942092\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 18\nTraining loss: 0.05230115667557505 | Validation loss: 0.04781449882857568\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 19\nTraining loss: 0.04979933290119012 | Validation loss: 0.04415781805868697\nValidation loss (ends of cycles): [0.10866411 0.050283  ]\n------------------------------\nEpoch: 20\nTraining loss: 0.048008861920363674 | Validation loss: 0.04327868562257659\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 21\nTraining loss: 0.04882487160425017 | Validation loss: 0.043836381352317016\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 22\nTraining loss: 0.049844463726168305 | Validation loss: 0.044874199962787395\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 23\nTraining loss: 0.05121253936104183 | Validation loss: 0.04625493370458088\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 24\nTraining loss: 0.05227855082021982 | Validation loss: 0.06580113441543242\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 25\nTraining loss: 0.05329341503725512 | Validation loss: 0.04736212479461611\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 26\nTraining loss: 0.05157633041174657 | Validation loss: 0.04704422052059554\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 27\nTraining loss: 0.04926364007405937 | Validation loss: 0.05678830411186261\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 28\nTraining loss: 0.047098851822294646 | Validation loss: 0.043971751379755744\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 29\nTraining loss: 0.044588426964936825 | Validation loss: 0.041111332207786296\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n------------------------------\nEpoch: 30\nTraining loss: 0.043297484392432244 | Validation loss: 0.04007098248508652\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 31\nTraining loss: 0.044140496467218154 | Validation loss: 0.041449001322673484\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 32\nTraining loss: 0.04528621606360094 | Validation loss: 0.043515847155214414\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 33\nTraining loss: 0.046532035345123623 | Validation loss: 0.04557813516100951\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 34\nTraining loss: 0.04797187640779014 | Validation loss: 0.04983178259129018\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 35\nTraining loss: 0.04931510559569194 | Validation loss: 0.047406337331090356\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 36\nTraining loss: 0.04766238858651986 | Validation loss: 0.045556718137411946\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 37\nTraining loss: 0.045729071468364184 | Validation loss: 0.043208298611298074\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 38\nTraining loss: 0.04352147483237557 | Validation loss: 0.04185216669487742\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 39\nTraining loss: 0.041625120196726144 | Validation loss: 0.03896412177555329\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n------------------------------\nEpoch: 40\nTraining loss: 0.04038160263494713 | Validation loss: 0.03796320309681175\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 41\nTraining loss: 0.04075280056100368 | Validation loss: 0.03851696993924875\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 42\nTraining loss: 0.04215318926217724 | Validation loss: 0.03938400522863443\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 43\nTraining loss: 0.04349201472703455 | Validation loss: 0.04077490941679056\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 44\nTraining loss: 0.04480456488250982 | Validation loss: 0.04196629445769091\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 45\nTraining loss: 0.046417191071897804 | Validation loss: 0.04734774543780141\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 46\nTraining loss: 0.044565415281186425 | Validation loss: 0.042343570760129824\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 47\nTraining loss: 0.04292275310736003 | Validation loss: 0.046334737252477\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 48\nTraining loss: 0.04080903969382442 | Validation loss: 0.039976048242069975\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 49\nTraining loss: 0.038945722352442015 | Validation loss: 0.03901533678635559\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n------------------------------\nEpoch: 50\nTraining loss: 0.03775392111123165 | Validation loss: 0.03635692800832006\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 51\nTraining loss: 0.03847538466898974 | Validation loss: 0.037682123169039204\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 52\nTraining loss: 0.03952817849660894 | Validation loss: 0.038835352138344166\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 53\nTraining loss: 0.040770487166853164 | Validation loss: 0.04136987426112183\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 54\nTraining loss: 0.04249348099743141 | Validation loss: 0.04505128795857978\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 55\nTraining loss: 0.04351341669029725 | Validation loss: 0.054758314850979144\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 56\nTraining loss: 0.04219748325440593 | Validation loss: 0.044231936352047245\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 57\nTraining loss: 0.040449189506177825 | Validation loss: 0.04580309190911002\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 58\nTraining loss: 0.03866173452184367 | Validation loss: 0.0378917996994162\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 59\nTraining loss: 0.037179108895178094 | Validation loss: 0.03695562399462261\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n------------------------------\nEpoch: 60\nTraining loss: 0.035941219553131405 | Validation loss: 0.034984574340020134\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 61\nTraining loss: 0.03639172680041657 | Validation loss: 0.03606705074157335\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 62\nTraining loss: 0.0375638076628551 | Validation loss: 0.03725699788635283\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 63\nTraining loss: 0.039039047017847515 | Validation loss: 0.038210892637746526\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 64\nTraining loss: 0.04020949403707701 | Validation loss: 0.04285801162498187\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 65\nTraining loss: 0.041740124291689024 | Validation loss: 0.041410352474292825\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 66\nTraining loss: 0.040310035649445054 | Validation loss: 0.05140106689877215\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 67\nTraining loss: 0.03865856288173273 | Validation loss: 0.04588074636010997\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 68\nTraining loss: 0.036848236220030806 | Validation loss: 0.039373264755163576\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 69\nTraining loss: 0.03542020992562908 | Validation loss: 0.036005012377832844\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457]\n------------------------------\nEpoch: 70\nTraining loss: 0.034385565286098915 | Validation loss: 0.03407285567702709\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 71\nTraining loss: 0.034998325296442985 | Validation loss: 0.03499888496852554\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 72\nTraining loss: 0.03608989844824679 | Validation loss: 0.037975290641847965\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 73\nTraining loss: 0.037150994481291534 | Validation loss: 0.03802902005876588\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 74\nTraining loss: 0.03874208623778046 | Validation loss: 0.041800051696796334\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 75\nTraining loss: 0.04030366619135891 | Validation loss: 0.04402384821292574\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 76\nTraining loss: 0.03897080734265807 | Validation loss: 0.038405700957616876\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 77\nTraining loss: 0.03700368836206773 | Validation loss: 0.03706241132839855\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 78\nTraining loss: 0.03561326385912023 | Validation loss: 0.03773653503935949\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 79\nTraining loss: 0.03413904608890971 | Validation loss: 0.034595715953449235\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286]\n------------------------------\nEpoch: 80\nTraining loss: 0.033077710618880964 | Validation loss: 0.03346761355621625\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 81\nTraining loss: 0.03339501076990255 | Validation loss: 0.03464310675595714\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 82\nTraining loss: 0.03449362341413553 | Validation loss: 0.03528704326105329\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 83\nTraining loss: 0.03574517990213152 | Validation loss: 0.03956749140227263\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 84\nTraining loss: 0.03712140995547117 | Validation loss: 0.04075488192647432\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 85\nTraining loss: 0.03851752373111236 | Validation loss: 0.04053122321127263\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 86\nTraining loss: 0.037289504063550354 | Validation loss: 0.038621348683285504\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 87\nTraining loss: 0.03606580819487278 | Validation loss: 0.03755889262641426\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 88\nTraining loss: 0.03403875287750545 | Validation loss: 0.03664958490207132\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 89\nTraining loss: 0.032725286617779764 | Validation loss: 0.03518285962497502\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761]\n------------------------------\nEpoch: 90\nTraining loss: 0.03179220572630985 | Validation loss: 0.03308977194097454\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 91\nTraining loss: 0.03204954804313611 | Validation loss: 0.03412141737453969\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 92\nTraining loss: 0.033113976240187415 | Validation loss: 0.03650290599768668\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 93\nTraining loss: 0.03438642540282944 | Validation loss: 0.03640461193842698\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 94\nTraining loss: 0.035784698719356765 | Validation loss: 0.03660603991783826\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 95\nTraining loss: 0.03738688092631119 | Validation loss: 0.04603489715836744\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 96\nTraining loss: 0.0360823278718694 | Validation loss: 0.03907236244232781\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 97\nTraining loss: 0.03440698718571463 | Validation loss: 0.03614757999579991\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 98\nTraining loss: 0.032923828401624805 | Validation loss: 0.03688841782905887\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 99\nTraining loss: 0.03173442275582365 | Validation loss: 0.034847187158544506\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977]\n------------------------------\nEpoch: 100\nTraining loss: 0.030615978687268307 | Validation loss: 0.03256259926190946\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 101\nTraining loss: 0.030597679183165742 | Validation loss: 0.03352231219971338\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 102\nTraining loss: 0.03192661127614547 | Validation loss: 0.034362965502438295\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 103\nTraining loss: 0.03307850291681161 | Validation loss: 0.035471899154703175\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 104\nTraining loss: 0.03460319688741675 | Validation loss: 0.04395314779099638\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 105\nTraining loss: 0.03603993922812996 | Validation loss: 0.03989762382631281\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 106\nTraining loss: 0.03485340225568965 | Validation loss: 0.03633215443222924\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 107\nTraining loss: 0.03364605744420661 | Validation loss: 0.03456024982167029\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 108\nTraining loss: 0.03178448307852253 | Validation loss: 0.03427424061779691\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 109\nTraining loss: 0.03050562876870665 | Validation loss: 0.03418495161541268\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n------------------------------\nEpoch: 110\nTraining loss: 0.029790386914158255 | Validation loss: 0.03241037907886558\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 111\nTraining loss: 0.03013055137904933 | Validation loss: 0.03372434017339111\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 112\nTraining loss: 0.030869591665842872 | Validation loss: 0.03451865892527641\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 113\nTraining loss: 0.032268728440312124 | Validation loss: 0.036281384115593625\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 114\nTraining loss: 0.03397129182373797 | Validation loss: 0.037896803192860255\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 115\nTraining loss: 0.03532382988418621 | Validation loss: 0.04804187614174016\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 116\nTraining loss: 0.03400072505730608 | Validation loss: 0.03747494302821898\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 117\nTraining loss: 0.032401689846163956 | Validation loss: 0.03554237289436623\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 118\nTraining loss: 0.03092662235776945 | Validation loss: 0.03437768438814488\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 119\nTraining loss: 0.029526078819634583 | Validation loss: 0.03346088269840827\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n------------------------------\nEpoch: 120\nTraining loss: 0.028847131215395245 | Validation loss: 0.03171695443282349\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 121\nTraining loss: 0.029099270120248433 | Validation loss: 0.03273795564057289\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 122\nTraining loss: 0.03011367364243905 | Validation loss: 0.033902542846920215\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 123\nTraining loss: 0.03149592533238291 | Validation loss: 0.04488189828343096\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 124\nTraining loss: 0.03292691715219329 | Validation loss: 0.03982129518305306\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 125\nTraining loss: 0.03405568103294995 | Validation loss: 0.049960068979226384\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 126\nTraining loss: 0.0330583095444205 | Validation loss: 0.03734910000214534\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 127\nTraining loss: 0.03171656856324348 | Validation loss: 0.04288117178773458\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 128\nTraining loss: 0.03015985843582504 | Validation loss: 0.03478364519511176\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 129\nTraining loss: 0.02886176664715882 | Validation loss: 0.033183983189210425\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695]\n------------------------------\nEpoch: 130\nTraining loss: 0.02814806290759199 | Validation loss: 0.03104466441359404\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695 0.03104466]\n------------------------------\nEpoch: 131\nTraining loss: 0.028255866756149398 | Validation loss: 0.0327843960325143\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695 0.03104466]\n------------------------------\nEpoch: 132\nTraining loss: 0.02914011667071893 | Validation loss: 0.033506754794017934\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695 0.03104466]\n------------------------------\nEpoch: 133\nTraining loss: 0.030517671123425676 | Validation loss: 0.034617729789981275\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695 0.03104466]\n------------------------------\nEpoch: 134\nTraining loss: 0.03195116536941115 | Validation loss: 0.03802656727653425\nValidation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n 0.03171695 0.03104466]\n------------------------------\nEpoch: 135\n\n\n\nEvaluate on all\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\nseeds = range(20)\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_global_all, y_hats_all, y_trues_all, ns_all = learners.evaluate((X, y, depth_order[:, -1]),\n                                                                      src_dir_model=src_dir_model)\n\n\nprint(f'# of test samples: {ns_all.mean().item()}')\n\n# of test samples: 4032.0\n\n\n\n# Save spectific seed y_hat, y_true to plot \"Observed vs. predicted\" scatterplots\n# Replace following Paths with yours\ndest_dir_predicted = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/')\nseed = 1\nwith open(dest_dir_predicted/f'predicted-true-cnn-seed-{seed}.pickle', 'wb') as f: \n    pickle.dump((y_hats_all[seed].to_numpy(), y_trues_all[seed].to_numpy()), f)\n\n\nperfs_global_all.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.184830\n      2.992808\n      0.790243\n      0.884580\n      0.595624\n      0.377839\n      0.237318\n      30.946025\n      0.004749\n      0.009024\n    \n    \n      std\n      0.041494\n      0.067646\n      0.007929\n      0.004986\n      0.155836\n      0.259428\n      0.012061\n      0.932127\n      0.012705\n      0.024443\n    \n    \n      min\n      2.114635\n      2.889447\n      0.776315\n      0.874575\n      0.459073\n      0.210748\n      0.221478\n      29.163000\n      -0.012504\n      -0.024802\n    \n    \n      25%\n      2.159791\n      2.930840\n      0.785559\n      0.881369\n      0.504516\n      0.254547\n      0.225294\n      30.303971\n      -0.005159\n      -0.010186\n    \n    \n      50%\n      2.183314\n      3.015281\n      0.790165\n      0.884643\n      0.571964\n      0.327175\n      0.236342\n      30.827668\n      0.004057\n      0.007958\n    \n    \n      75%\n      2.205628\n      3.028623\n      0.794389\n      0.887016\n      0.617017\n      0.380868\n      0.246811\n      31.663001\n      0.016258\n      0.031279\n    \n    \n      max\n      2.262492\n      3.141478\n      0.804596\n      0.895479\n      1.197708\n      1.434504\n      0.260846\n      32.642108\n      0.032962\n      0.062948\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nEvaluate on Soil Tax. Orders\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\nseeds = range(20)\n\nfor k, v in tax_lookup.items():\n    print(80*'-')\n    print(f'Test metrics on {k}')\n    print(80*'-')\n    learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n    perfs_global, _, _, ns = learners.evaluate((X, y, depth_order[:, -1]),\n                                               order=v,\n                                               src_dir_model=src_dir_model)\n\n    print(f'# of test samples: {ns.mean().item()}')\n    print(perfs_global.describe())\n\n--------------------------------------------------------------------------------\nTest metrics on alfisols\n--------------------------------------------------------------------------------\n# of test samples: 422.4\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.809327   2.462286   0.691663   0.822509   0.382106   0.174798   \nstd     0.087705   0.182733   0.030946   0.018080   0.174094   0.157874   \nmin     1.602639   2.154843   0.609675   0.773057   0.150495   0.022649   \n25%     1.752131   2.328294   0.673422   0.810919   0.233184   0.054400   \n50%     1.814577   2.449380   0.695557   0.823634   0.371884   0.138515   \n75%     1.875471   2.573424   0.715007   0.835819   0.459433   0.211082   \nmax     1.949812   2.768350   0.736324   0.847946   0.794326   0.630953   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.134967  27.355655   0.003228   0.008986  \nstd     0.020114   1.268820   0.011029   0.028718  \nmin     0.101224  24.950266  -0.013376  -0.032392  \n25%     0.122148  26.654700  -0.005885  -0.015329  \n50%     0.130240  27.405189   0.004066   0.010273  \n75%     0.149955  27.915920   0.012193   0.033151  \nmax     0.168116  29.876232   0.020493   0.053057  \n--------------------------------------------------------------------------------\nTest metrics on mollisols\n--------------------------------------------------------------------------------\n# of test samples: 977.6\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.082906   2.750063   0.767633   0.868226   0.441974   0.200178   \nstd     0.105370   0.152068   0.022648   0.014038   0.071353   0.074478   \nmin     1.910614   2.503716   0.725773   0.839147   0.351621   0.123637   \n25%     2.016213   2.645241   0.753751   0.858901   0.398954   0.159184   \n50%     2.073633   2.727649   0.767201   0.867132   0.434792   0.189089   \n75%     2.158195   2.803545   0.785071   0.877551   0.462096   0.213538   \nmax     2.297545   3.113704   0.810361   0.892300   0.693749   0.481287   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.217482  27.362061   0.006410   0.014927  \nstd     0.011783   1.171648   0.014516   0.034156  \nmin     0.198063  25.863001  -0.017837  -0.042497  \n25%     0.208970  26.442070  -0.002144  -0.005110  \n50%     0.217809  27.306423   0.006673   0.016067  \n75%     0.226259  27.765553   0.015423   0.036009  \nmax     0.238283  30.162075   0.034060   0.082286  \n--------------------------------------------------------------------------------\nTest metrics on inceptisols\n--------------------------------------------------------------------------------\n# of test samples: 289.6\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.909866   2.651945   0.722069   0.838262   0.395929   0.161811   \nstd     0.112304   0.174091   0.033546   0.022861   0.072917   0.060037   \nmin     1.644649   2.362685   0.628917   0.773094   0.297366   0.088427   \n25%     1.843177   2.472268   0.704578   0.825389   0.341724   0.116911   \n50%     1.907174   2.692387   0.724081   0.840439   0.403799   0.163472   \n75%     1.972514   2.807575   0.742076   0.852019   0.446248   0.199140   \nmax     2.168335   2.898377   0.786479   0.878068   0.566843   0.321311   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.188652  35.004717  -0.003812  -0.007661  \nstd     0.019507   3.099745   0.016768   0.033159  \nmin     0.167560  31.472638  -0.023707  -0.052528  \n25%     0.173922  33.050188  -0.018816  -0.037482  \n50%     0.185284  34.055609  -0.007071  -0.014775  \n75%     0.193686  36.080701   0.008284   0.017426  \nmax     0.240127  43.579751   0.026359   0.048525  \n--------------------------------------------------------------------------------\nTest metrics on entisols\n--------------------------------------------------------------------------------\n# of test samples: 164.8\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.146993   3.000346   0.772961   0.875231   0.323115   0.107320   \nstd     0.250396   0.376957   0.053555   0.032886   0.055413   0.036646   \nmin     1.703938   2.254587   0.653383   0.784883   0.225837   0.051002   \n25%     2.015869   2.812905   0.752028   0.869245   0.281971   0.079520   \n50%     2.098403   3.022831   0.771459   0.877648   0.317529   0.100827   \n75%     2.324253   3.244836   0.813595   0.896911   0.360752   0.130172   \nmax     2.658273   3.657772   0.857475   0.920406   0.431522   0.186212   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.164777  30.832117   0.004347   0.008009  \nstd     0.019656   3.450412   0.020474   0.042995  \nmin     0.122440  24.067251  -0.025000  -0.061117  \n25%     0.151904  27.745744  -0.010036  -0.018291  \n50%     0.168026  31.689824   0.003999   0.007015  \n75%     0.175205  32.814956   0.017534   0.035175  \nmax     0.199315  37.243327   0.037462   0.079188  \n--------------------------------------------------------------------------------\nTest metrics on spodosols\n--------------------------------------------------------------------------------\n# of test samples: 64.0\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.181398   3.140114   0.779113   0.880473   0.412119   0.180965   \nstd     0.226818   0.688425   0.046961   0.025426   0.108204   0.098489   \nmin     1.776501   2.128022   0.677271   0.820691   0.268197   0.071930   \n25%     2.026443   2.670819   0.752034   0.863932   0.326987   0.106931   \n50%     2.153188   2.983713   0.780577   0.884321   0.386405   0.149414   \n75%     2.367428   3.278641   0.818189   0.898756   0.483470   0.233907   \nmax     2.553907   4.704971   0.843790   0.914914   0.681660   0.464660   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.223380  37.213633  -0.008654  -0.013208  \nstd     0.050800   4.705197   0.028514   0.043828  \nmin     0.148266  31.124797  -0.073955  -0.085358  \n25%     0.184255  32.850098  -0.031603  -0.049806  \n50%     0.215128  37.193228  -0.010030  -0.018675  \n75%     0.268383  40.618369   0.015755   0.023395  \nmax     0.314407  45.255888   0.037978   0.053857  \n--------------------------------------------------------------------------------\nTest metrics on undefined\n--------------------------------------------------------------------------------\n# of test samples: 1553.6\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.292085   3.140178   0.809147   0.896603   0.756287   0.646963   \nstd     0.061811   0.126350   0.010024   0.006282   0.280963   0.667752   \nmin     2.200176   2.961841   0.793285   0.887092   0.533183   0.284284   \n25%     2.250938   3.058737   0.802503   0.892838   0.609880   0.372009   \n50%     2.294312   3.141787   0.809902   0.896849   0.712569   0.507759   \n75%     2.315500   3.190854   0.813364   0.899591   0.796151   0.633870   \nmax     2.437377   3.432564   0.831563   0.912160   1.841250   3.390202   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.292059  31.645606   0.004302   0.007476  \nstd     0.023362   1.247020   0.014321   0.025455  \nmin     0.258135  29.001465  -0.016584  -0.031293  \n25%     0.272517  30.930875  -0.006044  -0.011075  \n50%     0.289404  31.708111   0.001588   0.002819  \n75%     0.306450  32.771565   0.014035   0.025830  \nmax     0.339195  33.584201   0.036192   0.061086  \n--------------------------------------------------------------------------------\nTest metrics on ultisols\n--------------------------------------------------------------------------------\n# of test samples: 192.0\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.632526   2.265209   0.617841   0.763237   0.260545   0.071859   \nstd     0.108925   0.250811   0.051179   0.038303   0.064686   0.036546   \nmin     1.428423   1.788526   0.507145   0.672963   0.169072   0.028586   \n25%     1.568656   2.078099   0.591292   0.742977   0.216448   0.046878   \n50%     1.622599   2.291745   0.617950   0.762776   0.246636   0.060842   \n75%     1.695520   2.441266   0.650373   0.784582   0.298181   0.089221   \nmax     1.858865   2.726562   0.708979   0.823643   0.394320   0.155488   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.135663  32.535614   0.017157   0.040477  \nstd     0.020282   2.442567   0.013812   0.032344  \nmin     0.107650  28.119987  -0.010331  -0.025534  \n25%     0.121366  31.080629   0.011006   0.025485  \n50%     0.136634  32.268478   0.015768   0.035893  \n75%     0.147672  33.884998   0.027038   0.058695  \nmax     0.179378  38.015869   0.047486   0.105611  \n--------------------------------------------------------------------------------\nTest metrics on andisols\n--------------------------------------------------------------------------------\n# of test samples: 132.8\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.982082   2.557273   0.738402   0.856554   0.476687   0.248451   \nstd     0.162387   0.330953   0.041831   0.025708   0.149459   0.166614   \nmin     1.738631   1.923586   0.666517   0.798802   0.276538   0.076474   \n25%     1.859672   2.319364   0.708526   0.839169   0.382923   0.146634   \n50%     1.946365   2.628500   0.733634   0.856545   0.454000   0.206157   \n75%     2.118545   2.801427   0.775115   0.878573   0.557137   0.310493   \nmax     2.280665   3.039670   0.806156   0.894918   0.833418   0.694586   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.229848  32.559297   0.015201   0.033912  \nstd     0.026493   3.362845   0.023263   0.050849  \nmin     0.175337  27.092579  -0.025021  -0.052350  \n25%     0.206640  30.636463  -0.003132  -0.006866  \n50%     0.234442  33.061846   0.017000   0.037901  \n75%     0.248909  35.253529   0.023922   0.056046  \nmax     0.262889  38.183209   0.064641   0.131825  \n--------------------------------------------------------------------------------\nTest metrics on histosols\n--------------------------------------------------------------------------------\n# of test samples: 80.0\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.093646   3.055053   0.758036   0.870220   0.866301   0.819420   \nstd     0.274214   0.770695   0.056379   0.028888   0.269391   0.556293   \nmin     1.656537   2.138313   0.630659   0.818658   0.570947   0.325981   \n25%     1.920327   2.384933   0.725072   0.850471   0.698688   0.488250   \n50%     2.058352   2.852104   0.760004   0.867652   0.787495   0.621255   \n75%     2.199841   3.507436   0.790265   0.882838   0.946578   0.896546   \nmax     2.885291   4.842908   0.877941   0.932256   1.453742   2.113365   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.449648  45.448765   0.024104   0.034142  \nstd     0.118631   7.463257   0.034970   0.048042  \nmin     0.294663  34.994602  -0.071150  -0.081119  \n25%     0.362897  39.577908   0.011031   0.011864  \n50%     0.415026  44.506969   0.034138   0.050279  \n75%     0.547573  51.371342   0.046856   0.066266  \nmax     0.660869  57.288700   0.067678   0.099684  \n--------------------------------------------------------------------------------\nTest metrics on oxisols\n--------------------------------------------------------------------------------\n# of test samples: 32.0\n            rpd      rpiq          r2      lccc      rmse       mse       mae  \\\ncount  2.000000  8.000000    2.000000  2.000000  8.000000  8.000000  8.000000   \nmean   0.118367  0.026377 -134.414165  0.030174  0.087786  0.013825  0.083611   \nstd    0.046499  0.051292   98.771373  0.054914  0.083623  0.022839  0.081877   \nmin    0.085487  0.000000 -204.256073 -0.008656  0.009060  0.000082  0.009060   \n25%    0.101927  0.000000 -169.335119  0.010759  0.025124  0.000655  0.025124   \n50%    0.118367  0.000000 -134.414165  0.030174  0.067840  0.004688  0.065089   \n75%    0.134807  0.019050  -99.493211  0.049589  0.118719  0.014272  0.111745   \nmax    0.151247  0.134818  -64.572257  0.069005  0.260096  0.067650  0.260096   \n\n             mape      bias        stb  \ncount    8.000000  8.000000   8.000000  \nmean    65.191675 -0.177251        NaN  \nstd     60.839932  0.164703        NaN  \nmin      7.445914 -0.467451       -inf  \n25%     20.544780 -0.258737        NaN  \n50%     52.729677 -0.180752        NaN  \n75%     88.397117 -0.074745 -10.384091  \nmax    193.393838  0.064107        inf  \n--------------------------------------------------------------------------------\nTest metrics on vertisols\n--------------------------------------------------------------------------------\n# of test samples: 94.4\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.039982   2.815862   0.744782   0.856862   0.273233   0.078001   \nstd     0.271953   0.511288   0.063033   0.036968   0.059333   0.036126   \nmin     1.577847   2.128196   0.592591   0.768764   0.182002   0.033125   \n25%     1.851439   2.431125   0.703943   0.837489   0.232396   0.054013   \n50%     1.992413   2.714564   0.744894   0.855618   0.253052   0.064036   \n75%     2.178701   3.148894   0.786316   0.881447   0.303549   0.092144   \nmax     2.713312   4.022168   0.862141   0.923391   0.427089   0.182405   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.173488  26.855655  -0.001182  -0.003823  \nstd     0.021406   3.272257   0.020647   0.055384  \nmin     0.138288  21.111543  -0.052648  -0.154459  \n25%     0.157317  24.537949  -0.011511  -0.028144  \n50%     0.173311  26.192354  -0.001295  -0.003575  \n75%     0.188509  28.037003   0.009025   0.019823  \nmax     0.208709  33.903366   0.040271   0.099009  \n--------------------------------------------------------------------------------\nTest metrics on aridisols\n--------------------------------------------------------------------------------\n# of test samples: 163.2\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.814492   2.405396   0.691015   0.821971   0.662013   0.489155   \nstd     0.109405   0.241063   0.037256   0.022591   0.231457   0.352484   \nmin     1.613652   1.941601   0.613362   0.776206   0.373065   0.139178   \n25%     1.750923   2.250058   0.671518   0.809034   0.485274   0.235720   \n50%     1.814514   2.415395   0.694165   0.819871   0.598650   0.359678   \n75%     1.885174   2.577828   0.716658   0.838137   0.773024   0.597940   \nmax     2.036902   2.856824   0.757279   0.864022   1.208685   1.460919   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.300450  35.089144   0.008393   0.015292  \nstd     0.059640   3.065990   0.021957   0.044950  \nmin     0.205739  28.651589  -0.029821  -0.072086  \n25%     0.256783  33.024969  -0.006753  -0.014939  \n50%     0.295750  34.871383   0.012934   0.028759  \n75%     0.330856  37.073732   0.020540   0.042719  \nmax     0.408710  41.000089   0.048986   0.085159  \n--------------------------------------------------------------------------------\nTest metrics on gelisols\n--------------------------------------------------------------------------------\n# of test samples: 60.8\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    2.094545   3.140563   0.744767   0.860555   0.588931   0.365489   \nstd     0.397299   1.088457   0.082866   0.047535   0.140113   0.173223   \nmin     1.563934   1.337414   0.579469   0.746647   0.378497   0.143260   \n25%     1.827473   2.396965   0.692855   0.838851   0.495282   0.245315   \n50%     2.025772   2.948529   0.749510   0.854638   0.568492   0.323183   \n75%     2.235309   3.706015   0.795246   0.890133   0.710962   0.505690   \nmax     3.026351   5.757985   0.888340   0.943058   0.834707   0.696736   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.308468  47.301260  -0.041032  -0.068766  \nstd     0.062994   9.997578   0.036220   0.072472  \nmin     0.194099  31.988180  -0.094267  -0.236790  \n25%     0.259407  38.336892  -0.074615  -0.102293  \n50%     0.303302  50.013152  -0.037441  -0.057775  \n75%     0.363768  55.542475  -0.021546  -0.025043  \nmax     0.404708  60.821646   0.054468   0.089767  \n\n\n\n\nEvaluate on Mollisols\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\nseeds = range(20)\norder = 1\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_global_mollisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                    order=order,\n                                                    src_dir_model=src_dir_model)\n\nperfs_global_mollisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.082906\n      2.750063\n      0.767633\n      0.868226\n      0.441974\n      0.200178\n      0.217482\n      27.362061\n      0.006410\n      0.014927\n    \n    \n      std\n      0.105370\n      0.152068\n      0.022648\n      0.014038\n      0.071353\n      0.074478\n      0.011783\n      1.171648\n      0.014516\n      0.034156\n    \n    \n      min\n      1.910614\n      2.503716\n      0.725773\n      0.839147\n      0.351621\n      0.123637\n      0.198063\n      25.863001\n      -0.017837\n      -0.042497\n    \n    \n      25%\n      2.016213\n      2.645241\n      0.753751\n      0.858901\n      0.398954\n      0.159184\n      0.208970\n      26.442070\n      -0.002144\n      -0.005110\n    \n    \n      50%\n      2.073633\n      2.727649\n      0.767201\n      0.867132\n      0.434792\n      0.189089\n      0.217809\n      27.306423\n      0.006673\n      0.016067\n    \n    \n      75%\n      2.158195\n      2.803545\n      0.785071\n      0.877551\n      0.462096\n      0.213538\n      0.226259\n      27.765553\n      0.015423\n      0.036009\n    \n    \n      max\n      2.297545\n      3.113704\n      0.810361\n      0.892300\n      0.693749\n      0.481287\n      0.238283\n      30.162075\n      0.034060\n      0.082286\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nEvaluate on Gelisols\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\nseeds = range(20)\norder = 12\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_global_gelisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                   order = order,\n                                                   src_dir_model=src_dir_model)\n\nperfs_global_gelisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n    \n    \n      mean\n      2.064524\n      3.052019\n      0.742612\n      0.858711\n      0.584043\n      0.357354\n      0.308433\n      47.883342\n      -0.042954\n      -0.072261\n    \n    \n      std\n      0.339669\n      0.922599\n      0.076703\n      0.045312\n      0.131164\n      0.160157\n      0.061630\n      10.194859\n      0.037772\n      0.075571\n    \n    \n      min\n      1.563934\n      1.337414\n      0.579469\n      0.746647\n      0.378497\n      0.143260\n      0.194099\n      31.988180\n      -0.094267\n      -0.236790\n    \n    \n      25%\n      1.848496\n      2.458523\n      0.699683\n      0.841321\n      0.499613\n      0.249631\n      0.266246\n      38.717125\n      -0.076805\n      -0.105620\n    \n    \n      50%\n      2.025773\n      2.948530\n      0.749510\n      0.854638\n      0.568492\n      0.323183\n      0.303302\n      51.015234\n      -0.042409\n      -0.066190\n    \n    \n      75%\n      2.215420\n      3.613597\n      0.791696\n      0.889089\n      0.680335\n      0.464308\n      0.360686\n      55.561434\n      -0.021409\n      -0.028615\n    \n    \n      max\n      3.023628\n      4.793782\n      0.888340\n      0.943058\n      0.834707\n      0.696736\n      0.404708\n      60.821658\n      0.054468\n      0.089767\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nEvaluate on Vertisols\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\nseeds = range(20)\norder = 10\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_global_vertisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                   order = order,\n                                                   src_dir_model=src_dir_model)\n\nperfs_global_vertisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.039982\n      2.815862\n      0.744782\n      0.856862\n      0.273233\n      0.078001\n      0.173488\n      26.855653\n      -0.001182\n      -0.003823\n    \n    \n      std\n      0.271953\n      0.511288\n      0.063033\n      0.036968\n      0.059333\n      0.036126\n      0.021406\n      3.272256\n      0.020647\n      0.055384\n    \n    \n      min\n      1.577848\n      2.128196\n      0.592591\n      0.768764\n      0.182002\n      0.033125\n      0.138288\n      21.111539\n      -0.052648\n      -0.154459\n    \n    \n      25%\n      1.851439\n      2.431125\n      0.703943\n      0.837489\n      0.232396\n      0.054013\n      0.157317\n      24.537946\n      -0.011511\n      -0.028144\n    \n    \n      50%\n      1.992414\n      2.714563\n      0.744894\n      0.855618\n      0.253052\n      0.064036\n      0.173311\n      26.192354\n      -0.001295\n      -0.003575\n    \n    \n      75%\n      2.178701\n      3.148894\n      0.786316\n      0.881447\n      0.303549\n      0.092144\n      0.188509\n      28.037003\n      0.009025\n      0.019823\n    \n    \n      max\n      2.713312\n      4.022168\n      0.862141\n      0.923391\n      0.427089\n      0.182405\n      0.208709\n      33.903363\n      0.040271\n      0.099009\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\nTrain and test on Mollisols\n\n# Replace following Paths with yours\ndest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/losses')\ndest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/models')\n\norder = 1\nseeds = range(20)\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               dest_dir_loss=dest_dir_loss,\n               dest_dir_model=dest_dir_model,\n               n_epochs=n_epochs,\n               sc_kwargs=params_scheduler)\n\nStreaming output truncated to the last 5000 lines.\n 0.0254631  0.02506297 0.02542974]\n------------------------------\nEpoch: 148\nTraining loss: 0.01703691177108154 | Validation loss: 0.03745046781440233\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974]\n------------------------------\nEpoch: 149\nTraining loss: 0.01636921336912379 | Validation loss: 0.029602385222398\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974]\n------------------------------\nEpoch: 150\nTraining loss: 0.01618192989227115 | Validation loss: 0.02575919661542465\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 151\nTraining loss: 0.016052027107501518 | Validation loss: 0.02995985579387895\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 152\nTraining loss: 0.01630766587521957 | Validation loss: 0.036192602445853164\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 153\nTraining loss: 0.016913761476016774 | Validation loss: 0.043863068752247714\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 154\nTraining loss: 0.01821006020264966 | Validation loss: 0.028571049008389998\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 155\nTraining loss: 0.0200055970297176 | Validation loss: 0.03199511515940058\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 156\nTraining loss: 0.01871272486881638 | Validation loss: 0.0318490580238145\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 157\nTraining loss: 0.01749704931957685 | Validation loss: 0.04083131170221444\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 158\nTraining loss: 0.016811477638096834 | Validation loss: 0.036076820102231254\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 159\nTraining loss: 0.016043470830333476 | Validation loss: 0.032022082824902286\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592 ]\n------------------------------\nEpoch: 160\nTraining loss: 0.01610600998130988 | Validation loss: 0.024903937371383453\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 161\nTraining loss: 0.015034414651062415 | Validation loss: 0.02793283073295807\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 162\nTraining loss: 0.01557806123010054 | Validation loss: 0.028809203798400945\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 163\nTraining loss: 0.01631182431984617 | Validation loss: 0.02934042409319302\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 164\nTraining loss: 0.017696567615304064 | Validation loss: 0.04410623669110496\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 165\nTraining loss: 0.0189071940410198 | Validation loss: 0.05060041310458348\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 166\nTraining loss: 0.018668438469496916 | Validation loss: 0.030868354253470898\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 167\nTraining loss: 0.017217437183598475 | Validation loss: 0.04276854185194805\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 168\nTraining loss: 0.016072474085554785 | Validation loss: 0.037263108963339495\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 169\nTraining loss: 0.015596854644922577 | Validation loss: 0.02746792192217605\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n------------------------------\nEpoch: 170\nTraining loss: 0.015756507012612966 | Validation loss: 0.025396741316493214\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 171\nTraining loss: 0.01487494743220052 | Validation loss: 0.02894811505644486\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 172\nTraining loss: 0.014975935009745311 | Validation loss: 0.034921294720522286\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 173\nTraining loss: 0.01575764608664476 | Validation loss: 0.035113322580682824\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 174\nTraining loss: 0.01644058152272993 | Validation loss: 0.041267819253021275\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 175\nTraining loss: 0.018470817106794945 | Validation loss: 0.05743228978124158\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 176\nTraining loss: 0.01725016314802425 | Validation loss: 0.028639680866537422\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 177\nTraining loss: 0.016041831985800243 | Validation loss: 0.0426863385685559\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 178\nTraining loss: 0.014942902400709536 | Validation loss: 0.03376636066441906\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 179\nTraining loss: 0.015016029911990069 | Validation loss: 0.02791190057479102\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n------------------------------\nEpoch: 180\nTraining loss: 0.015328187172358133 | Validation loss: 0.024651944123465438\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 181\nTraining loss: 0.014163121622892058 | Validation loss: 0.029626857617805744\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 182\nTraining loss: 0.01449333244212428 | Validation loss: 0.02998114056114493\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 183\nTraining loss: 0.015647320470259504 | Validation loss: 0.042618878442665625\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 184\nTraining loss: 0.01614186215043372 | Validation loss: 0.029807858361766255\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 185\nTraining loss: 0.01824572786346686 | Validation loss: 0.03824911529904809\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 186\nTraining loss: 0.01688562039171859 | Validation loss: 0.03462533713800126\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 187\nTraining loss: 0.015406720159689382 | Validation loss: 0.027114455973536802\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 188\nTraining loss: 0.015270027453650017 | Validation loss: 0.03875390183309029\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 189\nTraining loss: 0.014476752381923856 | Validation loss: 0.028192470909963394\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194]\n------------------------------\nEpoch: 190\nTraining loss: 0.015035120354091026 | Validation loss: 0.024417148613981133\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 191\nTraining loss: 0.01392070555641335 | Validation loss: 0.027935292764470494\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 192\nTraining loss: 0.014260541837738484 | Validation loss: 0.038586160296510005\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 193\nTraining loss: 0.015028002004766342 | Validation loss: 0.03351751615389668\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 194\nTraining loss: 0.015989612158844056 | Validation loss: 0.06615991421557706\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 195\nTraining loss: 0.017272643091119064 | Validation loss: 0.04459637872360904\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 196\nTraining loss: 0.01664832424638527 | Validation loss: 0.04218823045235256\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 197\nTraining loss: 0.015222868038227363 | Validation loss: 0.04500981189053634\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 198\nTraining loss: 0.014712775491025983 | Validation loss: 0.03708768857578779\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 199\nTraining loss: 0.014625412261835774 | Validation loss: 0.028367176780412937\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715]\n------------------------------\nEpoch: 200\nTraining loss: 0.014733538981907221 | Validation loss: 0.024383184050434624\nValidation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n 0.02465194 0.02441715 0.02438318]\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.23029391500886862 | Validation loss: 0.2263513808803899\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 1\nTraining loss: 0.1298492884460261 | Validation loss: 0.07065250990646225\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 2\nTraining loss: 0.06877072449258673 | Validation loss: 0.06425394796367202\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 3\nTraining loss: 0.061665143847950105 | Validation loss: 0.06861826524670635\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 4\nTraining loss: 0.058033256072217855 | Validation loss: 0.0625213146475809\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 5\nTraining loss: 0.055559850668882936 | Validation loss: 0.08042318240872451\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 6\nTraining loss: 0.0519350595560258 | Validation loss: 0.04697381053119898\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 7\nTraining loss: 0.048388288273074766 | Validation loss: 0.11100427699940545\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 8\nTraining loss: 0.04608837391696567 | Validation loss: 0.07308738904872111\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 9\nTraining loss: 0.04391489770051425 | Validation loss: 0.10111017578414508\nValidation loss (ends of cycles): [0.22635138]\n------------------------------\nEpoch: 10\nTraining loss: 0.0414222451229769 | Validation loss: 0.04029037490753191\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 11\nTraining loss: 0.04222617237412227 | Validation loss: 0.05305742405887161\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 12\nTraining loss: 0.042907760452055105 | Validation loss: 0.04166700045711228\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 13\nTraining loss: 0.043801927363606004 | Validation loss: 0.11412931340081352\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 14\nTraining loss: 0.04472829210685521 | Validation loss: 0.05646906566939184\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 15\nTraining loss: 0.046035228539409674 | Validation loss: 0.04421177878975868\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 16\nTraining loss: 0.043207717533185475 | Validation loss: 0.042158282095832486\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 17\nTraining loss: 0.040840370588125736 | Validation loss: 0.038402439294649024\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 18\nTraining loss: 0.03911800839279483 | Validation loss: 0.042062657086976936\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 19\nTraining loss: 0.03648519006792486 | Validation loss: 0.03675581481573837\nValidation loss (ends of cycles): [0.22635138 0.04029037]\n------------------------------\nEpoch: 20\nTraining loss: 0.034782703212515365 | Validation loss: 0.03432650278721537\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.035905772971549656 | Validation loss: 0.03975177935457656\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.037213405435223404 | Validation loss: 0.03562755955915366\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.03865513603043992 | Validation loss: 0.11057296767830849\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.03931918034587449 | Validation loss: 0.0369640770368278\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.039821294779942285 | Validation loss: 0.045848207348691564\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.038668711453948804 | Validation loss: 0.042994737558599026\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.03670025159551845 | Validation loss: 0.04410590357812388\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.03455162040389529 | Validation loss: 0.034312766577516286\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.032675492149452125 | Validation loss: 0.03203583408945373\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.03106907684719417 | Validation loss: 0.03267500549554825\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 31\nTraining loss: 0.03156270837502145 | Validation loss: 0.03273458672421319\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 32\nTraining loss: 0.03294173397532687 | Validation loss: 0.03579591427530561\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 33\nTraining loss: 0.03416234837481525 | Validation loss: 0.03482184831851295\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 34\nTraining loss: 0.03490711622909317 | Validation loss: 0.050317411577062945\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 35\nTraining loss: 0.03727862610655829 | Validation loss: 0.03666511264496616\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 36\nTraining loss: 0.03482806728774212 | Validation loss: 0.04250362701714039\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 37\nTraining loss: 0.0331079214924901 | Validation loss: 0.036069130791085105\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 38\nTraining loss: 0.031223616937584266 | Validation loss: 0.03177707828581333\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 39\nTraining loss: 0.029834562658930454 | Validation loss: 0.03063737727435572\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n------------------------------\nEpoch: 40\nTraining loss: 0.027999205529932084 | Validation loss: 0.030917407213045017\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 41\nTraining loss: 0.028703854284892843 | Validation loss: 0.029636949034673826\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 42\nTraining loss: 0.029869006272799117 | Validation loss: 0.033834958083129356\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 43\nTraining loss: 0.031108734364492623 | Validation loss: 0.03308493041965578\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 44\nTraining loss: 0.03213380325615891 | Validation loss: 0.03766348745141711\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 45\nTraining loss: 0.03385435201121661 | Validation loss: 0.06962902418204717\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 46\nTraining loss: 0.032608688203239346 | Validation loss: 0.03770013412992869\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 47\nTraining loss: 0.03003532220268758 | Validation loss: 0.03331893762307508\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 48\nTraining loss: 0.029064043122154427 | Validation loss: 0.030430115326972946\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 49\nTraining loss: 0.027540825190007445 | Validation loss: 0.03614342541966055\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n------------------------------\nEpoch: 50\nTraining loss: 0.02594501869109406 | Validation loss: 0.02946952005316104\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 51\nTraining loss: 0.026418998663321258 | Validation loss: 0.02940933160217745\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 52\nTraining loss: 0.027487545262840463 | Validation loss: 0.031994336284697056\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 53\nTraining loss: 0.02819501824025822 | Validation loss: 0.03246153142702367\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 54\nTraining loss: 0.029957246288990345 | Validation loss: 0.04025745957291552\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 55\nTraining loss: 0.03173960026020441 | Validation loss: 0.036120299715548754\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 56\nTraining loss: 0.029832631266274588 | Validation loss: 0.038290795271417925\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 57\nTraining loss: 0.02817412253991678 | Validation loss: 0.03558004015524473\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 58\nTraining loss: 0.026782008531556382 | Validation loss: 0.03356315528175661\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 59\nTraining loss: 0.0255094195689582 | Validation loss: 0.035678128977971416\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n------------------------------\nEpoch: 60\nTraining loss: 0.024149992295003277 | Validation loss: 0.029395997058600187\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 61\nTraining loss: 0.024749841983997968 | Validation loss: 0.033817525553916185\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 62\nTraining loss: 0.025393155030527612 | Validation loss: 0.030692029611340592\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 63\nTraining loss: 0.02652351208016034 | Validation loss: 0.03221092772270952\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 64\nTraining loss: 0.0284297236192184 | Validation loss: 0.05041489251224058\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 65\nTraining loss: 0.029818793497525337 | Validation loss: 0.05839175677725247\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 66\nTraining loss: 0.0283982064877826 | Validation loss: 0.035961875425917764\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 67\nTraining loss: 0.026623119829161986 | Validation loss: 0.031139185119952475\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 68\nTraining loss: 0.025278138358342816 | Validation loss: 0.028934034053236246\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 69\nTraining loss: 0.024122276865854497 | Validation loss: 0.03136570605316332\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396  ]\n------------------------------\nEpoch: 70\nTraining loss: 0.02287417855227321 | Validation loss: 0.030645103260342563\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 71\nTraining loss: 0.023657108806603686 | Validation loss: 0.03877665574795434\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 72\nTraining loss: 0.02401751356305388 | Validation loss: 0.03245842074310141\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 73\nTraining loss: 0.0250010577000193 | Validation loss: 0.031518987978675535\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 74\nTraining loss: 0.0268116102750769 | Validation loss: 0.03552219265007547\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 75\nTraining loss: 0.0279629380249123 | Validation loss: 0.030541598929890564\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 76\nTraining loss: 0.026884139392418953 | Validation loss: 0.040323719648378234\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 77\nTraining loss: 0.025402794497435897 | Validation loss: 0.030501875061807886\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 78\nTraining loss: 0.024031604201025594 | Validation loss: 0.029525416969720806\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 79\nTraining loss: 0.02290574425634572 | Validation loss: 0.03327698606465544\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451 ]\n------------------------------\nEpoch: 80\nTraining loss: 0.02188899334921403 | Validation loss: 0.02941775681184871\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 81\nTraining loss: 0.022578422642668815 | Validation loss: 0.03252738980310304\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 82\nTraining loss: 0.02271142152395493 | Validation loss: 0.029476982774212956\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 83\nTraining loss: 0.02377902618722945 | Validation loss: 0.028569937317765186\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 84\nTraining loss: 0.025357732032539278 | Validation loss: 0.029860973424677337\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 85\nTraining loss: 0.02673468557262142 | Validation loss: 0.030372924005080546\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 86\nTraining loss: 0.025845129039077982 | Validation loss: 0.030393184827906743\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 87\nTraining loss: 0.024658304374150144 | Validation loss: 0.03845536522567272\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 88\nTraining loss: 0.023075178920526088 | Validation loss: 0.04256981957171645\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 89\nTraining loss: 0.022049285026600328 | Validation loss: 0.02848286697241877\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776]\n------------------------------\nEpoch: 90\nTraining loss: 0.021476434740593763 | Validation loss: 0.027760279697499106\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 91\nTraining loss: 0.021816314943891957 | Validation loss: 0.03098618079509054\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 92\nTraining loss: 0.022551793093997533 | Validation loss: 0.028238401216055666\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 93\nTraining loss: 0.02308060474527197 | Validation loss: 0.058629569464496205\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 94\nTraining loss: 0.02402396142725053 | Validation loss: 0.03021541171308075\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 95\nTraining loss: 0.025654969449208035 | Validation loss: 0.031186954717018774\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 96\nTraining loss: 0.024714468421823368 | Validation loss: 0.03125418509755816\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 97\nTraining loss: 0.023940351401735855 | Validation loss: 0.03264554476897631\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 98\nTraining loss: 0.022440335754971435 | Validation loss: 0.028461951529607177\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 99\nTraining loss: 0.021251822916442543 | Validation loss: 0.02842777268961072\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028]\n------------------------------\nEpoch: 100\nTraining loss: 0.020347900972588033 | Validation loss: 0.028554076927581003\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 101\nTraining loss: 0.020555462850999785 | Validation loss: 0.02733869418235762\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 102\nTraining loss: 0.021171740918776126 | Validation loss: 0.029619899511869465\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 103\nTraining loss: 0.02220173811585438 | Validation loss: 0.0321574957509126\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 104\nTraining loss: 0.023414065705506297 | Validation loss: 0.044605502991804054\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 105\nTraining loss: 0.024443574798694714 | Validation loss: 0.07190789042838983\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 106\nTraining loss: 0.023700900576493846 | Validation loss: 0.03251718570079122\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 107\nTraining loss: 0.02247146178960679 | Validation loss: 0.0362437991425395\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 108\nTraining loss: 0.022056101961439947 | Validation loss: 0.0278464819171599\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 109\nTraining loss: 0.020700616911583678 | Validation loss: 0.028671946642654284\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n------------------------------\nEpoch: 110\nTraining loss: 0.01958366129282347 | Validation loss: 0.027183285082823465\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 111\nTraining loss: 0.019975665894268854 | Validation loss: 0.028586781550464884\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 112\nTraining loss: 0.020705172235555038 | Validation loss: 0.03128787662301745\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 113\nTraining loss: 0.02107667978436542 | Validation loss: 0.030051063679690872\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 114\nTraining loss: 0.022610661867112528 | Validation loss: 0.04852165055594274\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 115\nTraining loss: 0.023561506458308276 | Validation loss: 0.06899371064667191\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 116\nTraining loss: 0.0233066179448875 | Validation loss: 0.04566473427361676\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 117\nTraining loss: 0.021634616137565513 | Validation loss: 0.04841635775353227\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 118\nTraining loss: 0.020792985552679596 | Validation loss: 0.03520082090316074\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 119\nTraining loss: 0.019773167533406275 | Validation loss: 0.03094479950544025\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n------------------------------\nEpoch: 120\nTraining loss: 0.01904741757950647 | Validation loss: 0.026810297376609275\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 121\nTraining loss: 0.019134292223801214 | Validation loss: 0.02902802246223603\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 122\nTraining loss: 0.02028068473820037 | Validation loss: 0.03634647925251296\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 123\nTraining loss: 0.021072582406090286 | Validation loss: 0.040620867628604174\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 124\nTraining loss: 0.022031776501032396 | Validation loss: 0.06249447592667171\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 125\nTraining loss: 0.023192723667839678 | Validation loss: 0.03574155330924051\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 126\nTraining loss: 0.02227437675635262 | Validation loss: 0.07030766138008662\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 127\nTraining loss: 0.020985245057268113 | Validation loss: 0.03414160332509449\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 128\nTraining loss: 0.019905526866772917 | Validation loss: 0.033079460595867465\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 129\nTraining loss: 0.01939150384756002 | Validation loss: 0.029185153178072402\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103 ]\n------------------------------\nEpoch: 130\nTraining loss: 0.01826417470366005 | Validation loss: 0.026967533764296343\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 131\nTraining loss: 0.018468463992154818 | Validation loss: 0.031554549100941846\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 132\nTraining loss: 0.019273764086599515 | Validation loss: 0.033024010847189596\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 133\nTraining loss: 0.019958532049994523 | Validation loss: 0.030514458859605447\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 134\nTraining loss: 0.02119076339626397 | Validation loss: 0.03831826276811106\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 135\nTraining loss: 0.02249043524583302 | Validation loss: 0.07168984226882458\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 136\nTraining loss: 0.021602770646776612 | Validation loss: 0.03142295591533184\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 137\nTraining loss: 0.02026850242182855 | Validation loss: 0.035348252459828346\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 138\nTraining loss: 0.01939214747474809 | Validation loss: 0.030120848145868098\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 139\nTraining loss: 0.018289907337582813 | Validation loss: 0.029019300393494114\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753]\n------------------------------\nEpoch: 140\nTraining loss: 0.01761306544038944 | Validation loss: 0.026833688773747002\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 141\nTraining loss: 0.017443752841933106 | Validation loss: 0.028782523203907267\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 142\nTraining loss: 0.018654791837012987 | Validation loss: 0.028098878982876028\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 143\nTraining loss: 0.019123788587976157 | Validation loss: 0.03758659366784351\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 144\nTraining loss: 0.020131324921409045 | Validation loss: 0.029096519175384725\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 145\nTraining loss: 0.02155784703980435 | Validation loss: 0.03155151415350182\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 146\nTraining loss: 0.020959843873856514 | Validation loss: 0.058786045626870224\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 147\nTraining loss: 0.019591272861613492 | Validation loss: 0.03170138590836099\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 148\nTraining loss: 0.01901846544673227 | Validation loss: 0.027040495770052075\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 149\nTraining loss: 0.017848043925908764 | Validation loss: 0.030091944117365137\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369]\n------------------------------\nEpoch: 150\nTraining loss: 0.017326634074372006 | Validation loss: 0.026434293615498712\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 151\nTraining loss: 0.017380717077966387 | Validation loss: 0.029182153953505412\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 152\nTraining loss: 0.017231200886057403 | Validation loss: 0.030624900878007923\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 153\nTraining loss: 0.018560519161808297 | Validation loss: 0.03458159559938524\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 154\nTraining loss: 0.019787704303285213 | Validation loss: 0.05616758417870317\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 155\nTraining loss: 0.020979577028079004 | Validation loss: 0.06432282871433667\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 156\nTraining loss: 0.01990094185973389 | Validation loss: 0.028011672664433718\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 157\nTraining loss: 0.019052211199016348 | Validation loss: 0.03703709126317075\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 158\nTraining loss: 0.017911518242482733 | Validation loss: 0.03025994263589382\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 159\nTraining loss: 0.017369525834569723 | Validation loss: 0.02964053342917136\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429]\n------------------------------\nEpoch: 160\nTraining loss: 0.016482023953846316 | Validation loss: 0.025826098497158716\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 161\nTraining loss: 0.01659158083261937 | Validation loss: 0.02873330456869943\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 162\nTraining loss: 0.01691494110453026 | Validation loss: 0.027677631338260004\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 163\nTraining loss: 0.017991937094981352 | Validation loss: 0.04074185960260885\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 164\nTraining loss: 0.018723719999406155 | Validation loss: 0.030231072633926357\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 165\nTraining loss: 0.020276146350292172 | Validation loss: 0.043174420217318196\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 166\nTraining loss: 0.019531042235562714 | Validation loss: 0.03581643250903913\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 167\nTraining loss: 0.018447091965746832 | Validation loss: 0.044610560472522466\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 168\nTraining loss: 0.017366024497063544 | Validation loss: 0.028554132840197\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 169\nTraining loss: 0.016597856987055723 | Validation loss: 0.03181937616318464\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n------------------------------\nEpoch: 170\nTraining loss: 0.0163807039247538 | Validation loss: 0.02585350861772895\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 171\nTraining loss: 0.01622825662597893 | Validation loss: 0.03245220848891352\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 172\nTraining loss: 0.01662803715004063 | Validation loss: 0.02856071732406105\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 173\nTraining loss: 0.01692930633595925 | Validation loss: 0.030707208572753837\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 174\nTraining loss: 0.018463662787666167 | Validation loss: 0.03464765181498868\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 175\nTraining loss: 0.019447343614202264 | Validation loss: 0.03708033948870642\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 176\nTraining loss: 0.018903135850162404 | Validation loss: 0.03530201914587191\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 177\nTraining loss: 0.01786497042786966 | Validation loss: 0.03155623323151043\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 178\nTraining loss: 0.016593552877884207 | Validation loss: 0.028088790896747793\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 179\nTraining loss: 0.015996790239284558 | Validation loss: 0.030268458050808737\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n------------------------------\nEpoch: 180\nTraining loss: 0.015754348903182683 | Validation loss: 0.026033666616837894\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 181\nTraining loss: 0.015584597916804194 | Validation loss: 0.029790076526946256\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 182\nTraining loss: 0.016471456264986133 | Validation loss: 0.028614665381610394\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 183\nTraining loss: 0.01635930025987933 | Validation loss: 0.028159564627068385\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 184\nTraining loss: 0.01770466239933621 | Validation loss: 0.030318220911015357\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 185\nTraining loss: 0.019188743718801353 | Validation loss: 0.028221322023975\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 186\nTraining loss: 0.018213050498634697 | Validation loss: 0.03194981573947838\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 187\nTraining loss: 0.017296552865357116 | Validation loss: 0.040214699027793746\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 188\nTraining loss: 0.01603712801957821 | Validation loss: 0.03133211996672409\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 189\nTraining loss: 0.015969472931607102 | Validation loss: 0.029272598114662936\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367]\n------------------------------\nEpoch: 190\nTraining loss: 0.015087590197941697 | Validation loss: 0.024819848526801382\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 191\nTraining loss: 0.015174830080638451 | Validation loss: 0.03065409930422902\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 192\nTraining loss: 0.015296503244634203 | Validation loss: 0.031787786699299304\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 193\nTraining loss: 0.015783516960824286 | Validation loss: 0.03863530486289944\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 194\nTraining loss: 0.017338460010317403 | Validation loss: 0.027448586221518263\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 195\nTraining loss: 0.018564120671386276 | Validation loss: 0.032875948186431615\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 196\nTraining loss: 0.0181109133248437 | Validation loss: 0.03741413594356605\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 197\nTraining loss: 0.016321961830438273 | Validation loss: 0.03071831968346877\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 198\nTraining loss: 0.015929741894720288 | Validation loss: 0.0271761506529791\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 199\nTraining loss: 0.015383159098162399 | Validation loss: 0.026942191678764566\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985]\n------------------------------\nEpoch: 200\nTraining loss: 0.014515927291429805 | Validation loss: 0.024746225814202\nValidation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n 0.02603367 0.02481985 0.02474623]\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2637900622629444 | Validation loss: 0.2553698234260082\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 1\nTraining loss: 0.16978094037546804 | Validation loss: 0.08061320481023618\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 2\nTraining loss: 0.07621631738145342 | Validation loss: 0.0826724971245442\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 3\nTraining loss: 0.061927354770150746 | Validation loss: 0.15350222560976232\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 4\nTraining loss: 0.05844452919114215 | Validation loss: 0.18728351167270116\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 5\nTraining loss: 0.055577036785150344 | Validation loss: 0.061056972414787324\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 6\nTraining loss: 0.05109959298948407 | Validation loss: 0.05445564238886748\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 7\nTraining loss: 0.04711966514059527 | Validation loss: 0.05329546033005629\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 8\nTraining loss: 0.044755841250934826 | Validation loss: 0.049413079262844155\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 9\nTraining loss: 0.04260337657412054 | Validation loss: 0.04562658929665174\nValidation loss (ends of cycles): [0.25536982]\n------------------------------\nEpoch: 10\nTraining loss: 0.040551968409042606 | Validation loss: 0.042534173532788246\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 11\nTraining loss: 0.04085618404196462 | Validation loss: 0.042740331896181614\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 12\nTraining loss: 0.04147623869616855 | Validation loss: 0.04740859594728265\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 13\nTraining loss: 0.04226049646614534 | Validation loss: 0.06226673981707011\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 14\nTraining loss: 0.04316315458340925 | Validation loss: 0.0443901874324573\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 15\nTraining loss: 0.04279391495850284 | Validation loss: 0.060327282308467796\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 16\nTraining loss: 0.040710862269738184 | Validation loss: 0.05306545631693942\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 17\nTraining loss: 0.0389511743522728 | Validation loss: 0.03878095995501748\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 18\nTraining loss: 0.03631727450606432 | Validation loss: 0.03926701827107796\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 19\nTraining loss: 0.03461150976599228 | Validation loss: 0.035595839111400504\nValidation loss (ends of cycles): [0.25536982 0.04253417]\n------------------------------\nEpoch: 20\nTraining loss: 0.03270029825976624 | Validation loss: 0.034391016932204366\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 21\nTraining loss: 0.032733950070162054 | Validation loss: 0.033897070114367774\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 22\nTraining loss: 0.03373902814409994 | Validation loss: 0.03615709903117802\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 23\nTraining loss: 0.035701153863990594 | Validation loss: 0.04238018014335206\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 24\nTraining loss: 0.03666398287769633 | Validation loss: 0.053025079325639775\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 25\nTraining loss: 0.03761743522637528 | Validation loss: 0.040236693840207796\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 26\nTraining loss: 0.03550851068681913 | Validation loss: 0.03966777375899255\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 27\nTraining loss: 0.03362350801179404 | Validation loss: 0.03458143038941281\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 28\nTraining loss: 0.03134352446492264 | Validation loss: 0.03413207765801677\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 29\nTraining loss: 0.03009785374679305 | Validation loss: 0.039865221961268356\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n------------------------------\nEpoch: 30\nTraining loss: 0.02849840317373937 | Validation loss: 0.031089227979204485\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 31\nTraining loss: 0.02911056212794322 | Validation loss: 0.03221330076589116\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 32\nTraining loss: 0.030087379848034033 | Validation loss: 0.03465819009579718\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 33\nTraining loss: 0.03126076528960876 | Validation loss: 0.034185842610895634\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 34\nTraining loss: 0.03246060802190289 | Validation loss: 0.03449801728129387\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 35\nTraining loss: 0.0336248841860637 | Validation loss: 0.053725846244820526\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 36\nTraining loss: 0.031415612436831 | Validation loss: 0.037223817381475656\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 37\nTraining loss: 0.030353294953191088 | Validation loss: 0.04098063687394772\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 38\nTraining loss: 0.028800708188973217 | Validation loss: 0.052007128085408895\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 39\nTraining loss: 0.02750550863532885 | Validation loss: 0.03739549871534109\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n------------------------------\nEpoch: 40\nTraining loss: 0.02563297042721196 | Validation loss: 0.02868142564381872\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 41\nTraining loss: 0.02616311686852502 | Validation loss: 0.03076405274415655\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 42\nTraining loss: 0.027380637066657484 | Validation loss: 0.03508420488131898\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 43\nTraining loss: 0.02831252516711108 | Validation loss: 0.04598719187613044\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 44\nTraining loss: 0.029504929771005866 | Validation loss: 0.03175034220995648\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 45\nTraining loss: 0.03159959042663516 | Validation loss: 0.07434647623449564\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 46\nTraining loss: 0.030014191938918612 | Validation loss: 0.033090820303186774\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 47\nTraining loss: 0.02731679374284228 | Validation loss: 0.04520116526899593\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 48\nTraining loss: 0.0262628064155277 | Validation loss: 0.034826266346499324\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 49\nTraining loss: 0.024662429327333747 | Validation loss: 0.036349371408245394\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n------------------------------\nEpoch: 50\nTraining loss: 0.02363490600983503 | Validation loss: 0.02791736850381962\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 51\nTraining loss: 0.023922254276737148 | Validation loss: 0.03908925157572542\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 52\nTraining loss: 0.025039831698619522 | Validation loss: 0.038353501952120235\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 53\nTraining loss: 0.0258442997924893 | Validation loss: 0.039914444155458896\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 54\nTraining loss: 0.027072010339362178 | Validation loss: 0.058511382301471064\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 55\nTraining loss: 0.028190431893927607 | Validation loss: 0.04542659169861248\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 56\nTraining loss: 0.027136848568313034 | Validation loss: 0.035249374906665513\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 57\nTraining loss: 0.026047875482573923 | Validation loss: 0.04109857104984777\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 58\nTraining loss: 0.02399788330863362 | Validation loss: 0.038921072048002055\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 59\nTraining loss: 0.022918796129072244 | Validation loss: 0.03578687781867172\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n------------------------------\nEpoch: 60\nTraining loss: 0.02215754752413102 | Validation loss: 0.027188095430444394\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 61\nTraining loss: 0.022274650759648094 | Validation loss: 0.03344160255177745\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 62\nTraining loss: 0.02273730670482765 | Validation loss: 0.040124644458826096\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 63\nTraining loss: 0.02412148073375949 | Validation loss: 0.049811005658869235\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 64\nTraining loss: 0.025627500584974944 | Validation loss: 0.040126007582460134\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 65\nTraining loss: 0.027548986309875362 | Validation loss: 0.03670580784923264\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 66\nTraining loss: 0.025487642145153845 | Validation loss: 0.04663573281972536\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 67\nTraining loss: 0.02363148095606551 | Validation loss: 0.043784839500273974\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 68\nTraining loss: 0.022655583384563686 | Validation loss: 0.052880666351744106\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 69\nTraining loss: 0.021290708928998665 | Validation loss: 0.03772427241450974\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881 ]\n------------------------------\nEpoch: 70\nTraining loss: 0.020642276361267937 | Validation loss: 0.026211059918361052\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 71\nTraining loss: 0.020620109798077508 | Validation loss: 0.03342627454549074\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 72\nTraining loss: 0.021390452662384825 | Validation loss: 0.04179773193651012\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 73\nTraining loss: 0.022596948584051507 | Validation loss: 0.05714830097609332\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 74\nTraining loss: 0.023441489446561346 | Validation loss: 0.06146918742784432\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 75\nTraining loss: 0.025943865520721265 | Validation loss: 0.07486197179449457\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 76\nTraining loss: 0.02424021406091659 | Validation loss: 0.03962666341768844\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 77\nTraining loss: 0.022813132383411955 | Validation loss: 0.0878413732030562\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 78\nTraining loss: 0.0214004504234202 | Validation loss: 0.036098147343311994\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 79\nTraining loss: 0.020235511447917593 | Validation loss: 0.030150488950312138\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106]\n------------------------------\nEpoch: 80\nTraining loss: 0.019402427983428786 | Validation loss: 0.025317091960459948\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 81\nTraining loss: 0.019609720393381862 | Validation loss: 0.033331186211268814\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 82\nTraining loss: 0.020388572548444454 | Validation loss: 0.046018143982759545\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 83\nTraining loss: 0.021280125978776078 | Validation loss: 0.04034127773983138\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 84\nTraining loss: 0.022711353089946967 | Validation loss: 0.03409364766308239\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 85\nTraining loss: 0.02453193292097162 | Validation loss: 0.08150071637438876\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 86\nTraining loss: 0.02312512177717589 | Validation loss: 0.050399243499019315\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 87\nTraining loss: 0.021199130441857734 | Validation loss: 0.04424859756337745\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 88\nTraining loss: 0.02017700191217697 | Validation loss: 0.04524432375494923\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 89\nTraining loss: 0.019404342235914366 | Validation loss: 0.0321854111805026\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709]\n------------------------------\nEpoch: 90\nTraining loss: 0.01875441001826211 | Validation loss: 0.024785825766489973\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 91\nTraining loss: 0.018730865393648384 | Validation loss: 0.02874932675955019\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 92\nTraining loss: 0.01903830907545109 | Validation loss: 0.049793892606560676\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 93\nTraining loss: 0.019815182877018264 | Validation loss: 0.03450491282689784\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 94\nTraining loss: 0.02128261590549941 | Validation loss: 0.03380642605147192\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 95\nTraining loss: 0.023460421202183977 | Validation loss: 0.04820313796933208\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 96\nTraining loss: 0.022719959653009048 | Validation loss: 0.04494413600436279\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 97\nTraining loss: 0.020536305857935415 | Validation loss: 0.07395922485738993\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 98\nTraining loss: 0.019407542622252274 | Validation loss: 0.04475157501708184\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 99\nTraining loss: 0.018422949634990107 | Validation loss: 0.029646523080633154\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583]\n------------------------------\nEpoch: 100\nTraining loss: 0.01798046646481342 | Validation loss: 0.02428950648754835\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 101\nTraining loss: 0.01773889101702373 | Validation loss: 0.026878266878026937\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 102\nTraining loss: 0.017975333689708217 | Validation loss: 0.0366562896940325\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 103\nTraining loss: 0.01922888393223527 | Validation loss: 0.03991323919035494\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 104\nTraining loss: 0.021070435833608212 | Validation loss: 0.02945878236953701\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 105\nTraining loss: 0.022624185305043513 | Validation loss: 0.039455125973160775\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 106\nTraining loss: 0.021214887884404014 | Validation loss: 0.04384329728782177\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 107\nTraining loss: 0.019580216475042253 | Validation loss: 0.033993630370657356\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 108\nTraining loss: 0.01824560266568774 | Validation loss: 0.034660790281902464\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 109\nTraining loss: 0.017534620688496694 | Validation loss: 0.02668355663107442\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n------------------------------\nEpoch: 110\nTraining loss: 0.017282390727582368 | Validation loss: 0.023429298324377408\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 111\nTraining loss: 0.017200304728815792 | Validation loss: 0.028306924737989902\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 112\nTraining loss: 0.017457029978303534 | Validation loss: 0.039004568476229906\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 113\nTraining loss: 0.018181345012672396 | Validation loss: 0.037324963070984395\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 114\nTraining loss: 0.019610544768452403 | Validation loss: 0.05263229812096272\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 115\nTraining loss: 0.021719489280541657 | Validation loss: 0.10329486296645232\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 116\nTraining loss: 0.01992288611731247 | Validation loss: 0.03785114236442106\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 117\nTraining loss: 0.018402819861073484 | Validation loss: 0.037453519978693554\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 118\nTraining loss: 0.017824132010521677 | Validation loss: 0.04232800778533731\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 119\nTraining loss: 0.016857583978805345 | Validation loss: 0.027738639802139784\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n------------------------------\nEpoch: 120\nTraining loss: 0.016807740439212154 | Validation loss: 0.02330138411239854\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 121\nTraining loss: 0.01654348725845155 | Validation loss: 0.026412564745571996\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 122\nTraining loss: 0.016701313612326556 | Validation loss: 0.040867413128060956\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 123\nTraining loss: 0.017480136147937794 | Validation loss: 0.04603183016713176\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 124\nTraining loss: 0.01918658996302529 | Validation loss: 0.06513113408748593\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 125\nTraining loss: 0.02093221848973861 | Validation loss: 0.03038291127554008\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 126\nTraining loss: 0.019247326766129447 | Validation loss: 0.02680572501516768\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 127\nTraining loss: 0.018294873881165075 | Validation loss: 0.03915104289938297\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 128\nTraining loss: 0.017199005021031208 | Validation loss: 0.03470036167917507\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 129\nTraining loss: 0.01629751831903933 | Validation loss: 0.027707418227302178\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138]\n------------------------------\nEpoch: 130\nTraining loss: 0.016322553382153333 | Validation loss: 0.023030078088465546\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 131\nTraining loss: 0.01574328486517313 | Validation loss: 0.025941874748761102\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 132\nTraining loss: 0.01595830355736951 | Validation loss: 0.026795019695003117\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 133\nTraining loss: 0.01674997114278527 | Validation loss: 0.032723173864984086\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 134\nTraining loss: 0.018060487297246693 | Validation loss: 0.05259248175259147\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 135\nTraining loss: 0.019997143852128554 | Validation loss: 0.029469374334439635\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 136\nTraining loss: 0.018740022800078516 | Validation loss: 0.03724197328223714\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 137\nTraining loss: 0.017349006890918804 | Validation loss: 0.03206472541205585\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 138\nTraining loss: 0.016172322676505757 | Validation loss: 0.031014696528602923\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 139\nTraining loss: 0.015613548719674832 | Validation loss: 0.02622273386389549\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008]\n------------------------------\nEpoch: 140\nTraining loss: 0.015545637761563183 | Validation loss: 0.02298504370264709\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 141\nTraining loss: 0.015161730000289225 | Validation loss: 0.025938816684564308\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 142\nTraining loss: 0.015637453324972135 | Validation loss: 0.031124545161479285\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 143\nTraining loss: 0.016111005887555086 | Validation loss: 0.030322041545462395\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 144\nTraining loss: 0.017304543302081494 | Validation loss: 0.030219923222570548\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 145\nTraining loss: 0.01862924370673383 | Validation loss: 0.037004510373143215\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 146\nTraining loss: 0.01818571688403726 | Validation loss: 0.031312630412035754\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 147\nTraining loss: 0.016826508900429676 | Validation loss: 0.025547978468239307\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 148\nTraining loss: 0.015718223280103704 | Validation loss: 0.025384996768220196\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 149\nTraining loss: 0.01519297607250602 | Validation loss: 0.026305198087356985\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504]\n------------------------------\nEpoch: 150\nTraining loss: 0.015273088334799416 | Validation loss: 0.02278466535998242\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 151\nTraining loss: 0.014718375066936257 | Validation loss: 0.025838033728567616\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 152\nTraining loss: 0.015016126022240532 | Validation loss: 0.02520066301804036\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 153\nTraining loss: 0.015792525130199637 | Validation loss: 0.033811263440709026\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 154\nTraining loss: 0.016720541877302562 | Validation loss: 0.03242217247108264\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 155\nTraining loss: 0.01912484302370232 | Validation loss: 0.0287974347925878\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 156\nTraining loss: 0.017655496700451925 | Validation loss: 0.028920547171894993\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 157\nTraining loss: 0.016262264513158 | Validation loss: 0.026407693163491786\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 158\nTraining loss: 0.015085211046311537 | Validation loss: 0.026688831109952713\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 159\nTraining loss: 0.014528502254871282 | Validation loss: 0.026294793401445662\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467]\n------------------------------\nEpoch: 160\nTraining loss: 0.014659694655183778 | Validation loss: 0.022498133392738446\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 161\nTraining loss: 0.014347941379028896 | Validation loss: 0.026282045541198125\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 162\nTraining loss: 0.014263723311410379 | Validation loss: 0.03165137199019747\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 163\nTraining loss: 0.015251761050668625 | Validation loss: 0.027164413177940463\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 164\nTraining loss: 0.016272441059848677 | Validation loss: 0.029640018258110752\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 165\nTraining loss: 0.017680507535982107 | Validation loss: 0.029534945397504737\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 166\nTraining loss: 0.017353308862188326 | Validation loss: 0.02775626107385116\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 167\nTraining loss: 0.016023863398239982 | Validation loss: 0.02645636486288692\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 168\nTraining loss: 0.01442366958923849 | Validation loss: 0.02888286848818617\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 169\nTraining loss: 0.014458691623996989 | Validation loss: 0.027762371946924498\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n------------------------------\nEpoch: 170\nTraining loss: 0.014444907576690319 | Validation loss: 0.0224708960325058\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 171\nTraining loss: 0.013903563040938333 | Validation loss: 0.025974585957426046\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 172\nTraining loss: 0.014055204803976212 | Validation loss: 0.027087848567004715\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 173\nTraining loss: 0.014722083803635799 | Validation loss: 0.03538105211087635\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 174\nTraining loss: 0.015269574309559728 | Validation loss: 0.030134127608367374\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 175\nTraining loss: 0.01760749817686344 | Validation loss: 0.032296386281294484\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 176\nTraining loss: 0.016067436583310005 | Validation loss: 0.02902796821269606\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 177\nTraining loss: 0.015462717003667886 | Validation loss: 0.027272284696144716\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 178\nTraining loss: 0.014177237382829793 | Validation loss: 0.0292433856853417\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 179\nTraining loss: 0.014127726506381503 | Validation loss: 0.02626519976183772\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n------------------------------\nEpoch: 180\nTraining loss: 0.014095621309827576 | Validation loss: 0.02228838494712753\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 181\nTraining loss: 0.013186523555080418 | Validation loss: 0.026403546100482345\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 182\nTraining loss: 0.013458487034779087 | Validation loss: 0.03125196968072227\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 183\nTraining loss: 0.014144350702003369 | Validation loss: 0.027981760312936137\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 184\nTraining loss: 0.01527679824790642 | Validation loss: 0.02871349892978157\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 185\nTraining loss: 0.01699371934351832 | Validation loss: 0.03547387194287564\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 186\nTraining loss: 0.016371346983741892 | Validation loss: 0.029047775315120816\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 187\nTraining loss: 0.014853418538882304 | Validation loss: 0.029449129876281534\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 188\nTraining loss: 0.013668709351617073 | Validation loss: 0.02833327539597771\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 189\nTraining loss: 0.013487473279162337 | Validation loss: 0.028004308929666877\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838]\n------------------------------\nEpoch: 190\nTraining loss: 0.013926172044565562 | Validation loss: 0.021582843336675848\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 191\nTraining loss: 0.013011930078675269 | Validation loss: 0.025751842485208596\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 192\nTraining loss: 0.012926986992928512 | Validation loss: 0.027355525941987122\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 193\nTraining loss: 0.013586024041117927 | Validation loss: 0.030126292291762575\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 194\nTraining loss: 0.015417855717467996 | Validation loss: 0.030633447425706044\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 195\nTraining loss: 0.016156206774687477 | Validation loss: 0.03374442298497472\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 196\nTraining loss: 0.014926741387966553 | Validation loss: 0.02969645961586918\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 197\nTraining loss: 0.014366873232685482 | Validation loss: 0.028297341195866466\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 198\nTraining loss: 0.013488575995254975 | Validation loss: 0.02735566158246781\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 199\nTraining loss: 0.012988377337038576 | Validation loss: 0.024107716279104352\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284]\n------------------------------\nEpoch: 200\nTraining loss: 0.013492380542235697 | Validation loss: 0.021781244115637883\nValidation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n 0.02228838 0.02158284 0.02178124]\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.1930667787182088 | Validation loss: 0.1819245464823864\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 1\nTraining loss: 0.11809876634150135 | Validation loss: 0.07254008576273918\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 2\nTraining loss: 0.06877991482615471 | Validation loss: 0.0805306493960045\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 3\nTraining loss: 0.0631012045835354 | Validation loss: 0.05939289858495748\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 4\nTraining loss: 0.05970304113413606 | Validation loss: 0.056863684483148436\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 5\nTraining loss: 0.057145256518709414 | Validation loss: 0.052280418988731175\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 6\nTraining loss: 0.05306313728191415 | Validation loss: 0.046050683284799256\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 7\nTraining loss: 0.0496613971204782 | Validation loss: 0.04423446822221632\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 8\nTraining loss: 0.04634420350574109 | Validation loss: 0.04120242133460663\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 9\nTraining loss: 0.04390166444057713 | Validation loss: 0.03790110322060408\nValidation loss (ends of cycles): [0.18192455]\n------------------------------\nEpoch: 10\nTraining loss: 0.042118645292155595 | Validation loss: 0.03582969597644276\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.04284505806863308 | Validation loss: 0.03783127010144569\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.04320145487709313 | Validation loss: 0.03778853467493146\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.044070840353260234 | Validation loss: 0.03866708637387664\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.045151009517056605 | Validation loss: 0.0393032300527449\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.04501143620178408 | Validation loss: 0.0551495544474434\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.043016062821356615 | Validation loss: 0.07461784904201825\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.0403907396849625 | Validation loss: 0.036994416266679764\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.0384160772764257 | Validation loss: 0.03323371660102297\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.036099768840536776 | Validation loss: 0.03114642182158099\nValidation loss (ends of cycles): [0.18192455 0.0358297 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.034705798651034736 | Validation loss: 0.030475210260461877\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 21\nTraining loss: 0.03530205554347866 | Validation loss: 0.030978830186305224\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 22\nTraining loss: 0.03607280668327395 | Validation loss: 0.03204350987518275\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 23\nTraining loss: 0.03704883032763491 | Validation loss: 0.033623026132031726\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 24\nTraining loss: 0.03808547970743812 | Validation loss: 0.034693438559770584\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 25\nTraining loss: 0.0391657291474391 | Validation loss: 0.035492373906351904\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 26\nTraining loss: 0.03752170335881564 | Validation loss: 0.03386187332647818\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 27\nTraining loss: 0.035501095694394746 | Validation loss: 0.033731289483882765\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 28\nTraining loss: 0.03345001264266213 | Validation loss: 0.029232336277211154\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 29\nTraining loss: 0.03161927238106728 | Validation loss: 0.028818574630551867\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n------------------------------\nEpoch: 30\nTraining loss: 0.029917811959677812 | Validation loss: 0.02814174229624095\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 31\nTraining loss: 0.030526079264070305 | Validation loss: 0.028566454364745704\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 32\nTraining loss: 0.031387867964804174 | Validation loss: 0.028836151205555157\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 33\nTraining loss: 0.03292266800239378 | Validation loss: 0.03008065648652889\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 34\nTraining loss: 0.03430468309860753 | Validation loss: 0.030857506449575776\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 35\nTraining loss: 0.0357665854715267 | Validation loss: 0.031715808574248244\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 36\nTraining loss: 0.03369065793611261 | Validation loss: 0.033253426422123554\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 37\nTraining loss: 0.03193698752367375 | Validation loss: 0.030685135404820794\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 38\nTraining loss: 0.030100774111188187 | Validation loss: 0.0328261045118173\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 39\nTraining loss: 0.028577730930125226 | Validation loss: 0.027603525944330073\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n------------------------------\nEpoch: 40\nTraining loss: 0.027103183905080874 | Validation loss: 0.02621252317395475\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 41\nTraining loss: 0.02798980980047158 | Validation loss: 0.02808722888154012\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 42\nTraining loss: 0.02831293367685712 | Validation loss: 0.02703952934179041\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 43\nTraining loss: 0.03022744706166642 | Validation loss: 0.029000978502962325\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 44\nTraining loss: 0.031051456407472797 | Validation loss: 0.030444668605923653\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 45\nTraining loss: 0.032409838005444225 | Validation loss: 0.03240376365957437\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 46\nTraining loss: 0.0312417740116314 | Validation loss: 0.04385587483368538\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 47\nTraining loss: 0.0294837532641024 | Validation loss: 0.03135377789537112\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 48\nTraining loss: 0.027556763897288818 | Validation loss: 0.02746297750208113\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 49\nTraining loss: 0.025821912870267215 | Validation loss: 0.027316523577879975\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n------------------------------\nEpoch: 50\nTraining loss: 0.024773090705275537 | Validation loss: 0.02547023341887527\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 51\nTraining loss: 0.0251947464079273 | Validation loss: 0.026439772573886095\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 52\nTraining loss: 0.026060407675270524 | Validation loss: 0.030076515550414722\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 53\nTraining loss: 0.02717079576485011 | Validation loss: 0.03210321586165163\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 54\nTraining loss: 0.0285390988296392 | Validation loss: 0.030368766023053065\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 55\nTraining loss: 0.030521390408429563 | Validation loss: 0.029636258983777627\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 56\nTraining loss: 0.029085740456547663 | Validation loss: 0.03383835812133772\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 57\nTraining loss: 0.02747641666887366 | Validation loss: 0.029344955597210814\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 58\nTraining loss: 0.025737719032533315 | Validation loss: 0.032367387855494464\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 59\nTraining loss: 0.024154662392197216 | Validation loss: 0.026446550946544717\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n------------------------------\nEpoch: 60\nTraining loss: 0.023362241928674737 | Validation loss: 0.024779479191810998\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 61\nTraining loss: 0.02353256069579903 | Validation loss: 0.028069672726646618\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 62\nTraining loss: 0.023943516661470033 | Validation loss: 0.026808338826177298\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 63\nTraining loss: 0.025357702752689316 | Validation loss: 0.02782607140640418\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 64\nTraining loss: 0.02692143836115696 | Validation loss: 0.030238174040008475\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 65\nTraining loss: 0.028473555140805486 | Validation loss: 0.03152257176461043\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 66\nTraining loss: 0.02699487556000145 | Validation loss: 0.047619540589275186\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 67\nTraining loss: 0.02489455645257721 | Validation loss: 0.028346326340127875\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 68\nTraining loss: 0.02349343087182057 | Validation loss: 0.028713655347625416\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 69\nTraining loss: 0.02266941082530788 | Validation loss: 0.02549334270534692\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948]\n------------------------------\nEpoch: 70\nTraining loss: 0.021703293554636897 | Validation loss: 0.024168244942470832\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 71\nTraining loss: 0.02196174266995216 | Validation loss: 0.02505502008177616\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 72\nTraining loss: 0.022541736478784255 | Validation loss: 0.03061453970494094\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 73\nTraining loss: 0.023306790157696423 | Validation loss: 0.029162768481506243\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 74\nTraining loss: 0.025527279784104653 | Validation loss: 0.04011596442648658\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 75\nTraining loss: 0.026604378816424586 | Validation loss: 0.033134016794738944\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 76\nTraining loss: 0.025109626031575762 | Validation loss: 0.027811537048331014\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 77\nTraining loss: 0.024159897717514207 | Validation loss: 0.027146308924312943\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 78\nTraining loss: 0.022194400768042827 | Validation loss: 0.026139097840145783\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 79\nTraining loss: 0.020927915326794798 | Validation loss: 0.025522519178964472\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824]\n------------------------------\nEpoch: 80\nTraining loss: 0.02010866692479776 | Validation loss: 0.023466593240974127\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 81\nTraining loss: 0.02052211769457374 | Validation loss: 0.025561627552465157\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 82\nTraining loss: 0.02113350276189039 | Validation loss: 0.03258011841939555\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 83\nTraining loss: 0.02219036114687214 | Validation loss: 0.031846943552847264\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 84\nTraining loss: 0.023618147117370855 | Validation loss: 0.030687368647367867\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 85\nTraining loss: 0.02571254876651326 | Validation loss: 0.031185772821859078\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 86\nTraining loss: 0.024023403993294556 | Validation loss: 0.02871771556911645\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 87\nTraining loss: 0.022371417447468456 | Validation loss: 0.032844013155058575\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 88\nTraining loss: 0.02110713280889453 | Validation loss: 0.030332010584297003\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 89\nTraining loss: 0.020046295809122372 | Validation loss: 0.025618491763318027\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659]\n------------------------------\nEpoch: 90\nTraining loss: 0.019356959265637762 | Validation loss: 0.02316210146441504\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 91\nTraining loss: 0.019383190148415008 | Validation loss: 0.02634717537849038\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 92\nTraining loss: 0.01964251248888215 | Validation loss: 0.03047156140760139\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 93\nTraining loss: 0.02092831273164068 | Validation loss: 0.029502556693774683\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 94\nTraining loss: 0.02249736651322063 | Validation loss: 0.041276842148767576\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 95\nTraining loss: 0.024790223473112803 | Validation loss: 0.03220497692624728\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 96\nTraining loss: 0.02344361155160836 | Validation loss: 0.04404209623182261\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 97\nTraining loss: 0.021694707273676686 | Validation loss: 0.03870899168153604\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 98\nTraining loss: 0.02010431843524685 | Validation loss: 0.028377596702840593\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 99\nTraining loss: 0.018850200651783725 | Validation loss: 0.030515969972367638\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621 ]\n------------------------------\nEpoch: 100\nTraining loss: 0.018333698068840467 | Validation loss: 0.022866654968648044\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 101\nTraining loss: 0.01845642739184657 | Validation loss: 0.027251339827974636\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 102\nTraining loss: 0.018576808184461325 | Validation loss: 0.03162037546711939\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 103\nTraining loss: 0.019754362957818166 | Validation loss: 0.030628113283051386\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 104\nTraining loss: 0.020817349562231375 | Validation loss: 0.03573075723316935\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 105\nTraining loss: 0.023296759935209944 | Validation loss: 0.03717152795030011\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 106\nTraining loss: 0.021819874887563744 | Validation loss: 0.04560715691358955\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 107\nTraining loss: 0.02036462054987039 | Validation loss: 0.03906206234737679\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 108\nTraining loss: 0.018941566669287122 | Validation loss: 0.02962375542631856\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 109\nTraining loss: 0.017907579606208875 | Validation loss: 0.025408087988142616\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n------------------------------\nEpoch: 110\nTraining loss: 0.017118974164964593 | Validation loss: 0.023260762639067793\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 111\nTraining loss: 0.017573000283493678 | Validation loss: 0.027502070560499473\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 112\nTraining loss: 0.018396547465224046 | Validation loss: 0.02959495989812745\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 113\nTraining loss: 0.018755224162750706 | Validation loss: 0.032247422745934236\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 114\nTraining loss: 0.020228182037874146 | Validation loss: 0.02642800127742467\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 115\nTraining loss: 0.021511572188868816 | Validation loss: 0.029202266009869398\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 116\nTraining loss: 0.02057119747813867 | Validation loss: 0.028558928381513665\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 117\nTraining loss: 0.019055473568792246 | Validation loss: 0.04045708553382644\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 118\nTraining loss: 0.018233418848593624 | Validation loss: 0.035353186505812186\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 119\nTraining loss: 0.017574515155687625 | Validation loss: 0.030527102243569162\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n------------------------------\nEpoch: 120\nTraining loss: 0.017130743881345403 | Validation loss: 0.022757303659562713\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 121\nTraining loss: 0.016826823591349686 | Validation loss: 0.025191029671717574\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 122\nTraining loss: 0.016850333925032494 | Validation loss: 0.03346616526444753\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 123\nTraining loss: 0.018192078241584253 | Validation loss: 0.047841162869223845\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 124\nTraining loss: 0.019429788616847018 | Validation loss: 0.038563721226873224\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 125\nTraining loss: 0.021230487420926897 | Validation loss: 0.03223240699757029\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 126\nTraining loss: 0.02028775199373462 | Validation loss: 0.03193793335446605\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 127\nTraining loss: 0.018140019360473568 | Validation loss: 0.02840865741449374\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 128\nTraining loss: 0.017588311623857947 | Validation loss: 0.031451340212866115\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 129\nTraining loss: 0.016793060888137135 | Validation loss: 0.024730978499132174\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573 ]\n------------------------------\nEpoch: 130\nTraining loss: 0.016106459407173857 | Validation loss: 0.022969599364808312\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 131\nTraining loss: 0.016320625238348634 | Validation loss: 0.024469188311033778\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 132\nTraining loss: 0.016396109552635832 | Validation loss: 0.02705709463744252\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 133\nTraining loss: 0.01734909897252005 | Validation loss: 0.031023156035829474\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 134\nTraining loss: 0.01820811143297018 | Validation loss: 0.05327481610907449\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 135\nTraining loss: 0.019724413256484028 | Validation loss: 0.03707399743574637\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 136\nTraining loss: 0.019301350711255657 | Validation loss: 0.03078989284457984\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 137\nTraining loss: 0.017556924152435087 | Validation loss: 0.028413188471286384\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 138\nTraining loss: 0.016612043256434252 | Validation loss: 0.025720094572062844\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 139\nTraining loss: 0.016134973512772394 | Validation loss: 0.027998936052123707\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696 ]\n------------------------------\nEpoch: 140\nTraining loss: 0.015872289350598444 | Validation loss: 0.022802802872050693\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 141\nTraining loss: 0.015516076696922584 | Validation loss: 0.024056544630891748\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 142\nTraining loss: 0.015703990444426937 | Validation loss: 0.0258673885492263\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 143\nTraining loss: 0.01668028063425908 | Validation loss: 0.030482867594670365\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 144\nTraining loss: 0.017571322114339896 | Validation loss: 0.03652796528681561\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 145\nTraining loss: 0.019416323446725704 | Validation loss: 0.03051293613734069\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 146\nTraining loss: 0.018579784489940014 | Validation loss: 0.03504657910929786\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 147\nTraining loss: 0.016626955300797615 | Validation loss: 0.028009693586715945\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 148\nTraining loss: 0.016029098249819813 | Validation loss: 0.029912206999681615\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 149\nTraining loss: 0.015353832416692558 | Validation loss: 0.023703449750664057\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028 ]\n------------------------------\nEpoch: 150\nTraining loss: 0.014883509628018555 | Validation loss: 0.021929300873091927\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 151\nTraining loss: 0.01512682895385185 | Validation loss: 0.023662176673059112\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 152\nTraining loss: 0.014992564006195384 | Validation loss: 0.02634827120022641\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 153\nTraining loss: 0.015701996538864105 | Validation loss: 0.02868762995219893\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 154\nTraining loss: 0.016893358146581723 | Validation loss: 0.026929746002510743\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 155\nTraining loss: 0.01879451618130718 | Validation loss: 0.034038130538883035\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 156\nTraining loss: 0.017982265125123822 | Validation loss: 0.03113197583567213\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 157\nTraining loss: 0.0168210576383435 | Validation loss: 0.03469343262690085\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 158\nTraining loss: 0.015516883760158504 | Validation loss: 0.025853695451385446\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 159\nTraining loss: 0.014961472537596615 | Validation loss: 0.0240436724766537\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293 ]\n------------------------------\nEpoch: 160\nTraining loss: 0.01467474195535998 | Validation loss: 0.022376254062961648\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 161\nTraining loss: 0.014463082505199983 | Validation loss: 0.02429829292965156\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 162\nTraining loss: 0.014489603282085487 | Validation loss: 0.024390017406808004\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 163\nTraining loss: 0.014960362051366543 | Validation loss: 0.03855056495026306\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 164\nTraining loss: 0.016004944959541363 | Validation loss: 0.027088109058914362\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 165\nTraining loss: 0.017839457700979344 | Validation loss: 0.027761828982167773\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 166\nTraining loss: 0.016734649878641476 | Validation loss: 0.02709192072075826\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 167\nTraining loss: 0.015986402323279452 | Validation loss: 0.02521773434623524\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 168\nTraining loss: 0.014949820494773437 | Validation loss: 0.028413208270514453\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 169\nTraining loss: 0.014427602009809746 | Validation loss: 0.025387840138541326\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n------------------------------\nEpoch: 170\nTraining loss: 0.014175291431649606 | Validation loss: 0.0222464417003923\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 171\nTraining loss: 0.014155056855964417 | Validation loss: 0.025276930174893804\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 172\nTraining loss: 0.013586985551733143 | Validation loss: 0.02459180938010966\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 173\nTraining loss: 0.014965886816534461 | Validation loss: 0.026229242380294535\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 174\nTraining loss: 0.01641260800617082 | Validation loss: 0.02679949150317245\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 175\nTraining loss: 0.017782501397863488 | Validation loss: 0.029586961009988078\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 176\nTraining loss: 0.01672653796493399 | Validation loss: 0.04584400052273715\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 177\nTraining loss: 0.015437085578712274 | Validation loss: 0.02551957640658926\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 178\nTraining loss: 0.014381962241034727 | Validation loss: 0.027132539078593254\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 179\nTraining loss: 0.014216822250841223 | Validation loss: 0.024544751720020064\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n------------------------------\nEpoch: 180\nTraining loss: 0.013737957268877297 | Validation loss: 0.02210113792507737\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 181\nTraining loss: 0.013467943696875354 | Validation loss: 0.024385717973389006\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 182\nTraining loss: 0.013636469504586897 | Validation loss: 0.023889954443331116\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 183\nTraining loss: 0.01405429927708239 | Validation loss: 0.02557995884368817\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 184\nTraining loss: 0.01594532945440436 | Validation loss: 0.02542697093277066\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 185\nTraining loss: 0.017133247770597132 | Validation loss: 0.026300121409197647\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 186\nTraining loss: 0.01628313361743123 | Validation loss: 0.027368043307904846\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 187\nTraining loss: 0.014727997079453601 | Validation loss: 0.026379065864064074\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 188\nTraining loss: 0.014223979732819966 | Validation loss: 0.025630985352176207\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 189\nTraining loss: 0.013516011896866317 | Validation loss: 0.025777899901624077\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114]\n------------------------------\nEpoch: 190\nTraining loss: 0.013738666835944264 | Validation loss: 0.02189505234774616\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 191\nTraining loss: 0.013424585791950931 | Validation loss: 0.025334533924857777\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 192\nTraining loss: 0.013389314419342851 | Validation loss: 0.0272432800244402\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 193\nTraining loss: 0.013665561119512636 | Validation loss: 0.025919210579660203\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 194\nTraining loss: 0.015035620595955727 | Validation loss: 0.027025571368910647\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 195\nTraining loss: 0.016460462674802664 | Validation loss: 0.02844604704942968\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 196\nTraining loss: 0.015581737829334273 | Validation loss: 0.026644890472568846\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 197\nTraining loss: 0.014256568825138467 | Validation loss: 0.027392463344666693\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 198\nTraining loss: 0.013787354469033224 | Validation loss: 0.027793116833048838\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 199\nTraining loss: 0.013205509652782764 | Validation loss: 0.024656895755065814\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505]\n------------------------------\nEpoch: 200\nTraining loss: 0.012971938055540835 | Validation loss: 0.02208654147883256\nValidation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n 0.02210114 0.02189505 0.02208654]\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.12228791815478628 | Validation loss: 0.1054369103577402\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 1\nTraining loss: 0.08345528741980471 | Validation loss: 0.06988698575231764\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 2\nTraining loss: 0.06868993813489996 | Validation loss: 0.06038115518512549\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 3\nTraining loss: 0.06472492747281383 | Validation loss: 0.054184967582976376\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 4\nTraining loss: 0.06308126072512894 | Validation loss: 0.13389017957228203\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 5\nTraining loss: 0.060122362686306 | Validation loss: 0.17819979400546462\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 6\nTraining loss: 0.055708695059203034 | Validation loss: 0.049357822785774864\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 7\nTraining loss: 0.052122626143197216 | Validation loss: 0.05457461749513944\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 8\nTraining loss: 0.049281957006551386 | Validation loss: 0.04812982305884361\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 9\nTraining loss: 0.04547054481851619 | Validation loss: 0.040970456407026005\nValidation loss (ends of cycles): [0.10543691]\n------------------------------\nEpoch: 10\nTraining loss: 0.043980468833077005 | Validation loss: 0.038650386242402926\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 11\nTraining loss: 0.04430758124747412 | Validation loss: 0.03830565043069698\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 12\nTraining loss: 0.04522882570035574 | Validation loss: 0.038157600081629224\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 13\nTraining loss: 0.04559288569915343 | Validation loss: 0.039070602782346586\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 14\nTraining loss: 0.04618575094585738 | Validation loss: 0.03879287556089737\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 15\nTraining loss: 0.04678545226683704 | Validation loss: 0.04081984361012777\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 16\nTraining loss: 0.0438527973951787 | Validation loss: 0.03588898617912222\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 17\nTraining loss: 0.041553754484417234 | Validation loss: 0.04742350084362207\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 18\nTraining loss: 0.03968509051161326 | Validation loss: 0.03589330261780156\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 19\nTraining loss: 0.0379939789058474 | Validation loss: 0.03520316261522196\nValidation loss (ends of cycles): [0.10543691 0.03865039]\n------------------------------\nEpoch: 20\nTraining loss: 0.03644957100760525 | Validation loss: 0.032497044583713566\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 21\nTraining loss: 0.03638568487230355 | Validation loss: 0.03465360265087198\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 22\nTraining loss: 0.03805654572428969 | Validation loss: 0.032737655737609776\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 23\nTraining loss: 0.03894560454352721 | Validation loss: 0.04324095565135832\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 24\nTraining loss: 0.04015268617105193 | Validation loss: 0.03395191821511145\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 25\nTraining loss: 0.04047986775697247 | Validation loss: 0.03522724899704809\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 26\nTraining loss: 0.038433285021200414 | Validation loss: 0.039532348375629495\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 27\nTraining loss: 0.03698946019195444 | Validation loss: 0.03227757614243914\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 28\nTraining loss: 0.03505631038404214 | Validation loss: 0.03230649564001295\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 29\nTraining loss: 0.033620467814579 | Validation loss: 0.03223192781485893\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n------------------------------\nEpoch: 30\nTraining loss: 0.03178789988690155 | Validation loss: 0.030227238519324198\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 31\nTraining loss: 0.03240554717679819 | Validation loss: 0.030879514326375944\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 32\nTraining loss: 0.03349711600978806 | Validation loss: 0.031250168948813724\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 33\nTraining loss: 0.034300035622909786 | Validation loss: 0.031172568392422464\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 34\nTraining loss: 0.03561411505731625 | Validation loss: 0.035509896154205\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 35\nTraining loss: 0.035965731965635364 | Validation loss: 0.037915085131923355\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 36\nTraining loss: 0.03542989279650818 | Validation loss: 0.03176571290802072\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 37\nTraining loss: 0.03348708177757699 | Validation loss: 0.04181969317573088\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 38\nTraining loss: 0.032106196553241914 | Validation loss: 0.029362012459724036\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 39\nTraining loss: 0.030441816941630548 | Validation loss: 0.029443050079323626\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n------------------------------\nEpoch: 40\nTraining loss: 0.028951545102082615 | Validation loss: 0.027948243957426813\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 41\nTraining loss: 0.02955955447159647 | Validation loss: 0.02866512626685478\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 42\nTraining loss: 0.030229009668971223 | Validation loss: 0.029844649274040153\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 43\nTraining loss: 0.031777786844357 | Validation loss: 0.029600525412846496\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 44\nTraining loss: 0.03273992139724939 | Validation loss: 0.03340166689897025\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 45\nTraining loss: 0.03442120896198037 | Validation loss: 0.03195419293586855\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 46\nTraining loss: 0.03261524584235215 | Validation loss: 0.0315606604433722\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 47\nTraining loss: 0.03111256630258347 | Validation loss: 0.029374615530724877\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 48\nTraining loss: 0.029745885477651182 | Validation loss: 0.029515129479545134\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 49\nTraining loss: 0.028203826734599303 | Validation loss: 0.027964135907866335\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n------------------------------\nEpoch: 50\nTraining loss: 0.02741435214089669 | Validation loss: 0.02691561297548038\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 51\nTraining loss: 0.02772532103628647 | Validation loss: 0.028478951848767423\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 52\nTraining loss: 0.028424028443490585 | Validation loss: 0.028361442671329888\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 53\nTraining loss: 0.028827177053998884 | Validation loss: 0.03354962459868855\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 54\nTraining loss: 0.030674236258719026 | Validation loss: 0.02970579656323901\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 55\nTraining loss: 0.03177045115337866 | Validation loss: 0.029968159445733937\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 56\nTraining loss: 0.03085786577435286 | Validation loss: 0.029443291947245598\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 57\nTraining loss: 0.02905051384832922 | Validation loss: 0.02832780895685708\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 58\nTraining loss: 0.02775720836276688 | Validation loss: 0.027414744236954936\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 59\nTraining loss: 0.02642078349760514 | Validation loss: 0.02696808020549792\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n------------------------------\nEpoch: 60\nTraining loss: 0.025448385527645185 | Validation loss: 0.02623730621956013\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 61\nTraining loss: 0.025777471902954385 | Validation loss: 0.0270316894683573\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 62\nTraining loss: 0.026880092464569138 | Validation loss: 0.02829531321509017\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 63\nTraining loss: 0.02747904618338841 | Validation loss: 0.029463160783052444\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 64\nTraining loss: 0.029024402031720412 | Validation loss: 0.030591066995704616\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 65\nTraining loss: 0.03056626948641568 | Validation loss: 0.03102897756077625\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 66\nTraining loss: 0.02904554159282063 | Validation loss: 0.03191932718510981\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 67\nTraining loss: 0.02752647077573872 | Validation loss: 0.02758541154778666\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 68\nTraining loss: 0.026392553770172644 | Validation loss: 0.027598489075899124\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 69\nTraining loss: 0.024825475034796124 | Validation loss: 0.027477786013925518\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731]\n------------------------------\nEpoch: 70\nTraining loss: 0.02389349034851099 | Validation loss: 0.025481665368985246\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 71\nTraining loss: 0.024810818970445694 | Validation loss: 0.0258330095352398\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 72\nTraining loss: 0.025311344723421628 | Validation loss: 0.026171780950217334\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 73\nTraining loss: 0.026799078659737498 | Validation loss: 0.027304544803444988\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 74\nTraining loss: 0.02739570381468147 | Validation loss: 0.029297827053125256\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 75\nTraining loss: 0.02901256551996358 | Validation loss: 0.03376480612765859\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 76\nTraining loss: 0.027922531536863585 | Validation loss: 0.031056516907281347\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 77\nTraining loss: 0.026418042119319845 | Validation loss: 0.02869530960365578\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 78\nTraining loss: 0.024494294804043887 | Validation loss: 0.02879283722076151\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 79\nTraining loss: 0.023640055410049068 | Validation loss: 0.025536342110070918\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167]\n------------------------------\nEpoch: 80\nTraining loss: 0.02292518796611244 | Validation loss: 0.024924556069352007\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 81\nTraining loss: 0.022964879946132017 | Validation loss: 0.025884984164602227\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 82\nTraining loss: 0.024365100468228745 | Validation loss: 0.02613054729860138\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 83\nTraining loss: 0.024947036228074534 | Validation loss: 0.026084760548891844\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 84\nTraining loss: 0.026113691951746378 | Validation loss: 0.029804825920749595\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 85\nTraining loss: 0.02732016758357243 | Validation loss: 0.028548474147639894\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 86\nTraining loss: 0.026706833134942907 | Validation loss: 0.02933104053415634\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 87\nTraining loss: 0.02524915052108406 | Validation loss: 0.028903299735652074\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 88\nTraining loss: 0.023512191850904043 | Validation loss: 0.02478121103787864\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 89\nTraining loss: 0.0227027192325672 | Validation loss: 0.024699235369485838\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456]\n------------------------------\nEpoch: 90\nTraining loss: 0.02195627812099287 | Validation loss: 0.024092087763603085\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 91\nTraining loss: 0.022029758766066374 | Validation loss: 0.024637732999744238\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 92\nTraining loss: 0.02214197951134264 | Validation loss: 0.02562687725380615\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 93\nTraining loss: 0.023682874324542236 | Validation loss: 0.025451905301047698\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 94\nTraining loss: 0.0249582189759921 | Validation loss: 0.026010741134760557\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 95\nTraining loss: 0.026149797552393945 | Validation loss: 0.029183034267690446\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 96\nTraining loss: 0.025613661204290584 | Validation loss: 0.029470085580315854\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 97\nTraining loss: 0.02367819772987831 | Validation loss: 0.02606600364325223\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 98\nTraining loss: 0.022782948269410346 | Validation loss: 0.025672975306709606\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 99\nTraining loss: 0.02164456419025858 | Validation loss: 0.024995564966014138\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209]\n------------------------------\nEpoch: 100\nTraining loss: 0.020927290345080257 | Validation loss: 0.023898190818727016\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 101\nTraining loss: 0.020838848724052672 | Validation loss: 0.024341666174155695\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 102\nTraining loss: 0.02157418485882321 | Validation loss: 0.02549752972468182\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 103\nTraining loss: 0.02215171731796449 | Validation loss: 0.027644692678694373\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 104\nTraining loss: 0.023533740482194636 | Validation loss: 0.02766746641309173\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 105\nTraining loss: 0.02504744735826564 | Validation loss: 0.03269333695923841\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 106\nTraining loss: 0.02399577457851511 | Validation loss: 0.026279965553570678\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 107\nTraining loss: 0.022653392671296995 | Validation loss: 0.025546919761432543\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 108\nTraining loss: 0.021445270692096736 | Validation loss: 0.024732283109592065\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 109\nTraining loss: 0.02024538448928454 | Validation loss: 0.024346998823737656\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n------------------------------\nEpoch: 110\nTraining loss: 0.020088354396308218 | Validation loss: 0.023614108493482625\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 111\nTraining loss: 0.020159628817705603 | Validation loss: 0.023966707841113762\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 112\nTraining loss: 0.02066820897767699 | Validation loss: 0.024948603266643152\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 113\nTraining loss: 0.021366572594345826 | Validation loss: 0.02552163469846602\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 114\nTraining loss: 0.022936123722361597 | Validation loss: 0.027489966126503767\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 115\nTraining loss: 0.02412121834960289 | Validation loss: 0.027228782815789734\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 116\nTraining loss: 0.023651672402260508 | Validation loss: 0.025690350233128777\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 117\nTraining loss: 0.021718305630261094 | Validation loss: 0.027504071075883176\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 118\nTraining loss: 0.020649877437428247 | Validation loss: 0.02455218826179151\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 119\nTraining loss: 0.02011051802986824 | Validation loss: 0.024612521649234824\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n------------------------------\nEpoch: 120\nTraining loss: 0.019264938839809682 | Validation loss: 0.023097472437829884\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 121\nTraining loss: 0.019316403066542576 | Validation loss: 0.02441213297209254\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 122\nTraining loss: 0.019887421912384955 | Validation loss: 0.02437097444716427\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 123\nTraining loss: 0.020397775916276666 | Validation loss: 0.027738412048805644\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 124\nTraining loss: 0.02181776584792367 | Validation loss: 0.02554246283101815\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 125\nTraining loss: 0.023289408678628082 | Validation loss: 0.032003110343659366\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 126\nTraining loss: 0.022173569053108615 | Validation loss: 0.025447359757015\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 127\nTraining loss: 0.02074599415593879 | Validation loss: 0.02787999822585671\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 128\nTraining loss: 0.019872636575918128 | Validation loss: 0.02400662622380036\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 129\nTraining loss: 0.0191136942778873 | Validation loss: 0.023773066078623135\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747]\n------------------------------\nEpoch: 130\nTraining loss: 0.018855253730453853 | Validation loss: 0.022893787844589463\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 131\nTraining loss: 0.018389209398878663 | Validation loss: 0.023396671577184287\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 132\nTraining loss: 0.01898594183574726 | Validation loss: 0.024825873094852322\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 133\nTraining loss: 0.019571426332118065 | Validation loss: 0.03233330883085728\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 134\nTraining loss: 0.020783360165399993 | Validation loss: 0.027140274229976866\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 135\nTraining loss: 0.022257593823823987 | Validation loss: 0.02835464863865464\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 136\nTraining loss: 0.02122602771922219 | Validation loss: 0.024941712376420146\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 137\nTraining loss: 0.020035431871750976 | Validation loss: 0.02537165092373336\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 138\nTraining loss: 0.019187722245337276 | Validation loss: 0.025162397124977025\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 139\nTraining loss: 0.018331920370323267 | Validation loss: 0.024782232664249563\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379]\n------------------------------\nEpoch: 140\nTraining loss: 0.018385974502917832 | Validation loss: 0.022544961250214664\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 141\nTraining loss: 0.01765795958008829 | Validation loss: 0.023712188798796247\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 142\nTraining loss: 0.018420973618522407 | Validation loss: 0.023575709166902083\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 143\nTraining loss: 0.019126582281224852 | Validation loss: 0.025220924919402157\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 144\nTraining loss: 0.01961053673526257 | Validation loss: 0.026265721975101367\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 145\nTraining loss: 0.02205375614538183 | Validation loss: 0.02857490042569461\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 146\nTraining loss: 0.020693585647618383 | Validation loss: 0.025435910698164393\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 147\nTraining loss: 0.01922064234797911 | Validation loss: 0.02503646554908267\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 148\nTraining loss: 0.018394955019353003 | Validation loss: 0.024210673067028874\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 149\nTraining loss: 0.017704154535128577 | Validation loss: 0.0237839009474825\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496]\n------------------------------\nEpoch: 150\nTraining loss: 0.017490101173686667 | Validation loss: 0.022615981495214835\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 151\nTraining loss: 0.017312443612627987 | Validation loss: 0.02387337434898924\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 152\nTraining loss: 0.017103146326281432 | Validation loss: 0.023941471551855404\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 153\nTraining loss: 0.018206471003728304 | Validation loss: 0.02593925995407281\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 154\nTraining loss: 0.019363362325082827 | Validation loss: 0.02541112327189357\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 155\nTraining loss: 0.020997219991574927 | Validation loss: 0.025342813727480394\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 156\nTraining loss: 0.019623544086073714 | Validation loss: 0.027581549491043442\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 157\nTraining loss: 0.01856963353125545 | Validation loss: 0.025144454367734766\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 158\nTraining loss: 0.017901460539807026 | Validation loss: 0.02464213549952816\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 159\nTraining loss: 0.017084604182197313 | Validation loss: 0.023683185516684142\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598]\n------------------------------\nEpoch: 160\nTraining loss: 0.01706684923473352 | Validation loss: 0.022672119033005502\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 161\nTraining loss: 0.01673129112914023 | Validation loss: 0.023932423822029873\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 162\nTraining loss: 0.017171250194371716 | Validation loss: 0.02393032217191325\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 163\nTraining loss: 0.017759795351771683 | Validation loss: 0.026334970016722328\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 164\nTraining loss: 0.01868446520340394 | Validation loss: 0.025735552940103743\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 165\nTraining loss: 0.019864219000441878 | Validation loss: 0.026584884738204657\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 166\nTraining loss: 0.019493408527649272 | Validation loss: 0.02552235554213877\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 167\nTraining loss: 0.0178543335580214 | Validation loss: 0.02480841196935486\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 168\nTraining loss: 0.017060703226948172 | Validation loss: 0.024558599520888593\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 169\nTraining loss: 0.01673183792717452 | Validation loss: 0.023999774656086055\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n------------------------------\nEpoch: 170\nTraining loss: 0.01663350821964867 | Validation loss: 0.02244613198908391\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 171\nTraining loss: 0.016103041254180838 | Validation loss: 0.02298469769044055\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 172\nTraining loss: 0.01630746733351815 | Validation loss: 0.02470569105611907\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 173\nTraining loss: 0.017092946812707354 | Validation loss: 0.024231813089163216\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 174\nTraining loss: 0.018157666808418262 | Validation loss: 0.02737403698955421\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 175\nTraining loss: 0.019655547646154476 | Validation loss: 0.027440111980670027\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 176\nTraining loss: 0.01850139775230148 | Validation loss: 0.025333901763790183\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 177\nTraining loss: 0.017738321042672648 | Validation loss: 0.026279438459486874\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 178\nTraining loss: 0.01703572527853757 | Validation loss: 0.02614127082267293\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 179\nTraining loss: 0.0160271596784393 | Validation loss: 0.02310966721011533\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n------------------------------\nEpoch: 180\nTraining loss: 0.016043832913661998 | Validation loss: 0.022625376642854127\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 181\nTraining loss: 0.015809292392048046 | Validation loss: 0.023693999517019147\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 182\nTraining loss: 0.016034167735057513 | Validation loss: 0.024256648495793343\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 183\nTraining loss: 0.016748781766500173 | Validation loss: 0.024400403792107547\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 184\nTraining loss: 0.017791693662936852 | Validation loss: 0.02896435482910386\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 185\nTraining loss: 0.019187645261683237 | Validation loss: 0.029182869561568455\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 186\nTraining loss: 0.018036984204413083 | Validation loss: 0.024585284361684764\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 187\nTraining loss: 0.0170689941084815 | Validation loss: 0.02560491528775957\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 188\nTraining loss: 0.016152752020085853 | Validation loss: 0.025931633905404143\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 189\nTraining loss: 0.015870558355397326 | Validation loss: 0.022788437876712392\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538]\n------------------------------\nEpoch: 190\nTraining loss: 0.01561052555301628 | Validation loss: 0.022453050267089297\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 191\nTraining loss: 0.015502604122114618 | Validation loss: 0.02315064930115585\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 192\nTraining loss: 0.015278358620069012 | Validation loss: 0.023724095206017846\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 193\nTraining loss: 0.01631206663219425 | Validation loss: 0.02449931452671687\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 194\nTraining loss: 0.01718841513951983 | Validation loss: 0.02486682427978074\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 195\nTraining loss: 0.01918403732370797 | Validation loss: 0.02663724600440926\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 196\nTraining loss: 0.018109782920711165 | Validation loss: 0.02764201440193035\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 197\nTraining loss: 0.016803728043654462 | Validation loss: 0.026375215983501187\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 198\nTraining loss: 0.015610241978542834 | Validation loss: 0.02521015338047787\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 199\nTraining loss: 0.015274432143272182 | Validation loss: 0.023028226431321214\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305]\n------------------------------\nEpoch: 200\nTraining loss: 0.015197526428818218 | Validation loss: 0.022503956462498063\nValidation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n 0.02262538 0.02245305 0.02250396]\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.15279348028695536 | Validation loss: 0.14506536836807543\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 1\nTraining loss: 0.0992763647545687 | Validation loss: 0.08038875761513527\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 2\nTraining loss: 0.06854788238853819 | Validation loss: 0.07798023822789009\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 3\nTraining loss: 0.06264783143393907 | Validation loss: 0.06590831738251907\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 4\nTraining loss: 0.06060648671076124 | Validation loss: 0.11362000554800034\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 5\nTraining loss: 0.05775577272571292 | Validation loss: 0.16101208042639953\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 6\nTraining loss: 0.05431262623171816 | Validation loss: 0.05958796557612144\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 7\nTraining loss: 0.05047389544425947 | Validation loss: 0.05647731199860573\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 8\nTraining loss: 0.04694410883312525 | Validation loss: 0.04427507806282777\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 9\nTraining loss: 0.04459618917300633 | Validation loss: 0.04090933745297102\nValidation loss (ends of cycles): [0.14506537]\n------------------------------\nEpoch: 10\nTraining loss: 0.04245714181921019 | Validation loss: 0.03974307392938779\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 11\nTraining loss: 0.04292430049977322 | Validation loss: 0.04144837728773172\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 12\nTraining loss: 0.043996825655884586 | Validation loss: 0.042766958904954105\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 13\nTraining loss: 0.04482655116330515 | Validation loss: 0.046519542686068095\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 14\nTraining loss: 0.04563175601062746 | Validation loss: 0.040774742857767984\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 15\nTraining loss: 0.044861849880049584 | Validation loss: 0.047626798757566854\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 16\nTraining loss: 0.0434767716914898 | Validation loss: 0.05095124581398872\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 17\nTraining loss: 0.0410251230046696 | Validation loss: 0.045552261030444734\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 18\nTraining loss: 0.03894493085622546 | Validation loss: 0.03951437076410422\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 19\nTraining loss: 0.03676822814366475 | Validation loss: 0.036260907824795977\nValidation loss (ends of cycles): [0.14506537 0.03974307]\n------------------------------\nEpoch: 20\nTraining loss: 0.03533417369551987 | Validation loss: 0.0353879089682148\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 21\nTraining loss: 0.03569188686502486 | Validation loss: 0.037464807741343975\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 22\nTraining loss: 0.03668812884070612 | Validation loss: 0.03660466314221804\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 23\nTraining loss: 0.037543252314090246 | Validation loss: 0.041893985194082446\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 24\nTraining loss: 0.03912920638285426 | Validation loss: 0.05439267536768547\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 25\nTraining loss: 0.040333300218106764 | Validation loss: 0.03696370225113172\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 26\nTraining loss: 0.03786210742918586 | Validation loss: 0.03918445941347342\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 27\nTraining loss: 0.03594942207670646 | Validation loss: 0.037800938105927065\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 28\nTraining loss: 0.0335141976216906 | Validation loss: 0.03604494651349691\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 29\nTraining loss: 0.03202974798557488 | Validation loss: 0.03684409148991108\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n------------------------------\nEpoch: 30\nTraining loss: 0.031134030864638115 | Validation loss: 0.03303491732535454\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 31\nTraining loss: 0.03132372897359644 | Validation loss: 0.05156394910927002\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 32\nTraining loss: 0.03225202846382311 | Validation loss: 0.03565296494903473\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 33\nTraining loss: 0.033366888260039 | Validation loss: 0.04518821405676695\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 34\nTraining loss: 0.03463769986139618 | Validation loss: 0.0466082334661713\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 35\nTraining loss: 0.035970898729707546 | Validation loss: 0.035931507841898844\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 36\nTraining loss: 0.033702543582239376 | Validation loss: 0.039530598415205113\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 37\nTraining loss: 0.03226060464390015 | Validation loss: 0.035033257463230535\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 38\nTraining loss: 0.030439186973218253 | Validation loss: 0.03421707866856685\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 39\nTraining loss: 0.028959295595524764 | Validation loss: 0.03411362642565599\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n------------------------------\nEpoch: 40\nTraining loss: 0.028355397585673854 | Validation loss: 0.03144805756612466\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 41\nTraining loss: 0.028508084403871283 | Validation loss: 0.034525489721160665\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 42\nTraining loss: 0.02908853772152894 | Validation loss: 0.03552745999051975\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 43\nTraining loss: 0.030279508965308607 | Validation loss: 0.0412328397282041\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 44\nTraining loss: 0.03161932406607668 | Validation loss: 0.04157860963963545\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 45\nTraining loss: 0.032413170465573606 | Validation loss: 0.03746562243367617\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 46\nTraining loss: 0.03155249784802377 | Validation loss: 0.03418631808689007\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 47\nTraining loss: 0.029698396640780726 | Validation loss: 0.0433016954562985\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 48\nTraining loss: 0.02817893827613066 | Validation loss: 0.032205368463809676\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 49\nTraining loss: 0.026418235477225983 | Validation loss: 0.031983047186468654\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n------------------------------\nEpoch: 50\nTraining loss: 0.02586102904940424 | Validation loss: 0.03005178444660627\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 51\nTraining loss: 0.025684328844039307 | Validation loss: 0.0325129132789488\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 52\nTraining loss: 0.026494500268748413 | Validation loss: 0.035909466803647004\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 53\nTraining loss: 0.02768527107079502 | Validation loss: 0.03419312082517605\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 54\nTraining loss: 0.029022690255632283 | Validation loss: 0.0756713766604662\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 55\nTraining loss: 0.030787047692518003 | Validation loss: 0.05678711172479849\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 56\nTraining loss: 0.029405175485772643 | Validation loss: 0.03094115208547849\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 57\nTraining loss: 0.027223537728823872 | Validation loss: 0.03423833968834235\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 58\nTraining loss: 0.025910726122497788 | Validation loss: 0.03238570869255524\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 59\nTraining loss: 0.024620303329879696 | Validation loss: 0.032775767075900845\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n------------------------------\nEpoch: 60\nTraining loss: 0.024254488279884644 | Validation loss: 0.02928792341397359\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 61\nTraining loss: 0.023843600908633668 | Validation loss: 0.03604337021421928\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 62\nTraining loss: 0.024692987382170642 | Validation loss: 0.03211906702759174\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 63\nTraining loss: 0.025721851939916128 | Validation loss: 0.0374490234714288\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 64\nTraining loss: 0.027135886828818543 | Validation loss: 0.03476525499270512\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 65\nTraining loss: 0.028328854775410674 | Validation loss: 0.06682556294477902\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 66\nTraining loss: 0.027366366840687842 | Validation loss: 0.03239961333859425\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 67\nTraining loss: 0.02528798375052479 | Validation loss: 0.0460323214244384\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 68\nTraining loss: 0.024234542412012214 | Validation loss: 0.03999455079722863\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 69\nTraining loss: 0.02321765907033373 | Validation loss: 0.03608835130356825\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792]\n------------------------------\nEpoch: 70\nTraining loss: 0.0221617315492408 | Validation loss: 0.028585847467184067\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 71\nTraining loss: 0.022177805008012273 | Validation loss: 0.030094460082741883\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 72\nTraining loss: 0.022704321559262178 | Validation loss: 0.03219181979791476\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 73\nTraining loss: 0.023915208310552456 | Validation loss: 0.031648835382209375\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 74\nTraining loss: 0.02473944200858896 | Validation loss: 0.03561788174108817\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 75\nTraining loss: 0.027072073639132957 | Validation loss: 0.03957040794193745\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 76\nTraining loss: 0.025478746504466302 | Validation loss: 0.04291144543542312\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 77\nTraining loss: 0.02397834242280075 | Validation loss: 0.03551239255242623\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 78\nTraining loss: 0.022762301095161842 | Validation loss: 0.035777714963142686\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 79\nTraining loss: 0.021810038727877835 | Validation loss: 0.03426963635362112\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585]\n------------------------------\nEpoch: 80\nTraining loss: 0.021307517216363658 | Validation loss: 0.028310040656763773\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 81\nTraining loss: 0.021035488740152677 | Validation loss: 0.04373047608309067\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 82\nTraining loss: 0.021291860884530583 | Validation loss: 0.044406829974972285\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 83\nTraining loss: 0.022908585479762027 | Validation loss: 0.034169698492265664\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 84\nTraining loss: 0.024446617925034362 | Validation loss: 0.034328691518077485\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 85\nTraining loss: 0.025942979482292888 | Validation loss: 0.060097817689753495\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 86\nTraining loss: 0.024310822792818793 | Validation loss: 0.03501157462596893\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 87\nTraining loss: 0.022634415940083714 | Validation loss: 0.03473313940832248\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 88\nTraining loss: 0.021089079691783377 | Validation loss: 0.03671561274677515\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 89\nTraining loss: 0.0206126121479852 | Validation loss: 0.038643558796208635\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004]\n------------------------------\nEpoch: 90\nTraining loss: 0.02040816003999669 | Validation loss: 0.027402662170621064\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 91\nTraining loss: 0.019678906183161957 | Validation loss: 0.034164513962773174\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 92\nTraining loss: 0.020458108885025205 | Validation loss: 0.043395852025311724\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 93\nTraining loss: 0.02149372065801792 | Validation loss: 0.04887792203002251\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 94\nTraining loss: 0.022725580859672925 | Validation loss: 0.03178696441822327\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 95\nTraining loss: 0.024382544900120994 | Validation loss: 0.04414457856462552\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 96\nTraining loss: 0.022855830495717072 | Validation loss: 0.041788846684189945\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 97\nTraining loss: 0.021017923792786445 | Validation loss: 0.039392892127999894\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 98\nTraining loss: 0.020251478479519063 | Validation loss: 0.035716857617864244\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 99\nTraining loss: 0.01932824108945696 | Validation loss: 0.034940155796133555\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266]\n------------------------------\nEpoch: 100\nTraining loss: 0.020082863217393155 | Validation loss: 0.026871812601502124\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n------------------------------\nEpoch: 101\nTraining loss: 0.018461225810743536 | Validation loss: 0.03337421948806597\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n------------------------------\nEpoch: 102\nTraining loss: 0.01906144128818261 | Validation loss: 0.044319855908934884\nValidation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n------------------------------\nEpoch: 103\n\n\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/models')\nseeds = range(20)\norder = 1\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_local_mollisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                   order = order,\n                                                   src_dir_model=src_dir_model)\n\nperfs_local_mollisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.103741\n      2.777945\n      0.772714\n      0.873048\n      0.454543\n      0.212442\n      0.214790\n      26.963759\n      0.008362\n      0.019571\n    \n    \n      std\n      0.085642\n      0.136850\n      0.019085\n      0.011321\n      0.078355\n      0.080627\n      0.010765\n      1.352271\n      0.013197\n      0.031140\n    \n    \n      min\n      1.923080\n      2.550751\n      0.729316\n      0.846127\n      0.360252\n      0.129782\n      0.200901\n      25.281143\n      -0.008894\n      -0.021487\n    \n    \n      25%\n      2.058124\n      2.688481\n      0.763672\n      0.865870\n      0.408399\n      0.166824\n      0.206547\n      25.622950\n      -0.004228\n      -0.009841\n    \n    \n      50%\n      2.111893\n      2.778513\n      0.775535\n      0.873508\n      0.428046\n      0.183224\n      0.212339\n      27.126652\n      0.009768\n      0.022086\n    \n    \n      75%\n      2.166518\n      2.872471\n      0.786733\n      0.881705\n      0.470035\n      0.220976\n      0.221762\n      27.655639\n      0.017485\n      0.041642\n    \n    \n      max\n      2.256515\n      3.048478\n      0.803395\n      0.891445\n      0.664916\n      0.442113\n      0.240433\n      30.713740\n      0.039076\n      0.092671\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nTrain and test on Gelisols\n\n# Replace following Paths with yours\ndest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/losses')\ndest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/models')\n\norder = 12\nseeds = range(20) \nn_epochs = 31\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               dest_dir_loss=dest_dir_loss,\n               dest_dir_model=dest_dir_model,\n               n_epochs=n_epochs,\n               sc_kwargs=params_scheduler)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.44518339904871856 | Validation loss: 0.41080141067504883\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 1\nTraining loss: 0.4287309023466977 | Validation loss: 0.4031795859336853\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 2\nTraining loss: 0.4021740339019082 | Validation loss: 0.3900226950645447\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 3\nTraining loss: 0.37238084186207165 | Validation loss: 0.37106457352638245\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 4\nTraining loss: 0.3336567445234819 | Validation loss: 0.3466283082962036\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 5\nTraining loss: 0.28763867101886054 | Validation loss: 0.31739506125450134\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 6\nTraining loss: 0.2438681653954766 | Validation loss: 0.28248274326324463\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 7\nTraining loss: 0.21182731132615695 | Validation loss: 0.2361939251422882\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 8\nTraining loss: 0.19058756801215085 | Validation loss: 0.1982204169034958\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 9\nTraining loss: 0.17826715314930136 | Validation loss: 0.16747665405273438\nValidation loss (ends of cycles): [0.41080141]\n------------------------------\nEpoch: 10\nTraining loss: 0.17355785789814862 | Validation loss: 0.1784353256225586\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 11\nTraining loss: 0.16932719945907593 | Validation loss: 0.15043631196022034\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 12\nTraining loss: 0.15943419459191235 | Validation loss: 0.12366648763418198\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 13\nTraining loss: 0.14414918016303668 | Validation loss: 0.14775261282920837\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 14\nTraining loss: 0.12919475544582715 | Validation loss: 0.10592546314001083\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 15\nTraining loss: 0.1137502295049754 | Validation loss: 0.09366549551486969\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 16\nTraining loss: 0.09954378415237773 | Validation loss: 0.13239187002182007\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 17\nTraining loss: 0.09346607327461243 | Validation loss: 0.09203055500984192\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 18\nTraining loss: 0.08725625784559683 | Validation loss: 0.08837558329105377\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 19\nTraining loss: 0.08394723622636362 | Validation loss: 0.08583536744117737\nValidation loss (ends of cycles): [0.41080141 0.17843533]\n------------------------------\nEpoch: 20\nTraining loss: 0.08184474774382332 | Validation loss: 0.08680430799722672\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 21\nTraining loss: 0.07987467301162807 | Validation loss: 0.09593759477138519\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 22\nTraining loss: 0.07881784202022986 | Validation loss: 0.15475192666053772\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 23\nTraining loss: 0.08277476172555577 | Validation loss: 0.11691433191299438\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 24\nTraining loss: 0.07523983920162375 | Validation loss: 0.09792322665452957\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 25\nTraining loss: 0.08776683563535864 | Validation loss: 0.14275996387004852\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 26\nTraining loss: 0.08074803311716426 | Validation loss: 0.09750550240278244\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 27\nTraining loss: 0.07898637584664604 | Validation loss: 0.08195220679044724\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 28\nTraining loss: 0.07386642017147758 | Validation loss: 0.08875860273838043\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 29\nTraining loss: 0.06991859241812066 | Validation loss: 0.08599168062210083\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n------------------------------\nEpoch: 30\nTraining loss: 0.07041569883850488 | Validation loss: 0.08440250158309937\nValidation loss (ends of cycles): [0.41080141 0.17843533 0.08680431 0.0844025 ]\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2712295380505649 | Validation loss: 0.29694056510925293\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 1\nTraining loss: 0.2557703337886117 | Validation loss: 0.2916700839996338\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 2\nTraining loss: 0.23915838382460855 | Validation loss: 0.2821319103240967\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 3\nTraining loss: 0.21806168962608685 | Validation loss: 0.27006006240844727\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 4\nTraining loss: 0.19106543605977838 | Validation loss: 0.2555059790611267\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 5\nTraining loss: 0.1593422293663025 | Validation loss: 0.23977532982826233\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 6\nTraining loss: 0.1341805329377001 | Validation loss: 0.21363312005996704\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 7\nTraining loss: 0.11576782234690407 | Validation loss: 0.17318867146968842\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 8\nTraining loss: 0.10450286960059946 | Validation loss: 0.14469444751739502\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 9\nTraining loss: 0.0977388471364975 | Validation loss: 0.1382673680782318\nValidation loss (ends of cycles): [0.29694057]\n------------------------------\nEpoch: 10\nTraining loss: 0.0953068191354925 | Validation loss: 0.1350218504667282\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 11\nTraining loss: 0.09208406778899106 | Validation loss: 0.13349907100200653\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 12\nTraining loss: 0.08720466291362589 | Validation loss: 0.1306271106004715\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 13\nTraining loss: 0.08318295160477812 | Validation loss: 0.12258781492710114\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 14\nTraining loss: 0.0795554786243222 | Validation loss: 0.1179313138127327\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 15\nTraining loss: 0.0752499575980685 | Validation loss: 0.21455103158950806\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 16\nTraining loss: 0.07196846248751337 | Validation loss: 0.10781153291463852\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 17\nTraining loss: 0.07144239443269643 | Validation loss: 0.11611378192901611\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 18\nTraining loss: 0.061659027060324494 | Validation loss: 0.09441842883825302\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 19\nTraining loss: 0.0629522180692716 | Validation loss: 0.09655869007110596\nValidation loss (ends of cycles): [0.29694057 0.13502185]\n------------------------------\nEpoch: 20\nTraining loss: 0.056854997338219124 | Validation loss: 0.09356872737407684\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 21\nTraining loss: 0.05831065249036659 | Validation loss: 0.10779251158237457\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 22\nTraining loss: 0.05809896981174296 | Validation loss: 0.08884022384881973\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 23\nTraining loss: 0.05916342342441732 | Validation loss: 0.11716677248477936\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 24\nTraining loss: 0.060772972012108024 | Validation loss: 0.09863302856683731\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 25\nTraining loss: 0.06027982553297823 | Validation loss: 0.10262470692396164\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 26\nTraining loss: 0.06343780009245331 | Validation loss: 0.10198401659727097\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 27\nTraining loss: 0.057109395719387314 | Validation loss: 0.08557749539613724\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 28\nTraining loss: 0.05680512789298187 | Validation loss: 0.08958760648965836\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 29\nTraining loss: 0.05255689133297314 | Validation loss: 0.08726699650287628\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n------------------------------\nEpoch: 30\nTraining loss: 0.05449707111851736 | Validation loss: 0.0852150097489357\nValidation loss (ends of cycles): [0.29694057 0.13502185 0.09356873 0.08521501]\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.3403178215026855 | Validation loss: 0.22224114835262299\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 1\nTraining loss: 0.3237610936164856 | Validation loss: 0.2175069898366928\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 2\nTraining loss: 0.3036596342921257 | Validation loss: 0.2090708464384079\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 3\nTraining loss: 0.278908571600914 | Validation loss: 0.1986195296049118\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 4\nTraining loss: 0.24769878536462783 | Validation loss: 0.19036133587360382\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 5\nTraining loss: 0.2119302600622177 | Validation loss: 0.1850602626800537\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 6\nTraining loss: 0.17966288328170776 | Validation loss: 0.17428995668888092\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 7\nTraining loss: 0.1580403283238411 | Validation loss: 0.14990781247615814\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 8\nTraining loss: 0.1425566166639328 | Validation loss: 0.12038183957338333\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 9\nTraining loss: 0.1347818359732628 | Validation loss: 0.10062086582183838\nValidation loss (ends of cycles): [0.22224115]\n------------------------------\nEpoch: 10\nTraining loss: 0.13022587075829506 | Validation loss: 0.09236151725053787\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 11\nTraining loss: 0.128421900421381 | Validation loss: 0.08484034240245819\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 12\nTraining loss: 0.12211894914507866 | Validation loss: 0.0809514969587326\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 13\nTraining loss: 0.11671075597405434 | Validation loss: 0.09415645897388458\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 14\nTraining loss: 0.1062808632850647 | Validation loss: 0.0794791579246521\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 15\nTraining loss: 0.09774378426373005 | Validation loss: 0.07385452091693878\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 16\nTraining loss: 0.09059763960540294 | Validation loss: 0.06823520362377167\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 17\nTraining loss: 0.09244940280914307 | Validation loss: 0.10553069412708282\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 18\nTraining loss: 0.0844466209411621 | Validation loss: 0.06962426751852036\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 19\nTraining loss: 0.08120766542851925 | Validation loss: 0.06540507078170776\nValidation loss (ends of cycles): [0.22224115 0.09236152]\n------------------------------\nEpoch: 20\nTraining loss: 0.08298086747527122 | Validation loss: 0.06289994716644287\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 21\nTraining loss: 0.07931055799126625 | Validation loss: 0.06145286187529564\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 22\nTraining loss: 0.07616247236728668 | Validation loss: 0.05724290385842323\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 23\nTraining loss: 0.07831368632614613 | Validation loss: 0.061430856585502625\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 24\nTraining loss: 0.08261116072535515 | Validation loss: 0.1134483739733696\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 25\nTraining loss: 0.07242584079504014 | Validation loss: 0.07263419032096863\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 26\nTraining loss: 0.08369440548121929 | Validation loss: 0.05704887583851814\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 27\nTraining loss: 0.07497833706438542 | Validation loss: 0.05711729824542999\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 28\nTraining loss: 0.06945027969777584 | Validation loss: 0.05534932762384415\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 29\nTraining loss: 0.06933295093476773 | Validation loss: 0.054547473788261414\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n------------------------------\nEpoch: 30\nTraining loss: 0.06596663519740105 | Validation loss: 0.05319888889789581\nValidation loss (ends of cycles): [0.22224115 0.09236152 0.06289995 0.05319889]\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.3599785728888078 | Validation loss: 0.4432780146598816\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 1\nTraining loss: 0.3369556678967042 | Validation loss: 0.43250834941864014\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 2\nTraining loss: 0.30670858242295007 | Validation loss: 0.4114508032798767\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 3\nTraining loss: 0.272465627301823 | Validation loss: 0.3817843198776245\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 4\nTraining loss: 0.23151018267328088 | Validation loss: 0.3536441922187805\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 5\nTraining loss: 0.18663159283724698 | Validation loss: 0.32844647765159607\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 6\nTraining loss: 0.15000829168341376 | Validation loss: 0.28304171562194824\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 7\nTraining loss: 0.12784207612276077 | Validation loss: 0.21613815426826477\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 8\nTraining loss: 0.11503372409126976 | Validation loss: 0.16611164808273315\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 9\nTraining loss: 0.10725069452415813 | Validation loss: 0.15188439190387726\nValidation loss (ends of cycles): [0.44327801]\n------------------------------\nEpoch: 10\nTraining loss: 0.10527931221506813 | Validation loss: 0.13959506154060364\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 11\nTraining loss: 0.1036420301957564 | Validation loss: 0.1270444095134735\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 12\nTraining loss: 0.09993583234873685 | Validation loss: 0.1236078217625618\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 13\nTraining loss: 0.09465839510614221 | Validation loss: 0.13082820177078247\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 14\nTraining loss: 0.08756625313650478 | Validation loss: 0.10408846288919449\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 15\nTraining loss: 0.0838624509898099 | Validation loss: 0.08756735175848007\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 16\nTraining loss: 0.08244303072040732 | Validation loss: 0.09559406340122223\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 17\nTraining loss: 0.0741471800614487 | Validation loss: 0.10337742418050766\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 18\nTraining loss: 0.07662723992358554 | Validation loss: 0.06411650031805038\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 19\nTraining loss: 0.06983052329583601 | Validation loss: 0.07078924030065536\nValidation loss (ends of cycles): [0.44327801 0.13959506]\n------------------------------\nEpoch: 20\nTraining loss: 0.0687717121433128 | Validation loss: 0.06904434412717819\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 21\nTraining loss: 0.06762443178079346 | Validation loss: 0.06313017755746841\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 22\nTraining loss: 0.0678871947933327 | Validation loss: 0.067040354013443\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 23\nTraining loss: 0.06613235378807242 | Validation loss: 0.07850442826747894\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 24\nTraining loss: 0.06488648564978079 | Validation loss: 0.051457930356264114\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 25\nTraining loss: 0.0654671046544205 | Validation loss: 0.061340026557445526\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 26\nTraining loss: 0.06411620466546579 | Validation loss: 0.06412345170974731\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 27\nTraining loss: 0.060056577013297516 | Validation loss: 0.05233113467693329\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 28\nTraining loss: 0.05766375464471904 | Validation loss: 0.04654216393828392\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 29\nTraining loss: 0.05332683399319649 | Validation loss: 0.04950576648116112\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n------------------------------\nEpoch: 30\nTraining loss: 0.05111788822845979 | Validation loss: 0.05081142485141754\nValidation loss (ends of cycles): [0.44327801 0.13959506 0.06904434 0.05081142]\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.5454315461895682 | Validation loss: 0.4730074107646942\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 1\nTraining loss: 0.5257494151592255 | Validation loss: 0.4644114375114441\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 2\nTraining loss: 0.49964225021275604 | Validation loss: 0.4493078589439392\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 3\nTraining loss: 0.4701995172283866 | Validation loss: 0.4282127916812897\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 4\nTraining loss: 0.43141826445406134 | Validation loss: 0.4020078182220459\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 5\nTraining loss: 0.3827357996593822 | Validation loss: 0.362201452255249\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 6\nTraining loss: 0.3341675915501334 | Validation loss: 0.3249988853931427\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 7\nTraining loss: 0.2958235442638397 | Validation loss: 0.2761434316635132\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 8\nTraining loss: 0.2697145681489598 | Validation loss: 0.25178632140159607\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 9\nTraining loss: 0.25373841009356757 | Validation loss: 0.22395309805870056\nValidation loss (ends of cycles): [0.47300741]\n------------------------------\nEpoch: 10\nTraining loss: 0.24752120673656464 | Validation loss: 0.22707553207874298\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 11\nTraining loss: 0.24136914312839508 | Validation loss: 0.2116468846797943\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 12\nTraining loss: 0.2281808080998334 | Validation loss: 0.2141018956899643\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 13\nTraining loss: 0.20625835250724445 | Validation loss: 0.2063857465982437\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 14\nTraining loss: 0.1818149442022497 | Validation loss: 0.1678750216960907\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 15\nTraining loss: 0.1542614088817076 | Validation loss: 0.24311645328998566\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 16\nTraining loss: 0.13340282982045953 | Validation loss: 0.11346697062253952\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 17\nTraining loss: 0.11361510171131654 | Validation loss: 0.08446861058473587\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 18\nTraining loss: 0.10257813706994057 | Validation loss: 0.1038476824760437\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 19\nTraining loss: 0.09851626374504784 | Validation loss: 0.10009600967168808\nValidation loss (ends of cycles): [0.47300741 0.22707553]\n------------------------------\nEpoch: 20\nTraining loss: 0.09389783983880823 | Validation loss: 0.10324864834547043\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 21\nTraining loss: 0.09191824529658664 | Validation loss: 0.08838817477226257\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 22\nTraining loss: 0.08743048831820488 | Validation loss: 0.11275480687618256\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 23\nTraining loss: 0.08301741765304045 | Validation loss: 0.08835740387439728\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 24\nTraining loss: 0.07729637080972845 | Validation loss: 0.15197023749351501\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 25\nTraining loss: 0.07531545412811366 | Validation loss: 0.19605480134487152\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 26\nTraining loss: 0.0744694381613623 | Validation loss: 0.08726125210523605\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 27\nTraining loss: 0.07172296805815263 | Validation loss: 0.08883035182952881\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 28\nTraining loss: 0.05978721922094172 | Validation loss: 0.10437580943107605\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 29\nTraining loss: 0.058711225844242355 | Validation loss: 0.07623975723981857\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n------------------------------\nEpoch: 30\nTraining loss: 0.05624144473536448 | Validation loss: 0.0785088762640953\nValidation loss (ends of cycles): [0.47300741 0.22707553 0.10324865 0.07850888]\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.3115223288536072 | Validation loss: 0.35713261365890503\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 1\nTraining loss: 0.29661754220724107 | Validation loss: 0.3485311195254326\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 2\nTraining loss: 0.27701484113931657 | Validation loss: 0.3320363536477089\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 3\nTraining loss: 0.2542939618229866 | Validation loss: 0.3089185282588005\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 4\nTraining loss: 0.2231126084923744 | Validation loss: 0.28109800815582275\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 5\nTraining loss: 0.18675014078617097 | Validation loss: 0.2502268999814987\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 6\nTraining loss: 0.15313852950930595 | Validation loss: 0.2070472240447998\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 7\nTraining loss: 0.13002245053648948 | Validation loss: 0.19130465388298035\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 8\nTraining loss: 0.11442725360393524 | Validation loss: 0.20052699744701385\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 9\nTraining loss: 0.10721210837364196 | Validation loss: 0.21451064944267273\nValidation loss (ends of cycles): [0.35713261]\n------------------------------\nEpoch: 10\nTraining loss: 0.10336671397089958 | Validation loss: 0.21087735146284103\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 11\nTraining loss: 0.0994449395686388 | Validation loss: 0.20340285822749138\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 12\nTraining loss: 0.09221443757414818 | Validation loss: 0.22622137889266014\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 13\nTraining loss: 0.08567270934581757 | Validation loss: 0.2773045003414154\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 14\nTraining loss: 0.07911113798618316 | Validation loss: 0.43483686447143555\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 15\nTraining loss: 0.07443125247955322 | Validation loss: 0.1533229500055313\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 16\nTraining loss: 0.06726926937699318 | Validation loss: 0.1722557358443737\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 17\nTraining loss: 0.06305484026670456 | Validation loss: 0.21744874492287636\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 18\nTraining loss: 0.057345937564969064 | Validation loss: 0.2609431743621826\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 19\nTraining loss: 0.05563645977526903 | Validation loss: 0.2218475341796875\nValidation loss (ends of cycles): [0.35713261 0.21087735]\n------------------------------\nEpoch: 20\nTraining loss: 0.052960326336324214 | Validation loss: 0.19616811349987984\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 21\nTraining loss: 0.05901392139494419 | Validation loss: 0.1624758467078209\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 22\nTraining loss: 0.050959855690598486 | Validation loss: 0.22551586478948593\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 23\nTraining loss: 0.051076297834515574 | Validation loss: 0.2302572764456272\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 24\nTraining loss: 0.05112339500337839 | Validation loss: 0.21048244833946228\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 25\nTraining loss: 0.05467013940215111 | Validation loss: 0.12336333841085434\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 26\nTraining loss: 0.057834042236208916 | Validation loss: 0.2566063143312931\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 27\nTraining loss: 0.04988208692520857 | Validation loss: 0.22633014991879463\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 28\nTraining loss: 0.05284797567874193 | Validation loss: 0.21418332308530807\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 29\nTraining loss: 0.045893807895481586 | Validation loss: 0.1591845452785492\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n------------------------------\nEpoch: 30\nTraining loss: 0.0433046093210578 | Validation loss: 0.14972137287259102\nValidation loss (ends of cycles): [0.35713261 0.21087735 0.19616811 0.14972137]\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.5947945535182952 | Validation loss: 0.5636097490787506\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 1\nTraining loss: 0.5739877611398697 | Validation loss: 0.5552766025066376\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 2\nTraining loss: 0.5444172233343124 | Validation loss: 0.5409025847911835\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 3\nTraining loss: 0.5102303147315979 | Validation loss: 0.5223296880722046\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 4\nTraining loss: 0.4671860933303833 | Validation loss: 0.5054818987846375\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 5\nTraining loss: 0.4151140213012695 | Validation loss: 0.49210914969444275\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 6\nTraining loss: 0.3648500770330429 | Validation loss: 0.4792882353067398\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 7\nTraining loss: 0.3277033656835556 | Validation loss: 0.4582698494195938\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 8\nTraining loss: 0.30160123109817505 | Validation loss: 0.40672582387924194\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 9\nTraining loss: 0.28555808067321775 | Validation loss: 0.3488713800907135\nValidation loss (ends of cycles): [0.56360975]\n------------------------------\nEpoch: 10\nTraining loss: 0.2787675619125366 | Validation loss: 0.31419089436531067\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 11\nTraining loss: 0.27361820340156556 | Validation loss: 0.2943081259727478\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 12\nTraining loss: 0.26058380156755445 | Validation loss: 0.26867610216140747\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 13\nTraining loss: 0.24124586433172227 | Validation loss: 0.24037369340658188\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 14\nTraining loss: 0.21548341661691667 | Validation loss: 0.22535283118486404\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 15\nTraining loss: 0.18438775539398194 | Validation loss: 0.1685718446969986\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 16\nTraining loss: 0.16061918511986734 | Validation loss: 0.13537318259477615\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 17\nTraining loss: 0.14224363416433333 | Validation loss: 0.13322928547859192\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 18\nTraining loss: 0.13033997938036918 | Validation loss: 0.12760929018259048\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 19\nTraining loss: 0.12404320240020753 | Validation loss: 0.13474282249808311\nValidation loss (ends of cycles): [0.56360975 0.31419089]\n------------------------------\nEpoch: 20\nTraining loss: 0.11867297813296318 | Validation loss: 0.13434157520532608\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 21\nTraining loss: 0.11793936267495156 | Validation loss: 0.13654564321041107\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 22\nTraining loss: 0.11286920569837093 | Validation loss: 0.12956713140010834\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 23\nTraining loss: 0.10548075810074806 | Validation loss: 0.12283502891659737\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 24\nTraining loss: 0.09794336780905724 | Validation loss: 0.10408468917012215\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 25\nTraining loss: 0.0924295324832201 | Validation loss: 0.15008461475372314\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 26\nTraining loss: 0.09017585180699825 | Validation loss: 0.07894822582602501\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 27\nTraining loss: 0.08053090944886207 | Validation loss: 0.07444114610552788\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 28\nTraining loss: 0.07959246821701527 | Validation loss: 0.09047586098313332\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 29\nTraining loss: 0.0734288364648819 | Validation loss: 0.07841610908508301\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n------------------------------\nEpoch: 30\nTraining loss: 0.07281158864498138 | Validation loss: 0.07585301622748375\nValidation loss (ends of cycles): [0.56360975 0.31419089 0.13434158 0.07585302]\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.24966259002685548 | Validation loss: 0.24267390370368958\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 1\nTraining loss: 0.235618394613266 | Validation loss: 0.2372320294380188\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 2\nTraining loss: 0.21840053498744966 | Validation loss: 0.22739332169294357\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 3\nTraining loss: 0.19703763723373413 | Validation loss: 0.21525921672582626\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 4\nTraining loss: 0.1717545986175537 | Validation loss: 0.20377518981695175\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 5\nTraining loss: 0.14265027046203613 | Validation loss: 0.19772907346487045\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 6\nTraining loss: 0.11750846356153488 | Validation loss: 0.1927606761455536\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 7\nTraining loss: 0.10381745249032974 | Validation loss: 0.18614127486944199\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 8\nTraining loss: 0.0941122256219387 | Validation loss: 0.16012199968099594\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 9\nTraining loss: 0.09131616652011872 | Validation loss: 0.15815239399671555\nValidation loss (ends of cycles): [0.2426739]\n------------------------------\nEpoch: 10\nTraining loss: 0.08770758658647537 | Validation loss: 0.15310294181108475\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 11\nTraining loss: 0.08528880327939987 | Validation loss: 0.14396170154213905\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 12\nTraining loss: 0.08279691264033318 | Validation loss: 0.14130394160747528\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 13\nTraining loss: 0.08135882690548897 | Validation loss: 0.14968570321798325\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 14\nTraining loss: 0.08186716139316559 | Validation loss: 0.17318671941757202\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 15\nTraining loss: 0.08467233590781689 | Validation loss: 0.18237532302737236\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 16\nTraining loss: 0.0764297217130661 | Validation loss: 0.18119005113840103\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 17\nTraining loss: 0.07494740672409535 | Validation loss: 0.1301913782954216\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 18\nTraining loss: 0.07010909467935562 | Validation loss: 0.14423485472798347\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 19\nTraining loss: 0.0687748871743679 | Validation loss: 0.15484707802534103\nValidation loss (ends of cycles): [0.2426739  0.15310294]\n------------------------------\nEpoch: 20\nTraining loss: 0.0645689457654953 | Validation loss: 0.15342670306563377\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.062006055191159246 | Validation loss: 0.15138789266347885\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.06060033775866032 | Validation loss: 0.13369300588965416\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.061561013013124465 | Validation loss: 0.13530557602643967\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.06617009490728379 | Validation loss: 0.16354938223958015\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.06215674802660942 | Validation loss: 0.20111528038978577\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.059966203197836875 | Validation loss: 0.14292334020137787\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.059527568891644476 | Validation loss: 0.1250341236591339\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.05725446715950966 | Validation loss: 0.12287440150976181\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.054586905613541604 | Validation loss: 0.1597551889717579\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.05266690887510776 | Validation loss: 0.1473530475050211\nValidation loss (ends of cycles): [0.2426739  0.15310294 0.1534267  0.14735305]\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.41844989494843915 | Validation loss: 0.46415480971336365\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 1\nTraining loss: 0.39472986351359973 | Validation loss: 0.4504581391811371\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 2\nTraining loss: 0.35734637623483484 | Validation loss: 0.42483583092689514\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 3\nTraining loss: 0.31263969296758826 | Validation loss: 0.3877894878387451\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 4\nTraining loss: 0.2572383663871072 | Validation loss: 0.35185202956199646\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 5\nTraining loss: 0.19959831779653375 | Validation loss: 0.32558348774909973\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 6\nTraining loss: 0.15444783595475284 | Validation loss: 0.30798062682151794\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 7\nTraining loss: 0.12643937360156665 | Validation loss: 0.2347472906112671\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 8\nTraining loss: 0.1116909838535569 | Validation loss: 0.19630743563175201\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 9\nTraining loss: 0.10401421514424411 | Validation loss: 0.1785627156496048\nValidation loss (ends of cycles): [0.46415481]\n------------------------------\nEpoch: 10\nTraining loss: 0.10056576505303383 | Validation loss: 0.16850918531417847\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 11\nTraining loss: 0.0980552001432939 | Validation loss: 0.16101451218128204\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 12\nTraining loss: 0.09317369014024734 | Validation loss: 0.15137232840061188\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 13\nTraining loss: 0.08474765955047174 | Validation loss: 0.19075323641300201\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 14\nTraining loss: 0.08182131465185773 | Validation loss: 0.12207912653684616\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 15\nTraining loss: 0.08076543767343868 | Validation loss: 0.16828617453575134\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 16\nTraining loss: 0.07709567960013043 | Validation loss: 0.13723145425319672\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 17\nTraining loss: 0.07455341958186844 | Validation loss: 0.17370207607746124\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 18\nTraining loss: 0.06813155622644858 | Validation loss: 0.1326078176498413\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 19\nTraining loss: 0.06662491912191565 | Validation loss: 0.1284124106168747\nValidation loss (ends of cycles): [0.46415481 0.16850919]\n------------------------------\nEpoch: 20\nTraining loss: 0.06775809993798082 | Validation loss: 0.12708702683448792\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 21\nTraining loss: 0.06268392300063913 | Validation loss: 0.1257689744234085\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 22\nTraining loss: 0.06307559223337607 | Validation loss: 0.1285398006439209\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 23\nTraining loss: 0.06152626160870899 | Validation loss: 0.10883551836013794\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 24\nTraining loss: 0.06122888353737918 | Validation loss: 0.1875065714120865\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 25\nTraining loss: 0.06612172418019989 | Validation loss: 0.20511069893836975\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 26\nTraining loss: 0.06252996826713736 | Validation loss: 0.1681954264640808\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 27\nTraining loss: 0.06012651967731389 | Validation loss: 0.12634092569351196\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 28\nTraining loss: 0.052851350639354096 | Validation loss: 0.1592557728290558\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 29\nTraining loss: 0.05158912593668157 | Validation loss: 0.12818372249603271\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n------------------------------\nEpoch: 30\nTraining loss: 0.052270400591871956 | Validation loss: 0.12273656576871872\nValidation loss (ends of cycles): [0.46415481 0.16850919 0.12708703 0.12273657]\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.33450397036292334 | Validation loss: 0.4183863401412964\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 1\nTraining loss: 0.31508046659556305 | Validation loss: 0.4055172950029373\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 2\nTraining loss: 0.28755233775485645 | Validation loss: 0.38339458405971527\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 3\nTraining loss: 0.2563350403850729 | Validation loss: 0.35465146601200104\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 4\nTraining loss: 0.21771724657578903 | Validation loss: 0.32462094724178314\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 5\nTraining loss: 0.17677603594281457 | Validation loss: 0.2886727899312973\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 6\nTraining loss: 0.14331864734942262 | Validation loss: 0.25660841912031174\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 7\nTraining loss: 0.12253618409687822 | Validation loss: 0.20420953631401062\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 8\nTraining loss: 0.10970718989318068 | Validation loss: 0.16134043782949448\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 9\nTraining loss: 0.10511686839163303 | Validation loss: 0.136260487139225\nValidation loss (ends of cycles): [0.41838634]\n------------------------------\nEpoch: 10\nTraining loss: 0.10223364101892168 | Validation loss: 0.12221920490264893\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.09944640675728972 | Validation loss: 0.10605774819850922\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.09476278620687398 | Validation loss: 0.09146244078874588\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.09061784432692961 | Validation loss: 0.07169642113149166\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.08670035394077952 | Validation loss: 0.07171276770532131\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.08408490454100749 | Validation loss: 0.07400976028293371\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.07897272638299248 | Validation loss: 0.07414662837982178\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.07920655997639353 | Validation loss: 0.060708372853696346\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.0753878229721026 | Validation loss: 0.07837508991360664\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.07477120868861675 | Validation loss: 0.08723119460046291\nValidation loss (ends of cycles): [0.41838634 0.1222192 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.07195909118110483 | Validation loss: 0.07401845417916775\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 21\nTraining loss: 0.06806200789287686 | Validation loss: 0.064169991761446\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 22\nTraining loss: 0.07544434239918535 | Validation loss: 0.05839283112436533\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 23\nTraining loss: 0.06904576837339184 | Validation loss: 0.052823279052972794\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 24\nTraining loss: 0.07263032600960949 | Validation loss: 0.05023655481636524\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 25\nTraining loss: 0.06975930573588068 | Validation loss: 0.0573732815682888\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 26\nTraining loss: 0.07377178052609618 | Validation loss: 0.07613628916442394\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 27\nTraining loss: 0.06863558673384515 | Validation loss: 0.06069220509380102\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 28\nTraining loss: 0.0684154127639803 | Validation loss: 0.07957470044493675\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 29\nTraining loss: 0.06068193370645696 | Validation loss: 0.05438768491148949\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n------------------------------\nEpoch: 30\nTraining loss: 0.056117918638681825 | Validation loss: 0.0544932559132576\nValidation loss (ends of cycles): [0.41838634 0.1222192  0.07401845 0.05449326]\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2340872816064141 | Validation loss: 0.27015864849090576\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 1\nTraining loss: 0.21558139405467294 | Validation loss: 0.26325294375419617\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 2\nTraining loss: 0.19359120591120285 | Validation loss: 0.2500206530094147\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 3\nTraining loss: 0.17328757318583402 | Validation loss: 0.23665626347064972\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 4\nTraining loss: 0.1474518593062054 | Validation loss: 0.23030684888362885\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 5\nTraining loss: 0.12017566642977974 | Validation loss: 0.21531414985656738\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 6\nTraining loss: 0.10158516398885033 | Validation loss: 0.17602409422397614\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 7\nTraining loss: 0.0888047841462222 | Validation loss: 0.15436150133609772\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 8\nTraining loss: 0.08242148025469347 | Validation loss: 0.09863794595003128\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 9\nTraining loss: 0.07809551805257797 | Validation loss: 0.09693825244903564\nValidation loss (ends of cycles): [0.27015865]\n------------------------------\nEpoch: 10\nTraining loss: 0.0748989030041478 | Validation loss: 0.09435625374317169\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 11\nTraining loss: 0.07554080574349924 | Validation loss: 0.12675826251506805\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 12\nTraining loss: 0.07253992252729156 | Validation loss: 0.0890277624130249\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 13\nTraining loss: 0.06745774806900458 | Validation loss: 0.09933421015739441\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 14\nTraining loss: 0.06639396636323495 | Validation loss: 0.25477135181427\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 15\nTraining loss: 0.07153629342263396 | Validation loss: 0.12218687683343887\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 16\nTraining loss: 0.06362383981997316 | Validation loss: 0.09838857501745224\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 17\nTraining loss: 0.06499294090000066 | Validation loss: 0.12082856893539429\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 18\nTraining loss: 0.057603581385179 | Validation loss: 0.08688703179359436\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 19\nTraining loss: 0.054477095265280114 | Validation loss: 0.07416538149118423\nValidation loss (ends of cycles): [0.27015865 0.09435625]\n------------------------------\nEpoch: 20\nTraining loss: 0.053827235983176666 | Validation loss: 0.07726097851991653\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 21\nTraining loss: 0.05040800148113207 | Validation loss: 0.12319447845220566\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 22\nTraining loss: 0.04939531450244514 | Validation loss: 0.07437314093112946\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 23\nTraining loss: 0.04975901036099954 | Validation loss: 0.2664039731025696\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 24\nTraining loss: 0.05050407112999396 | Validation loss: 0.07585630565881729\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 25\nTraining loss: 0.052467319098385895 | Validation loss: 0.07166603952646255\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 26\nTraining loss: 0.04566661116074432 | Validation loss: 0.11543244868516922\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 27\nTraining loss: 0.048686319454149765 | Validation loss: 0.29796749353408813\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 28\nTraining loss: 0.04886364056305452 | Validation loss: 0.18778347969055176\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 29\nTraining loss: 0.04836133596572009 | Validation loss: 0.06662750244140625\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n------------------------------\nEpoch: 30\nTraining loss: 0.042628409510309044 | Validation loss: 0.06421707570552826\nValidation loss (ends of cycles): [0.27015865 0.09435625 0.07726098 0.06421708]\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.22647233442826706 | Validation loss: 0.23534537851810455\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 1\nTraining loss: 0.2113880677656694 | Validation loss: 0.23379206657409668\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 2\nTraining loss: 0.19402675059708682 | Validation loss: 0.23131398856639862\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 3\nTraining loss: 0.1751593459736217 | Validation loss: 0.22928957641124725\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 4\nTraining loss: 0.15052058547735214 | Validation loss: 0.22850777208805084\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 5\nTraining loss: 0.12454998357729478 | Validation loss: 0.22395320236682892\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 6\nTraining loss: 0.10516942563382062 | Validation loss: 0.19217549264431\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 7\nTraining loss: 0.09278485009616072 | Validation loss: 0.15953890979290009\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 8\nTraining loss: 0.08566174080426042 | Validation loss: 0.12811169028282166\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 9\nTraining loss: 0.08283901891925118 | Validation loss: 0.12298424541950226\nValidation loss (ends of cycles): [0.23534538]\n------------------------------\nEpoch: 10\nTraining loss: 0.08166876401413571 | Validation loss: 0.12157569825649261\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.07987238669937308 | Validation loss: 0.12463139742612839\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.07590313425118272 | Validation loss: 0.11578209698200226\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.0747046714479273 | Validation loss: 0.124772809445858\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.07252140851183371 | Validation loss: 0.11757127940654755\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.07997047291560606 | Validation loss: 0.11393731087446213\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.07546812854707241 | Validation loss: 0.1232791543006897\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.07227142189036716 | Validation loss: 0.11599378287792206\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.06743957068432462 | Validation loss: 0.10658861696720123\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.06617797978899696 | Validation loss: 0.10020831972360611\nValidation loss (ends of cycles): [0.23534538 0.1215757 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.06662786921316927 | Validation loss: 0.09910339117050171\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 21\nTraining loss: 0.0641032149168578 | Validation loss: 0.09659582376480103\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 22\nTraining loss: 0.06450243294239044 | Validation loss: 0.09174611419439316\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 23\nTraining loss: 0.06076653200117024 | Validation loss: 0.0868479311466217\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 24\nTraining loss: 0.057067283344539727 | Validation loss: 0.1249217838048935\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 25\nTraining loss: 0.06456780433654785 | Validation loss: 0.10062471777200699\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 26\nTraining loss: 0.060377268797971985 | Validation loss: 0.07824037969112396\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 27\nTraining loss: 0.05700255642560395 | Validation loss: 0.08699680119752884\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 28\nTraining loss: 0.053185255182060326 | Validation loss: 0.07531373202800751\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 29\nTraining loss: 0.05329576574943282 | Validation loss: 0.07017888128757477\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n------------------------------\nEpoch: 30\nTraining loss: 0.051533097231929954 | Validation loss: 0.06938274204730988\nValidation loss (ends of cycles): [0.23534538 0.1215757  0.09910339 0.06938274]\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.31496341824531554 | Validation loss: 0.3085462301969528\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 1\nTraining loss: 0.296749347448349 | Validation loss: 0.3101869523525238\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 2\nTraining loss: 0.2731067031621933 | Validation loss: 0.31405508518218994\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 3\nTraining loss: 0.24493784904479982 | Validation loss: 0.3216366618871689\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 4\nTraining loss: 0.21210817396640777 | Validation loss: 0.33603934943675995\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 5\nTraining loss: 0.17602172642946243 | Validation loss: 0.3545747697353363\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 6\nTraining loss: 0.14613406211137772 | Validation loss: 0.35567839443683624\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 7\nTraining loss: 0.1257573790848255 | Validation loss: 0.3237410634756088\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 8\nTraining loss: 0.11371021643280983 | Validation loss: 0.26202917098999023\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 9\nTraining loss: 0.10723200663924218 | Validation loss: 0.22564251720905304\nValidation loss (ends of cycles): [0.30854623]\n------------------------------\nEpoch: 10\nTraining loss: 0.10520545840263366 | Validation loss: 0.20360779762268066\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.10288014262914658 | Validation loss: 0.20160969346761703\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.09802041873335839 | Validation loss: 0.20292669162154198\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.09413954019546508 | Validation loss: 0.17354870960116386\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.08832933753728867 | Validation loss: 0.22394151240587234\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.08517145812511444 | Validation loss: 0.16865815967321396\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.08164194636046887 | Validation loss: 0.16437028348445892\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.08001556545495987 | Validation loss: 0.20558318868279457\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.0778819728642702 | Validation loss: 0.183094821870327\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.07783219516277314 | Validation loss: 0.1785370595753193\nValidation loss (ends of cycles): [0.30854623 0.2036078 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.07695055566728115 | Validation loss: 0.17569859325885773\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 21\nTraining loss: 0.07580824568867683 | Validation loss: 0.20672567933797836\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 22\nTraining loss: 0.07075491286814213 | Validation loss: 0.17038242146372795\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 23\nTraining loss: 0.07162440195679665 | Validation loss: 0.17492885142564774\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 24\nTraining loss: 0.06862989962100982 | Validation loss: 0.1177842915058136\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 25\nTraining loss: 0.06372749842703343 | Validation loss: 0.09121009334921837\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 26\nTraining loss: 0.06439065597951413 | Validation loss: 0.11329861357808113\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 27\nTraining loss: 0.05956905409693718 | Validation loss: 0.1521657519042492\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 28\nTraining loss: 0.054637243039906025 | Validation loss: 0.065576933324337\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 29\nTraining loss: 0.06076151393353939 | Validation loss: 0.058655871078372\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n------------------------------\nEpoch: 30\nTraining loss: 0.05368628203868866 | Validation loss: 0.06157389655709267\nValidation loss (ends of cycles): [0.30854623 0.2036078  0.17569859 0.0615739 ]\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.3849785625934601 | Validation loss: 0.45114198327064514\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 1\nTraining loss: 0.3704269663854079 | Validation loss: 0.44007259607315063\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 2\nTraining loss: 0.3476231748407537 | Validation loss: 0.4194194972515106\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 3\nTraining loss: 0.31672785905274475 | Validation loss: 0.39160406589508057\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 4\nTraining loss: 0.27650914002548566 | Validation loss: 0.35673192143440247\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 5\nTraining loss: 0.2297362427819859 | Validation loss: 0.31353193521499634\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 6\nTraining loss: 0.18639830432154916 | Validation loss: 0.27488814294338226\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 7\nTraining loss: 0.15894355489449066 | Validation loss: 0.2300477847456932\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 8\nTraining loss: 0.1421263353391127 | Validation loss: 0.18800196796655655\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 9\nTraining loss: 0.13018270107832822 | Validation loss: 0.1685379222035408\nValidation loss (ends of cycles): [0.45114198]\n------------------------------\nEpoch: 10\nTraining loss: 0.1273436485366388 | Validation loss: 0.15773064270615578\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 11\nTraining loss: 0.12412191317840056 | Validation loss: 0.15657923743128777\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 12\nTraining loss: 0.11569374122402885 | Validation loss: 0.14440815895795822\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 13\nTraining loss: 0.10674315149133856 | Validation loss: 0.08752488531172276\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 14\nTraining loss: 0.09485080702738329 | Validation loss: 0.08425725065171719\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 15\nTraining loss: 0.08512867479161783 | Validation loss: 0.06908361706882715\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 16\nTraining loss: 0.07970026880502701 | Validation loss: 0.12750844284892082\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 17\nTraining loss: 0.07524278997020288 | Validation loss: 0.07422960735857487\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 18\nTraining loss: 0.07073394175280225 | Validation loss: 0.06022882554680109\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 19\nTraining loss: 0.06288122284141454 | Validation loss: 0.06339698284864426\nValidation loss (ends of cycles): [0.45114198 0.15773064]\n------------------------------\nEpoch: 20\nTraining loss: 0.061651048165830696 | Validation loss: 0.06467864662408829\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 21\nTraining loss: 0.06111889738928188 | Validation loss: 0.06646665744483471\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 22\nTraining loss: 0.05873148346489126 | Validation loss: 0.06867950409650803\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 23\nTraining loss: 0.060050982643257485 | Validation loss: 0.061418455094099045\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 24\nTraining loss: 0.060610766437920655 | Validation loss: 0.05618366505950689\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 25\nTraining loss: 0.0633573637089946 | Validation loss: 0.04861140996217728\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 26\nTraining loss: 0.06116855178367008 | Validation loss: 0.07926485501229763\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 27\nTraining loss: 0.05911888656291095 | Validation loss: 0.058403012342751026\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 28\nTraining loss: 0.05666702342304317 | Validation loss: 0.042423720471560955\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 29\nTraining loss: 0.05089438266374848 | Validation loss: 0.03783706063404679\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n------------------------------\nEpoch: 30\nTraining loss: 0.05125425891442732 | Validation loss: 0.04267655219882727\nValidation loss (ends of cycles): [0.45114198 0.15773064 0.06467865 0.04267655]\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.41509213149547575 | Validation loss: 0.38505399227142334\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 1\nTraining loss: 0.39179774820804597 | Validation loss: 0.37304168939590454\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 2\nTraining loss: 0.35843105912208556 | Validation loss: 0.35156017541885376\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 3\nTraining loss: 0.31765433847904206 | Validation loss: 0.32422196865081787\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 4\nTraining loss: 0.2673868998885155 | Validation loss: 0.2933422923088074\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 5\nTraining loss: 0.21240467727184295 | Validation loss: 0.26028379797935486\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 6\nTraining loss: 0.1678038567304611 | Validation loss: 0.2316894382238388\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 7\nTraining loss: 0.1405745640397072 | Validation loss: 0.1750834584236145\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 8\nTraining loss: 0.12251835465431213 | Validation loss: 0.1384737491607666\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 9\nTraining loss: 0.11450750157237052 | Validation loss: 0.10572589933872223\nValidation loss (ends of cycles): [0.38505399]\n------------------------------\nEpoch: 10\nTraining loss: 0.10943077206611633 | Validation loss: 0.08536231517791748\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 11\nTraining loss: 0.10874983444809913 | Validation loss: 0.06326284259557724\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 12\nTraining loss: 0.10184967592358589 | Validation loss: 0.05090981721878052\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 13\nTraining loss: 0.09422398544847965 | Validation loss: 0.05360788851976395\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 14\nTraining loss: 0.09096146337687969 | Validation loss: 0.1130673959851265\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 15\nTraining loss: 0.0848462775349617 | Validation loss: 0.1005302146077156\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 16\nTraining loss: 0.08346045427024365 | Validation loss: 0.04016828536987305\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 17\nTraining loss: 0.08220213800668716 | Validation loss: 0.05280933529138565\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 18\nTraining loss: 0.07907124496996402 | Validation loss: 0.04327217862010002\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 19\nTraining loss: 0.07301834337413311 | Validation loss: 0.041828058660030365\nValidation loss (ends of cycles): [0.38505399 0.08536232]\n------------------------------\nEpoch: 20\nTraining loss: 0.0685610394924879 | Validation loss: 0.04052760452032089\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.06799696236848832 | Validation loss: 0.036265864968299866\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.06856372691690922 | Validation loss: 0.03583543002605438\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.06802004836499691 | Validation loss: 0.03715496510267258\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.06912181153893471 | Validation loss: 0.04452311620116234\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.0747891653329134 | Validation loss: 0.12103398144245148\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.06961565390229225 | Validation loss: 0.17962095141410828\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.06380176991224289 | Validation loss: 0.07026810944080353\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.06431049816310405 | Validation loss: 0.0371607169508934\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.06408782787621022 | Validation loss: 0.04384800046682358\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.05868135690689087 | Validation loss: 0.03950566053390503\nValidation loss (ends of cycles): [0.38505399 0.08536232 0.0405276  0.03950566]\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.497423380613327 | Validation loss: 0.5236005187034607\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 1\nTraining loss: 0.47451717203313654 | Validation loss: 0.5123611688613892\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 2\nTraining loss: 0.4432302469556982 | Validation loss: 0.4904400110244751\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 3\nTraining loss: 0.4015417017719962 | Validation loss: 0.4578700065612793\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 4\nTraining loss: 0.34881053458560596 | Validation loss: 0.41512584686279297\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 5\nTraining loss: 0.2884210998361761 | Validation loss: 0.37235116958618164\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 6\nTraining loss: 0.23100726983763956 | Validation loss: 0.3235102891921997\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 7\nTraining loss: 0.19382140717723154 | Validation loss: 0.24588394165039062\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 8\nTraining loss: 0.16921459138393402 | Validation loss: 0.187604621052742\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 9\nTraining loss: 0.15417007424614645 | Validation loss: 0.20316246151924133\nValidation loss (ends of cycles): [0.52360052]\n------------------------------\nEpoch: 10\nTraining loss: 0.14732958579605276 | Validation loss: 0.17677535116672516\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 11\nTraining loss: 0.1463459621776234 | Validation loss: 0.14159077405929565\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 12\nTraining loss: 0.13318475064906207 | Validation loss: 0.181656152009964\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 13\nTraining loss: 0.12301303310827776 | Validation loss: 0.10158068686723709\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 14\nTraining loss: 0.10729351978410374 | Validation loss: 0.09514954686164856\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 15\nTraining loss: 0.09595484286546707 | Validation loss: 0.08995315432548523\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 16\nTraining loss: 0.08417292159389365 | Validation loss: 0.0888112485408783\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 17\nTraining loss: 0.08165155740624125 | Validation loss: 0.0818200558423996\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 18\nTraining loss: 0.0723276978189295 | Validation loss: 0.08610425889492035\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 19\nTraining loss: 0.07191044092178345 | Validation loss: 0.08774658292531967\nValidation loss (ends of cycles): [0.52360052 0.17677535]\n------------------------------\nEpoch: 20\nTraining loss: 0.06833597780628638 | Validation loss: 0.08297054469585419\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 21\nTraining loss: 0.06698174876245586 | Validation loss: 0.07742486894130707\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 22\nTraining loss: 0.06470589475198225 | Validation loss: 0.07569701969623566\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 23\nTraining loss: 0.06563659706576304 | Validation loss: 0.090906523168087\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 24\nTraining loss: 0.06569409421221777 | Validation loss: 0.08477133512496948\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 25\nTraining loss: 0.06704503907398744 | Validation loss: 0.09006834775209427\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 26\nTraining loss: 0.07161630249836227 | Validation loss: 0.07447994500398636\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 27\nTraining loss: 0.06389857828617096 | Validation loss: 0.0818374902009964\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 28\nTraining loss: 0.060700812631032684 | Validation loss: 0.06722551584243774\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 29\nTraining loss: 0.05577064026147127 | Validation loss: 0.07123492658138275\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n------------------------------\nEpoch: 30\nTraining loss: 0.05637003786184571 | Validation loss: 0.0676247850060463\nValidation loss (ends of cycles): [0.52360052 0.17677535 0.08297054 0.06762479]\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.47987140308726917 | Validation loss: 0.4334615617990494\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 1\nTraining loss: 0.45977337793870404 | Validation loss: 0.42151939868927\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 2\nTraining loss: 0.42812349579551 | Validation loss: 0.39918772876262665\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 3\nTraining loss: 0.3865386030890725 | Validation loss: 0.3691086918115616\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 4\nTraining loss: 0.33454837040467694 | Validation loss: 0.3325386643409729\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 5\nTraining loss: 0.27320032634518365 | Validation loss: 0.29823416471481323\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 6\nTraining loss: 0.22226029634475708 | Validation loss: 0.2558329254388809\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 7\nTraining loss: 0.18792442774230783 | Validation loss: 0.22173385322093964\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 8\nTraining loss: 0.16651706397533417 | Validation loss: 0.18216048181056976\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 9\nTraining loss: 0.1563743305477229 | Validation loss: 0.15323598682880402\nValidation loss (ends of cycles): [0.43346156]\n------------------------------\nEpoch: 10\nTraining loss: 0.15220273286104202 | Validation loss: 0.14952246099710464\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 11\nTraining loss: 0.14907611906528473 | Validation loss: 0.15525143593549728\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 12\nTraining loss: 0.14079013602300125 | Validation loss: 0.12134148925542831\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 13\nTraining loss: 0.12847822565924039 | Validation loss: 0.15095221251249313\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 14\nTraining loss: 0.11500959301536734 | Validation loss: 0.09931726008653641\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 15\nTraining loss: 0.10405388474464417 | Validation loss: 0.12409628182649612\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 16\nTraining loss: 0.10094385120001706 | Validation loss: 0.10372162610292435\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 17\nTraining loss: 0.09531495787880638 | Validation loss: 0.09211777150630951\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 18\nTraining loss: 0.08857555281032216 | Validation loss: 0.09498467668890953\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 19\nTraining loss: 0.0935635573484681 | Validation loss: 0.09667631611227989\nValidation loss (ends of cycles): [0.43346156 0.14952246]\n------------------------------\nEpoch: 20\nTraining loss: 0.08716208182952621 | Validation loss: 0.09361663460731506\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 21\nTraining loss: 0.086543853987347 | Validation loss: 0.0879531130194664\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 22\nTraining loss: 0.08200569958849387 | Validation loss: 0.08755558729171753\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 23\nTraining loss: 0.085517230020328 | Validation loss: 0.08203435316681862\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 24\nTraining loss: 0.0809919996695085 | Validation loss: 0.07936260104179382\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 25\nTraining loss: 0.08055772056633775 | Validation loss: 0.08800634741783142\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 26\nTraining loss: 0.07741718976335092 | Validation loss: 0.08297253772616386\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 27\nTraining loss: 0.07764130017974159 | Validation loss: 0.07773812860250473\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 28\nTraining loss: 0.07542369074442169 | Validation loss: 0.0761253871023655\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 29\nTraining loss: 0.07147530669515784 | Validation loss: 0.07653047516942024\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n------------------------------\nEpoch: 30\nTraining loss: 0.07272762432694435 | Validation loss: 0.07702647522091866\nValidation loss (ends of cycles): [0.43346156 0.14952246 0.09361663 0.07702648]\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.5610803931951522 | Validation loss: 0.5742125511169434\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 1\nTraining loss: 0.5384890228509903 | Validation loss: 0.5626022219657898\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 2\nTraining loss: 0.5053218245506287 | Validation loss: 0.5410920977592468\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 3\nTraining loss: 0.4658749431371689 | Validation loss: 0.5129314661026001\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 4\nTraining loss: 0.4172980934381485 | Validation loss: 0.4840275049209595\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 5\nTraining loss: 0.3584092140197754 | Validation loss: 0.45695602893829346\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 6\nTraining loss: 0.3014706075191498 | Validation loss: 0.43395182490348816\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 7\nTraining loss: 0.26016793251037595 | Validation loss: 0.3845180869102478\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 8\nTraining loss: 0.23394290506839752 | Validation loss: 0.3097478747367859\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 9\nTraining loss: 0.21567025035619736 | Validation loss: 0.23616959154605865\nValidation loss (ends of cycles): [0.57421255]\n------------------------------\nEpoch: 10\nTraining loss: 0.20982832312583924 | Validation loss: 0.19686532020568848\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 11\nTraining loss: 0.20462629497051238 | Validation loss: 0.1726258248090744\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 12\nTraining loss: 0.19207933992147447 | Validation loss: 0.17744328081607819\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 13\nTraining loss: 0.17268036156892777 | Validation loss: 0.12105994671583176\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 14\nTraining loss: 0.14910297393798827 | Validation loss: 0.07750903069972992\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 15\nTraining loss: 0.12406309843063354 | Validation loss: 0.06648653000593185\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 16\nTraining loss: 0.1091826967895031 | Validation loss: 0.051474422216415405\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 17\nTraining loss: 0.09453097954392434 | Validation loss: 0.09065815806388855\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 18\nTraining loss: 0.08696112185716628 | Validation loss: 0.05250278487801552\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 19\nTraining loss: 0.08315690755844116 | Validation loss: 0.047353047877550125\nValidation loss (ends of cycles): [0.57421255 0.19686532]\n------------------------------\nEpoch: 20\nTraining loss: 0.08215439356863499 | Validation loss: 0.04667212441563606\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 21\nTraining loss: 0.08283475637435914 | Validation loss: 0.04465808719396591\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 22\nTraining loss: 0.07634258382022381 | Validation loss: 0.05145259574055672\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 23\nTraining loss: 0.07885098718106746 | Validation loss: 0.055073946714401245\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 24\nTraining loss: 0.07556320875883102 | Validation loss: 0.0855465829372406\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 25\nTraining loss: 0.07224686443805695 | Validation loss: 0.0590750053524971\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 26\nTraining loss: 0.07677309662103653 | Validation loss: 0.04134538397192955\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 27\nTraining loss: 0.06744715496897698 | Validation loss: 0.03748737648129463\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 28\nTraining loss: 0.06501720324158669 | Validation loss: 0.03147929161787033\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 29\nTraining loss: 0.06560679152607918 | Validation loss: 0.031396884471178055\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n------------------------------\nEpoch: 30\nTraining loss: 0.06533972807228565 | Validation loss: 0.027407711371779442\nValidation loss (ends of cycles): [0.57421255 0.19686532 0.04667212 0.02740771]\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.5691605454141443 | Validation loss: 0.608923077583313\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 1\nTraining loss: 0.5476431304758246 | Validation loss: 0.6018098592758179\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 2\nTraining loss: 0.5234504071148959 | Validation loss: 0.589514970779419\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 3\nTraining loss: 0.49886292490092193 | Validation loss: 0.5730634927749634\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 4\nTraining loss: 0.46711285276846454 | Validation loss: 0.5529573559761047\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 5\nTraining loss: 0.42809410799633374 | Validation loss: 0.5257182121276855\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 6\nTraining loss: 0.387255614454096 | Validation loss: 0.49099910259246826\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 7\nTraining loss: 0.35546800223263825 | Validation loss: 0.43925637006759644\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 8\nTraining loss: 0.3322257250547409 | Validation loss: 0.4052078127861023\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 9\nTraining loss: 0.3180681358684193 | Validation loss: 0.38430100679397583\nValidation loss (ends of cycles): [0.60892308]\n------------------------------\nEpoch: 10\nTraining loss: 0.31182124533436517 | Validation loss: 0.37427014112472534\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 11\nTraining loss: 0.30706333301284094 | Validation loss: 0.35669952630996704\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 12\nTraining loss: 0.29335934736511926 | Validation loss: 0.3390655219554901\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 13\nTraining loss: 0.27410248057408765 | Validation loss: 0.31075388193130493\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 14\nTraining loss: 0.2465149394490502 | Validation loss: 0.2842131555080414\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 15\nTraining loss: 0.2152640623125163 | Validation loss: 0.22721347212791443\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 16\nTraining loss: 0.18463559042323718 | Validation loss: 0.24249565601348877\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 17\nTraining loss: 0.16122723370790482 | Validation loss: 0.15358808636665344\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 18\nTraining loss: 0.14679778096350757 | Validation loss: 0.17089605331420898\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 19\nTraining loss: 0.13611172512173653 | Validation loss: 0.15524759888648987\nValidation loss (ends of cycles): [0.60892308 0.37427014]\n------------------------------\nEpoch: 20\nTraining loss: 0.13251594386317514 | Validation loss: 0.17342492938041687\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 21\nTraining loss: 0.12898269871419127 | Validation loss: 0.16011351346969604\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 22\nTraining loss: 0.12305541607466611 | Validation loss: 0.1949668526649475\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 23\nTraining loss: 0.11235611580989578 | Validation loss: 0.10236149281263351\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 24\nTraining loss: 0.10381116582588716 | Validation loss: 0.13342885673046112\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 25\nTraining loss: 0.08670141505585476 | Validation loss: 0.09188296645879745\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 26\nTraining loss: 0.08180331498045813 | Validation loss: 0.12372232973575592\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 27\nTraining loss: 0.07200923278419809 | Validation loss: 0.07592625916004181\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 28\nTraining loss: 0.066264333457432 | Validation loss: 0.0874938890337944\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 29\nTraining loss: 0.06131140947003256 | Validation loss: 0.07372075319290161\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n------------------------------\nEpoch: 30\nTraining loss: 0.06025365598245778 | Validation loss: 0.07412300258874893\nValidation loss (ends of cycles): [0.60892308 0.37427014 0.17342493 0.074123  ]\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.5367445856332779 | Validation loss: 0.4483487606048584\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 1\nTraining loss: 0.5206190437078476 | Validation loss: 0.44038158655166626\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 2\nTraining loss: 0.4976606220006943 | Validation loss: 0.4257300943136215\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 3\nTraining loss: 0.4683428943157196 | Validation loss: 0.4045237749814987\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 4\nTraining loss: 0.43049020171165464 | Validation loss: 0.382641464471817\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 5\nTraining loss: 0.3816151052713394 | Validation loss: 0.35842086374759674\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 6\nTraining loss: 0.3326111823320389 | Validation loss: 0.3364621549844742\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 7\nTraining loss: 0.29561977237463 | Validation loss: 0.318710595369339\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 8\nTraining loss: 0.26997964084148407 | Validation loss: 0.2970084324479103\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 9\nTraining loss: 0.2537916973233223 | Validation loss: 0.27319707721471786\nValidation loss (ends of cycles): [0.44834876]\n------------------------------\nEpoch: 10\nTraining loss: 0.24661508649587632 | Validation loss: 0.25705454498529434\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 11\nTraining loss: 0.2426185429096222 | Validation loss: 0.22602836042642593\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 12\nTraining loss: 0.22818358689546586 | Validation loss: 0.2005477249622345\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 13\nTraining loss: 0.2082375779747963 | Validation loss: 0.19975723326206207\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 14\nTraining loss: 0.1813648521900177 | Validation loss: 0.1895284503698349\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 15\nTraining loss: 0.15327624678611756 | Validation loss: 0.1219119057059288\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 16\nTraining loss: 0.12782533541321756 | Validation loss: 0.062152622267603874\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 17\nTraining loss: 0.10957009494304656 | Validation loss: 0.1485811546444893\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 18\nTraining loss: 0.0981064923107624 | Validation loss: 0.06758278980851173\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 19\nTraining loss: 0.0926601156592369 | Validation loss: 0.09994453191757202\nValidation loss (ends of cycles): [0.44834876 0.25705454]\n------------------------------\nEpoch: 20\nTraining loss: 0.08913373351097106 | Validation loss: 0.1018301472067833\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 21\nTraining loss: 0.086388049274683 | Validation loss: 0.08931709080934525\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 22\nTraining loss: 0.08263651877641678 | Validation loss: 0.1043701171875\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 23\nTraining loss: 0.07813577800989151 | Validation loss: 0.061198340728878975\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 24\nTraining loss: 0.07279523089528084 | Validation loss: 0.0668635182082653\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 25\nTraining loss: 0.06805610843002796 | Validation loss: 0.5645378977060318\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 26\nTraining loss: 0.06686716079711914 | Validation loss: 0.06682540383189917\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 27\nTraining loss: 0.059197721630334856 | Validation loss: 0.0642244964838028\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 28\nTraining loss: 0.06171116679906845 | Validation loss: 0.2145145758986473\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 29\nTraining loss: 0.056293224170804027 | Validation loss: 0.05585295893251896\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n------------------------------\nEpoch: 30\nTraining loss: 0.0526149595156312 | Validation loss: 0.053398482501506805\nValidation loss (ends of cycles): [0.44834876 0.25705454 0.10183015 0.05339848]\n\n\n\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/models')\norder = 12\nseeds = range(20)\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_local_gelisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                  order = order,\n                                                  src_dir_model=src_dir_model)\n\nperfs_local_gelisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n      18.000000\n    \n    \n      mean\n      1.887493\n      2.754606\n      0.691143\n      0.805993\n      0.657956\n      0.461482\n      0.346751\n      51.508686\n      -0.009657\n      -0.016915\n    \n    \n      std\n      0.330355\n      0.728265\n      0.088094\n      0.057573\n      0.173942\n      0.263369\n      0.094363\n      12.377707\n      0.048598\n      0.078858\n    \n    \n      min\n      1.547348\n      1.282034\n      0.572637\n      0.714614\n      0.376275\n      0.141583\n      0.181809\n      34.942499\n      -0.114649\n      -0.215123\n    \n    \n      25%\n      1.642766\n      2.256554\n      0.619215\n      0.753152\n      0.574179\n      0.329684\n      0.303541\n      43.296409\n      -0.029528\n      -0.041320\n    \n    \n      50%\n      1.817055\n      2.608120\n      0.690513\n      0.805246\n      0.617902\n      0.381848\n      0.335921\n      47.255385\n      -0.001811\n      -0.002683\n    \n    \n      75%\n      2.033550\n      3.186851\n      0.752098\n      0.844316\n      0.728023\n      0.530550\n      0.386050\n      61.034292\n      0.016324\n      0.030791\n    \n    \n      max\n      2.791070\n      4.049294\n      0.867742\n      0.920004\n      1.118009\n      1.249944\n      0.565283\n      73.572367\n      0.079008\n      0.092876\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nTrain and test on Vertisols\n\n# Replace following Paths with yours\ndest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/losses')\ndest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/models')\n\norder = 10\nseeds = range(20) \nn_epochs = 31\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               dest_dir_loss=dest_dir_loss,\n               dest_dir_model=dest_dir_model,\n               n_epochs=n_epochs,\n               sc_kwargs=params_scheduler)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.161204216511626 | Validation loss: 0.1679159700870514\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 1\nTraining loss: 0.15437143099935433 | Validation loss: 0.15762153267860413\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 2\nTraining loss: 0.13889641942162262 | Validation loss: 0.13728273659944534\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 3\nTraining loss: 0.11834369207683362 | Validation loss: 0.10279234126210213\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 4\nTraining loss: 0.09297808573434227 | Validation loss: 0.09408992528915405\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 5\nTraining loss: 0.07264437330396552 | Validation loss: 0.055685702711343765\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 6\nTraining loss: 0.06562904072435279 | Validation loss: 0.08587116375565529\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 7\nTraining loss: 0.055855809269767055 | Validation loss: 0.0615625474601984\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 8\nTraining loss: 0.05285423092151943 | Validation loss: 0.05325787328183651\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 9\nTraining loss: 0.05118850050003905 | Validation loss: 0.06249404326081276\nValidation loss (ends of cycles): [0.16791597]\n------------------------------\nEpoch: 10\nTraining loss: 0.04790492297003144 | Validation loss: 0.05141059495508671\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 11\nTraining loss: 0.04692842046681203 | Validation loss: 0.05575957149267197\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 12\nTraining loss: 0.04742170605612429 | Validation loss: 0.048377299681305885\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 13\nTraining loss: 0.045442073164801845 | Validation loss: 0.05056057125329971\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 14\nTraining loss: 0.049288692442994365 | Validation loss: 0.045949578285217285\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 15\nTraining loss: 0.04942320875431362 | Validation loss: 0.046182602643966675\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 16\nTraining loss: 0.048454557790568 | Validation loss: 0.061180008575320244\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 17\nTraining loss: 0.04570011774960317 | Validation loss: 0.05048673413693905\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 18\nTraining loss: 0.04413507094508723 | Validation loss: 0.049398086965084076\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 19\nTraining loss: 0.03897152105836492 | Validation loss: 0.04086455702781677\nValidation loss (ends of cycles): [0.16791597 0.05141059]\n------------------------------\nEpoch: 20\nTraining loss: 0.03762085471106203 | Validation loss: 0.04161454364657402\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 21\nTraining loss: 0.03897195544682051 | Validation loss: 0.04463193938136101\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 22\nTraining loss: 0.03908969050175265 | Validation loss: 0.047162629663944244\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 23\nTraining loss: 0.03908744160281984 | Validation loss: 0.05538894981145859\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 24\nTraining loss: 0.03990407954705389 | Validation loss: 0.04268927872180939\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 25\nTraining loss: 0.03997622842067167 | Validation loss: 0.045263996347784996\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 26\nTraining loss: 0.04153041404328848 | Validation loss: 0.042628781870007515\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 27\nTraining loss: 0.034978562671887245 | Validation loss: 0.043797941878437996\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 28\nTraining loss: 0.03331961276891984 | Validation loss: 0.038327883929014206\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 29\nTraining loss: 0.03273361440944044 | Validation loss: 0.03366365935653448\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n------------------------------\nEpoch: 30\nTraining loss: 0.029028360584848804 | Validation loss: 0.0335803534835577\nValidation loss (ends of cycles): [0.16791597 0.05141059 0.04161454 0.03358035]\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08880109026243813 | Validation loss: 0.08211258985102177\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 1\nTraining loss: 0.08559589107569895 | Validation loss: 0.07982487976551056\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 2\nTraining loss: 0.08120929841932498 | Validation loss: 0.07661886140704155\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 3\nTraining loss: 0.07261521545679946 | Validation loss: 0.07628549262881279\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 4\nTraining loss: 0.06738892393676858 | Validation loss: 0.06264040432870388\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 5\nTraining loss: 0.06283781206921528 | Validation loss: 0.05332220159471035\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 6\nTraining loss: 0.0563666181344735 | Validation loss: 0.052990976721048355\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 7\nTraining loss: 0.053898568314157035 | Validation loss: 0.05359513498842716\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 8\nTraining loss: 0.0495483853707188 | Validation loss: 0.04845046065747738\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 9\nTraining loss: 0.04790328227375683 | Validation loss: 0.04489264823496342\nValidation loss (ends of cycles): [0.08211259]\n------------------------------\nEpoch: 10\nTraining loss: 0.0470738457025666 | Validation loss: 0.044814372435212135\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 11\nTraining loss: 0.04557334945390099 | Validation loss: 0.045520488172769547\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 12\nTraining loss: 0.04488633739712991 | Validation loss: 0.045287614688277245\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 13\nTraining loss: 0.04274003678246548 | Validation loss: 0.05250200070440769\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 14\nTraining loss: 0.04469038634315917 | Validation loss: 0.06699041835963726\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 15\nTraining loss: 0.04659118973895123 | Validation loss: 0.0444390494376421\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 16\nTraining loss: 0.045118058786580435 | Validation loss: 0.04487036541104317\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 17\nTraining loss: 0.04162646614407238 | Validation loss: 0.04729745723307133\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 18\nTraining loss: 0.040547048084829986 | Validation loss: 0.039926101453602314\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 19\nTraining loss: 0.03623779441573118 | Validation loss: 0.03939523547887802\nValidation loss (ends of cycles): [0.08211259 0.04481437]\n------------------------------\nEpoch: 20\nTraining loss: 0.03616418807130111 | Validation loss: 0.03911025729030371\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 21\nTraining loss: 0.035900585745510305 | Validation loss: 0.03916540555655956\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 22\nTraining loss: 0.03519123988716226 | Validation loss: 0.03917317185550928\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 23\nTraining loss: 0.03432324999257138 | Validation loss: 0.04085913486778736\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 24\nTraining loss: 0.036748546616811505 | Validation loss: 0.041731780394911766\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 25\nTraining loss: 0.037517647111886425 | Validation loss: 0.09109430015087128\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 26\nTraining loss: 0.03717169126397685 | Validation loss: 0.04643022455275059\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 27\nTraining loss: 0.03365675114879483 | Validation loss: 0.039381884038448334\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 28\nTraining loss: 0.03359140867465421 | Validation loss: 0.035449360497295856\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 29\nTraining loss: 0.029328686920435804 | Validation loss: 0.03771654795855284\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n------------------------------\nEpoch: 30\nTraining loss: 0.029815007001161575 | Validation loss: 0.035608227364718914\nValidation loss (ends of cycles): [0.08211259 0.04481437 0.03911026 0.03560823]\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08840935884250535 | Validation loss: 0.08210575208067894\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 1\nTraining loss: 0.08507650593916576 | Validation loss: 0.08108918741345406\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 2\nTraining loss: 0.07737381735609637 | Validation loss: 0.0804343322912852\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 3\nTraining loss: 0.06796439374900526 | Validation loss: 0.08102053900559743\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 4\nTraining loss: 0.05701384869300657 | Validation loss: 0.06451516598463058\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 5\nTraining loss: 0.04790849404202567 | Validation loss: 0.04559866711497307\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 6\nTraining loss: 0.0425061976744069 | Validation loss: 0.05457633485396703\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 7\nTraining loss: 0.03917014981723494 | Validation loss: 0.10548477371533711\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 8\nTraining loss: 0.03815334942191839 | Validation loss: 0.07415188476443291\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 9\nTraining loss: 0.03634079255991512 | Validation loss: 0.03436201065778732\nValidation loss (ends of cycles): [0.08210575]\n------------------------------\nEpoch: 10\nTraining loss: 0.03238243547578653 | Validation loss: 0.03496560640633106\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 11\nTraining loss: 0.031512254331674844 | Validation loss: 0.03387966255346934\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 12\nTraining loss: 0.03323024997694625 | Validation loss: 0.06266245618462563\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 13\nTraining loss: 0.03258724557235837 | Validation loss: 0.07572312156359355\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 14\nTraining loss: 0.03392490858419074 | Validation loss: 0.08760666350523631\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 15\nTraining loss: 0.032957878481182784 | Validation loss: 0.06539637347062428\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 16\nTraining loss: 0.03240760095003578 | Validation loss: 0.030444981530308723\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 17\nTraining loss: 0.030572137277987268 | Validation loss: 0.028518366316954296\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 18\nTraining loss: 0.02843097411096096 | Validation loss: 0.028718551620841026\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 19\nTraining loss: 0.025377622495094936 | Validation loss: 0.03340643892685572\nValidation loss (ends of cycles): [0.08210575 0.03496561]\n------------------------------\nEpoch: 20\nTraining loss: 0.02524500247091055 | Validation loss: 0.027380989864468575\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 21\nTraining loss: 0.025558336534433894 | Validation loss: 0.026460225383440655\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 22\nTraining loss: 0.023630426679220464 | Validation loss: 0.028433510412772495\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 23\nTraining loss: 0.02497979895108276 | Validation loss: 0.032619635264078774\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 24\nTraining loss: 0.026406725351181295 | Validation loss: 0.035625407472252846\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 25\nTraining loss: 0.026628990140226152 | Validation loss: 0.12885981798171997\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 26\nTraining loss: 0.026922656533618767 | Validation loss: 0.055682502686977386\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 27\nTraining loss: 0.025312546226713393 | Validation loss: 0.04670518139998118\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 28\nTraining loss: 0.023591533665441804 | Validation loss: 0.075415700674057\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 29\nTraining loss: 0.02143574645742774 | Validation loss: 0.021575671931107838\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n------------------------------\nEpoch: 30\nTraining loss: 0.019714292811436787 | Validation loss: 0.024841646663844585\nValidation loss (ends of cycles): [0.08210575 0.03496561 0.02738099 0.02484165]\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08366356927313302 | Validation loss: 0.06628272930781047\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 1\nTraining loss: 0.08258866577556259 | Validation loss: 0.06598960359891255\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 2\nTraining loss: 0.08011062364829213 | Validation loss: 0.06656766682863235\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 3\nTraining loss: 0.07609734664622106 | Validation loss: 0.07429493218660355\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 4\nTraining loss: 0.06887183612898777 | Validation loss: 0.08531180272499721\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 5\nTraining loss: 0.062297113827968896 | Validation loss: 0.05507988358537356\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 6\nTraining loss: 0.057449167300211754 | Validation loss: 0.05169703687230746\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 7\nTraining loss: 0.05063312598749211 | Validation loss: 0.044730848322312035\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 8\nTraining loss: 0.04662537633588439 | Validation loss: 0.05200311293204626\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 9\nTraining loss: 0.042545238313706296 | Validation loss: 0.040968768298625946\nValidation loss (ends of cycles): [0.06628273]\n------------------------------\nEpoch: 10\nTraining loss: 0.040791098518591175 | Validation loss: 0.04076941559712092\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 11\nTraining loss: 0.03988300430539407 | Validation loss: 0.04062818984190623\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 12\nTraining loss: 0.041134035116747805 | Validation loss: 0.06347007056077321\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 13\nTraining loss: 0.040732931933904946 | Validation loss: 0.053751084953546524\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 14\nTraining loss: 0.0412690176775581 | Validation loss: 0.04342729101578394\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 15\nTraining loss: 0.04279994739121512 | Validation loss: 0.03779313713312149\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 16\nTraining loss: 0.04107574108791979 | Validation loss: 0.05226917316516241\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 17\nTraining loss: 0.03682027963039122 | Validation loss: 0.04407886415719986\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 18\nTraining loss: 0.03602115907951405 | Validation loss: 0.0353589312483867\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 19\nTraining loss: 0.03072228841483593 | Validation loss: 0.03361495025455952\nValidation loss (ends of cycles): [0.06628273 0.04076942]\n------------------------------\nEpoch: 20\nTraining loss: 0.03127634593922841 | Validation loss: 0.03227363092203935\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 21\nTraining loss: 0.030668180729997784 | Validation loss: 0.03339084858695666\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 22\nTraining loss: 0.03049330334914358 | Validation loss: 0.033799403036634125\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 23\nTraining loss: 0.030560468372545745 | Validation loss: 0.052750845750172935\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 24\nTraining loss: 0.032240519398137144 | Validation loss: 0.11552038788795471\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 25\nTraining loss: 0.03203717256455045 | Validation loss: 0.03324045240879059\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 26\nTraining loss: 0.03040875906222745 | Validation loss: 0.04665656387805939\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 27\nTraining loss: 0.029416492551957305 | Validation loss: 0.04413521351913611\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 28\nTraining loss: 0.029010920001095848 | Validation loss: 0.03647958238919576\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 29\nTraining loss: 0.02751604928389976 | Validation loss: 0.050870560109615326\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n------------------------------\nEpoch: 30\nTraining loss: 0.02429521475967608 | Validation loss: 0.028451986610889435\nValidation loss (ends of cycles): [0.06628273 0.04076942 0.03227363 0.02845199]\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.19473344401309364 | Validation loss: 0.15656746923923492\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 1\nTraining loss: 0.18613588496258385 | Validation loss: 0.14746378362178802\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 2\nTraining loss: 0.16706412207139165 | Validation loss: 0.12539705261588097\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 3\nTraining loss: 0.13910072571352908 | Validation loss: 0.08593828231096268\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 4\nTraining loss: 0.10771784460858295 | Validation loss: 0.06520361453294754\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 5\nTraining loss: 0.07648744355691106 | Validation loss: 0.07435402646660805\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 6\nTraining loss: 0.06046923661702558 | Validation loss: 0.08778238669037819\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 7\nTraining loss: 0.05391533515955273 | Validation loss: 0.06520635634660721\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 8\nTraining loss: 0.04810122106420366 | Validation loss: 0.06413957849144936\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 9\nTraining loss: 0.044574140816142686 | Validation loss: 0.045684026554226875\nValidation loss (ends of cycles): [0.15656747]\n------------------------------\nEpoch: 10\nTraining loss: 0.045094708470921764 | Validation loss: 0.04497688636183739\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 11\nTraining loss: 0.043865959503148734 | Validation loss: 0.04489790461957455\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 12\nTraining loss: 0.044169505273825245 | Validation loss: 0.04815280996263027\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 13\nTraining loss: 0.04281844169293579 | Validation loss: 0.08249081298708916\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 14\nTraining loss: 0.043214576024758186 | Validation loss: 0.05961386486887932\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 15\nTraining loss: 0.042521203917108084 | Validation loss: 0.0713415015488863\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 16\nTraining loss: 0.0419846284938486 | Validation loss: 0.07541047409176826\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 17\nTraining loss: 0.03957709375964968 | Validation loss: 0.05406338535249233\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 18\nTraining loss: 0.035520012049298534 | Validation loss: 0.10436020791530609\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 19\nTraining loss: 0.034939819456715336 | Validation loss: 0.03864527679979801\nValidation loss (ends of cycles): [0.15656747 0.04497689]\n------------------------------\nEpoch: 20\nTraining loss: 0.03136764094233513 | Validation loss: 0.03485617786645889\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 21\nTraining loss: 0.031533444790463695 | Validation loss: 0.032069167122244835\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 22\nTraining loss: 0.03128002505553396 | Validation loss: 0.03990967012941837\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 23\nTraining loss: 0.032222791231776536 | Validation loss: 0.06194067373871803\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 24\nTraining loss: 0.030879733495806392 | Validation loss: 0.04783654771745205\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 25\nTraining loss: 0.034811057640533695 | Validation loss: 0.19700831919908524\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 26\nTraining loss: 0.03208483088957636 | Validation loss: 0.04298360459506512\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 27\nTraining loss: 0.02921375377397788 | Validation loss: 0.04345952346920967\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 28\nTraining loss: 0.02759896424648009 | Validation loss: 0.030260787345468998\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 29\nTraining loss: 0.02629519714728782 | Validation loss: 0.04085123725235462\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n------------------------------\nEpoch: 30\nTraining loss: 0.02536522258857363 | Validation loss: 0.028398994356393814\nValidation loss (ends of cycles): [0.15656747 0.04497689 0.03485618 0.02839899]\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.09135209562049972 | Validation loss: 0.06946399062871933\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 1\nTraining loss: 0.0878267308904065 | Validation loss: 0.0675109475851059\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 2\nTraining loss: 0.07988109977708922 | Validation loss: 0.06493373587727547\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 3\nTraining loss: 0.0666891232960754 | Validation loss: 0.06385781802237034\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 4\nTraining loss: 0.05818599214156469 | Validation loss: 0.06167275831103325\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 5\nTraining loss: 0.05256716679367754 | Validation loss: 0.07637954130768776\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 6\nTraining loss: 0.049445088331898056 | Validation loss: 0.22612106055021286\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 7\nTraining loss: 0.045817383771969214 | Validation loss: 0.050643378868699074\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 8\nTraining loss: 0.040001612777511276 | Validation loss: 0.0446147657930851\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 9\nTraining loss: 0.03829288575798273 | Validation loss: 0.04114661552011967\nValidation loss (ends of cycles): [0.06946399]\n------------------------------\nEpoch: 10\nTraining loss: 0.03613738570776251 | Validation loss: 0.040317755192518234\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 11\nTraining loss: 0.034891982562839985 | Validation loss: 0.039226071909070015\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 12\nTraining loss: 0.03366432442433304 | Validation loss: 0.046774642542004585\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 13\nTraining loss: 0.03640781891428762 | Validation loss: 0.04789281450212002\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 14\nTraining loss: 0.03705685358080599 | Validation loss: 0.05857366323471069\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 15\nTraining loss: 0.035437823894123234 | Validation loss: 0.07393408939242363\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 16\nTraining loss: 0.03706711303028795 | Validation loss: 0.06966803222894669\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 17\nTraining loss: 0.03401203018923601 | Validation loss: 0.04148573614656925\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 18\nTraining loss: 0.03279454085148043 | Validation loss: 0.03339186776429415\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 19\nTraining loss: 0.029390734827352896 | Validation loss: 0.03046796005219221\nValidation loss (ends of cycles): [0.06946399 0.04031776]\n------------------------------\nEpoch: 20\nTraining loss: 0.029600716920362577 | Validation loss: 0.03065457008779049\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 21\nTraining loss: 0.029213293352060847 | Validation loss: 0.031029099598526955\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 22\nTraining loss: 0.028610273264348507 | Validation loss: 0.030708318576216698\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 23\nTraining loss: 0.028793518121043842 | Validation loss: 0.046449968591332436\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 24\nTraining loss: 0.029298070300784376 | Validation loss: 0.04168690741062164\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 25\nTraining loss: 0.030492768364234105 | Validation loss: 0.0755428783595562\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 26\nTraining loss: 0.030982901031772297 | Validation loss: 0.030249490402638912\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 27\nTraining loss: 0.028873344521141715 | Validation loss: 0.03989887796342373\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 28\nTraining loss: 0.02943373481846518 | Validation loss: 0.029455197043716908\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 29\nTraining loss: 0.025847461229811113 | Validation loss: 0.03161353338509798\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n------------------------------\nEpoch: 30\nTraining loss: 0.024220067593786452 | Validation loss: 0.027339047752320766\nValidation loss (ends of cycles): [0.06946399 0.04031776 0.03065457 0.02733905]\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08758864708636936 | Validation loss: 0.09045941010117531\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 1\nTraining loss: 0.08511891274860031 | Validation loss: 0.08673127368092537\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 2\nTraining loss: 0.0793429758203657 | Validation loss: 0.08125056326389313\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 3\nTraining loss: 0.07344777999739897 | Validation loss: 0.07784003764390945\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 4\nTraining loss: 0.06621623725483292 | Validation loss: 0.08580468967556953\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 5\nTraining loss: 0.06146987037439095 | Validation loss: 0.08310152217745781\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 6\nTraining loss: 0.05917700419300481 | Validation loss: 0.06960192322731018\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 7\nTraining loss: 0.054859228236110585 | Validation loss: 0.067630959674716\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 8\nTraining loss: 0.05065024114753071 | Validation loss: 0.055472830310463905\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 9\nTraining loss: 0.04813684611336181 | Validation loss: 0.05709882639348507\nValidation loss (ends of cycles): [0.09045941]\n------------------------------\nEpoch: 10\nTraining loss: 0.04541594505702194 | Validation loss: 0.053741781041026115\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 11\nTraining loss: 0.043925387784838676 | Validation loss: 0.055874619632959366\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 12\nTraining loss: 0.043821888142510465 | Validation loss: 0.05342106893658638\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 13\nTraining loss: 0.04354088537787136 | Validation loss: 0.05569586902856827\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 14\nTraining loss: 0.04596223790002497 | Validation loss: 0.08136944100260735\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 15\nTraining loss: 0.04429164842555398 | Validation loss: 0.056143974885344505\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 16\nTraining loss: 0.04604476484421052 | Validation loss: 0.058117739856243134\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 17\nTraining loss: 0.04353812356528483 | Validation loss: 0.06109851598739624\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 18\nTraining loss: 0.03982813224980706 | Validation loss: 0.04927295260131359\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 19\nTraining loss: 0.03627215433669718 | Validation loss: 0.047167809680104256\nValidation loss (ends of cycles): [0.09045941 0.05374178]\n------------------------------\nEpoch: 20\nTraining loss: 0.034992945233457966 | Validation loss: 0.047399308532476425\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 21\nTraining loss: 0.03621590147285085 | Validation loss: 0.04751015082001686\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 22\nTraining loss: 0.033725014152495486 | Validation loss: 0.047297827899456024\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 23\nTraining loss: 0.03643197546664037 | Validation loss: 0.045731207355856895\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 24\nTraining loss: 0.03781851704575514 | Validation loss: 0.04651406966149807\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 25\nTraining loss: 0.03732413926014775 | Validation loss: 0.04698087275028229\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 26\nTraining loss: 0.037296586522930546 | Validation loss: 0.04507102258503437\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 27\nTraining loss: 0.03556570381318268 | Validation loss: 0.042184218764305115\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 28\nTraining loss: 0.030846391461397473 | Validation loss: 0.04002702981233597\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 29\nTraining loss: 0.03090105475367684 | Validation loss: 0.038508640602231026\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n------------------------------\nEpoch: 30\nTraining loss: 0.028801383156525463 | Validation loss: 0.03822813369333744\nValidation loss (ends of cycles): [0.09045941 0.05374178 0.04739931 0.03822813]\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08331045665239033 | Validation loss: 0.06520787129799525\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 1\nTraining loss: 0.08203087198106866 | Validation loss: 0.06532517820596695\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 2\nTraining loss: 0.08000684745217625 | Validation loss: 0.06518079092105229\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 3\nTraining loss: 0.0739791032515074 | Validation loss: 0.06506842374801636\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 4\nTraining loss: 0.06385869787711847 | Validation loss: 0.05250853920976321\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 5\nTraining loss: 0.057416989615089016 | Validation loss: 0.05340991293390592\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 6\nTraining loss: 0.052579465861383234 | Validation loss: 0.029369194293394685\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 7\nTraining loss: 0.04429109718062376 | Validation loss: 0.04665235554178556\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 8\nTraining loss: 0.04031965685518164 | Validation loss: 0.06348493695259094\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 9\nTraining loss: 0.03789979886067541 | Validation loss: 0.03177877189591527\nValidation loss (ends of cycles): [0.06520787]\n------------------------------\nEpoch: 10\nTraining loss: 0.03492777098558451 | Validation loss: 0.030528849456459284\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 11\nTraining loss: 0.03243052214384079 | Validation loss: 0.025340224984878052\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 12\nTraining loss: 0.033256117744665394 | Validation loss: 0.030804906350870926\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 13\nTraining loss: 0.03333615707723718 | Validation loss: 0.035566401512672506\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 14\nTraining loss: 0.03184130630995098 | Validation loss: 0.03293635222750405\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 15\nTraining loss: 0.03302473133723987 | Validation loss: 0.028592127503846616\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 16\nTraining loss: 0.030961162704778344 | Validation loss: 0.02682249341160059\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 17\nTraining loss: 0.028243662789463997 | Validation loss: 0.027744205047686894\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 18\nTraining loss: 0.02676465332900223 | Validation loss: 0.02321198567127188\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 19\nTraining loss: 0.02272553192941766 | Validation loss: 0.02984648073712985\nValidation loss (ends of cycles): [0.06520787 0.03052885]\n------------------------------\nEpoch: 20\nTraining loss: 0.021555469567446334 | Validation loss: 0.023247383224467438\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 21\nTraining loss: 0.0220749583340397 | Validation loss: 0.021610831453775365\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 22\nTraining loss: 0.020930324485035318 | Validation loss: 0.023576893222828705\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 23\nTraining loss: 0.021814725116679545 | Validation loss: 0.025978229319055874\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 24\nTraining loss: 0.02283184808727942 | Validation loss: 0.025270794207851093\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 25\nTraining loss: 0.02307317018704979 | Validation loss: 0.08468196541070938\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 26\nTraining loss: 0.021557157663138288 | Validation loss: 0.027905408913890522\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 27\nTraining loss: 0.021273777182949215 | Validation loss: 0.02349347559114297\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 28\nTraining loss: 0.01986664131675896 | Validation loss: 0.0233243799302727\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 29\nTraining loss: 0.017681540676245566 | Validation loss: 0.022845523431897163\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n------------------------------\nEpoch: 30\nTraining loss: 0.016350375782502324 | Validation loss: 0.02178852337722977\nValidation loss (ends of cycles): [0.06520787 0.03052885 0.02324738 0.02178852]\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.14074529629004628 | Validation loss: 0.13468989357352257\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 1\nTraining loss: 0.13303319051077492 | Validation loss: 0.1283198483288288\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 2\nTraining loss: 0.11730245461589411 | Validation loss: 0.1113440953195095\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 3\nTraining loss: 0.09925832560187892 | Validation loss: 0.0883958488702774\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 4\nTraining loss: 0.08102804579232868 | Validation loss: 0.0767072718590498\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 5\nTraining loss: 0.06754080539471224 | Validation loss: 0.0630732923746109\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 6\nTraining loss: 0.0626953479490782 | Validation loss: 0.05822800286114216\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 7\nTraining loss: 0.05980836462817694 | Validation loss: 0.057572031393647194\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 8\nTraining loss: 0.05771961749384278 | Validation loss: 0.06076772231608629\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 9\nTraining loss: 0.0532009103580525 | Validation loss: 0.05623008869588375\nValidation loss (ends of cycles): [0.13468989]\n------------------------------\nEpoch: 10\nTraining loss: 0.05379903512565713 | Validation loss: 0.05519082024693489\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 11\nTraining loss: 0.052072555807076 | Validation loss: 0.05436007305979729\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 12\nTraining loss: 0.051279104265727495 | Validation loss: 0.06033742055296898\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 13\nTraining loss: 0.05221212537665116 | Validation loss: 0.05254641734063625\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 14\nTraining loss: 0.052044751024559924 | Validation loss: 0.05064303055405617\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 15\nTraining loss: 0.04771482081789719 | Validation loss: 0.18143466114997864\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 16\nTraining loss: 0.048779759654089025 | Validation loss: 0.049892572686076164\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 17\nTraining loss: 0.04602725470536634 | Validation loss: 0.0489846533164382\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 18\nTraining loss: 0.043393145756501904 | Validation loss: 0.04021776653826237\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 19\nTraining loss: 0.037118491863733845 | Validation loss: 0.043297613970935345\nValidation loss (ends of cycles): [0.13468989 0.05519082]\n------------------------------\nEpoch: 20\nTraining loss: 0.036233729535811825 | Validation loss: 0.04204665496945381\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 21\nTraining loss: 0.036370223486109785 | Validation loss: 0.04038366116583347\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 22\nTraining loss: 0.03642301369262369 | Validation loss: 0.04990543611347675\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 23\nTraining loss: 0.03510999748189198 | Validation loss: 0.035950854420661926\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 24\nTraining loss: 0.03829076278366541 | Validation loss: 0.08307567611336708\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 25\nTraining loss: 0.04044385832783423 | Validation loss: 0.047016918659210205\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 26\nTraining loss: 0.037926588403551204 | Validation loss: 0.037543052807450294\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 27\nTraining loss: 0.034744165445628916 | Validation loss: 0.05590544827282429\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 28\nTraining loss: 0.03150788292680916 | Validation loss: 0.03907494433224201\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 29\nTraining loss: 0.029333851839366713 | Validation loss: 0.030293073505163193\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n------------------------------\nEpoch: 30\nTraining loss: 0.027970748414334497 | Validation loss: 0.030819999054074287\nValidation loss (ends of cycles): [0.13468989 0.05519082 0.04204665 0.03082   ]\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.12100787852939807 | Validation loss: 0.14878312995036444\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 1\nTraining loss: 0.1156453094200084 | Validation loss: 0.14218211422363916\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 2\nTraining loss: 0.10426287549106698 | Validation loss: 0.12800817439953485\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 3\nTraining loss: 0.08943178112569608 | Validation loss: 0.10652916630109151\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 4\nTraining loss: 0.07398556525769986 | Validation loss: 0.0644477941095829\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 5\nTraining loss: 0.06363658705040028 | Validation loss: 0.05572609603404999\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 6\nTraining loss: 0.06081610623943178 | Validation loss: 0.09725653131802876\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 7\nTraining loss: 0.05732113220974019 | Validation loss: 0.06353205566604932\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 8\nTraining loss: 0.053295500772564036 | Validation loss: 0.05046262095371882\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 9\nTraining loss: 0.05417528699495291 | Validation loss: 0.047901748990019165\nValidation loss (ends of cycles): [0.14878313]\n------------------------------\nEpoch: 10\nTraining loss: 0.0489277304395249 | Validation loss: 0.046974229936798416\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 11\nTraining loss: 0.05104770844704226 | Validation loss: 0.046928669015566506\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 12\nTraining loss: 0.04871545045783645 | Validation loss: 0.04629917008181413\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 13\nTraining loss: 0.04950790577813199 | Validation loss: 0.06741906702518463\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 14\nTraining loss: 0.050524057428303515 | Validation loss: 0.0722799909611543\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 15\nTraining loss: 0.051111385226249695 | Validation loss: 0.08795048048098882\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 16\nTraining loss: 0.049861222603603414 | Validation loss: 0.04207110404968262\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 17\nTraining loss: 0.04603122537465472 | Validation loss: 0.03900467542310556\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 18\nTraining loss: 0.043004892276305905 | Validation loss: 0.0376884446789821\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 19\nTraining loss: 0.03993555070146134 | Validation loss: 0.03346223756670952\nValidation loss (ends of cycles): [0.14878313 0.04697423]\n------------------------------\nEpoch: 20\nTraining loss: 0.038372725444404704 | Validation loss: 0.033292777525881924\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 21\nTraining loss: 0.03876232497982288 | Validation loss: 0.03255048921952645\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 22\nTraining loss: 0.038259138892355715 | Validation loss: 0.03326376589636008\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 23\nTraining loss: 0.039389371773914286 | Validation loss: 0.03649425941208998\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 24\nTraining loss: 0.037030863918756186 | Validation loss: 0.03978176477054755\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 25\nTraining loss: 0.04088841525739745 | Validation loss: 0.04172546664873759\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 26\nTraining loss: 0.038719205234787966 | Validation loss: 0.03826622168223063\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 27\nTraining loss: 0.03795238160283158 | Validation loss: 0.039025234058499336\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 28\nTraining loss: 0.0355978360105502 | Validation loss: 0.035915122057000794\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 29\nTraining loss: 0.03182907368203527 | Validation loss: 0.03283580827216307\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n------------------------------\nEpoch: 30\nTraining loss: 0.03164514409084069 | Validation loss: 0.03219876562555631\nValidation loss (ends of cycles): [0.14878313 0.04697423 0.03329278 0.03219877]\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.09328515749228627 | Validation loss: 0.0794174075126648\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 1\nTraining loss: 0.08947268716598812 | Validation loss: 0.07702409103512764\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 2\nTraining loss: 0.08151930766670328 | Validation loss: 0.07422645390033722\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 3\nTraining loss: 0.07421657756755226 | Validation loss: 0.0915425568819046\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 4\nTraining loss: 0.06644049580944211 | Validation loss: 0.15212642401456833\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 5\nTraining loss: 0.06289171956871685 | Validation loss: 0.06643170863389969\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 6\nTraining loss: 0.060507005570750484 | Validation loss: 0.07660464942455292\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 7\nTraining loss: 0.05770743657883845 | Validation loss: 0.04702441208064556\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 8\nTraining loss: 0.055238479160164534 | Validation loss: 0.04257218353450298\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 9\nTraining loss: 0.04844514770727409 | Validation loss: 0.038986045867204666\nValidation loss (ends of cycles): [0.07941741]\n------------------------------\nEpoch: 10\nTraining loss: 0.04857376590371132 | Validation loss: 0.03860069438815117\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 11\nTraining loss: 0.0471659146837498 | Validation loss: 0.03836953267455101\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 12\nTraining loss: 0.048829075243127976 | Validation loss: 0.03513345122337341\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 13\nTraining loss: 0.05086563390336538 | Validation loss: 0.03178618475794792\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 14\nTraining loss: 0.04892818766989206 | Validation loss: 0.07313217967748642\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 15\nTraining loss: 0.05062789891503359 | Validation loss: 0.04181492328643799\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 16\nTraining loss: 0.04422282143250892 | Validation loss: 0.031231501139700413\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 17\nTraining loss: 0.0414473704601589 | Validation loss: 0.0330012571066618\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 18\nTraining loss: 0.041221494541356436 | Validation loss: 0.03016264084726572\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 19\nTraining loss: 0.035920314490795135 | Validation loss: 0.024509469978511333\nValidation loss (ends of cycles): [0.07941741 0.03860069]\n------------------------------\nEpoch: 20\nTraining loss: 0.03617992938349122 | Validation loss: 0.023955611512064934\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 21\nTraining loss: 0.03439663468222869 | Validation loss: 0.024173706769943237\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 22\nTraining loss: 0.03432707321879111 | Validation loss: 0.03172864858061075\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 23\nTraining loss: 0.035520500748565324 | Validation loss: 0.02993414457887411\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 24\nTraining loss: 0.03720426637875406 | Validation loss: 0.036875439807772636\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 25\nTraining loss: 0.03747940367381824 | Validation loss: 0.055036623030900955\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 26\nTraining loss: 0.035825867597994054 | Validation loss: 0.04328293725848198\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 27\nTraining loss: 0.03354562186685048 | Validation loss: 0.025072680786252022\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 28\nTraining loss: 0.03464834401874166 | Validation loss: 0.024809451773762703\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 29\nTraining loss: 0.03116374404022568 | Validation loss: 0.033944932743906975\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n------------------------------\nEpoch: 30\nTraining loss: 0.02939593855683741 | Validation loss: 0.022330881096422672\nValidation loss (ends of cycles): [0.07941741 0.03860069 0.02395561 0.02233088]\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.12622165640718058 | Validation loss: 0.13938650488853455\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 1\nTraining loss: 0.1200494174110262 | Validation loss: 0.13313303887844086\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 2\nTraining loss: 0.10607607701891347 | Validation loss: 0.12224111333489418\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 3\nTraining loss: 0.09052326138082303 | Validation loss: 0.10416682437062263\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 4\nTraining loss: 0.07436744202124446 | Validation loss: 0.07375293970108032\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 5\nTraining loss: 0.063938575747766 | Validation loss: 0.07426713779568672\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 6\nTraining loss: 0.057538029590719623 | Validation loss: 0.06209025718271732\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 7\nTraining loss: 0.05095285353691954 | Validation loss: 0.06709360145032406\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 8\nTraining loss: 0.04912557217635607 | Validation loss: 0.0595396663993597\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 9\nTraining loss: 0.04791054472719368 | Validation loss: 0.05402009002864361\nValidation loss (ends of cycles): [0.1393865]\n------------------------------\nEpoch: 10\nTraining loss: 0.04601734866829295 | Validation loss: 0.0537868607789278\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 11\nTraining loss: 0.04642667229238309 | Validation loss: 0.05364326946437359\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 12\nTraining loss: 0.04651502166923724 | Validation loss: 0.05703286826610565\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 13\nTraining loss: 0.04464602313543621 | Validation loss: 0.07510419934988022\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 14\nTraining loss: 0.04594061886401553 | Validation loss: 0.0557715930044651\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 15\nTraining loss: 0.0421954180653158 | Validation loss: 0.04985055699944496\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 16\nTraining loss: 0.04223741483139364 | Validation loss: 0.04710370860993862\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 17\nTraining loss: 0.038117557764053345 | Validation loss: 0.04934592917561531\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 18\nTraining loss: 0.03437867579295447 | Validation loss: 0.05659086816012859\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 19\nTraining loss: 0.03409294313506076 | Validation loss: 0.04514491185545921\nValidation loss (ends of cycles): [0.1393865  0.05378686]\n------------------------------\nEpoch: 20\nTraining loss: 0.03242792100890687 | Validation loss: 0.04265304282307625\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 21\nTraining loss: 0.03009496952750181 | Validation loss: 0.04248355142772198\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 22\nTraining loss: 0.03202511644677112 | Validation loss: 0.0439732950180769\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 23\nTraining loss: 0.03216276111963548 | Validation loss: 0.05873432569205761\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 24\nTraining loss: 0.032698783141217734 | Validation loss: 0.04619231075048447\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 25\nTraining loss: 0.03633682802319527 | Validation loss: 0.072358887642622\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 26\nTraining loss: 0.033581405681999105 | Validation loss: 0.05562468618154526\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 27\nTraining loss: 0.030268098472764616 | Validation loss: 0.059526894241571426\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 28\nTraining loss: 0.02835421930802496 | Validation loss: 0.035784799605607986\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 29\nTraining loss: 0.024994256367024622 | Validation loss: 0.036349328234791756\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n------------------------------\nEpoch: 30\nTraining loss: 0.023506488503986282 | Validation loss: 0.03648699168115854\nValidation loss (ends of cycles): [0.1393865  0.05378686 0.04265304 0.03648699]\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.12465888732358028 | Validation loss: 0.11083460599184036\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 1\nTraining loss: 0.11687581594053068 | Validation loss: 0.1044333999355634\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 2\nTraining loss: 0.10341314363636468 | Validation loss: 0.09221626569827397\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 3\nTraining loss: 0.08686307386348122 | Validation loss: 0.07851269468665123\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 4\nTraining loss: 0.07083727340949209 | Validation loss: 0.0551288320372502\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 5\nTraining loss: 0.06321360847275508 | Validation loss: 0.05201607135434946\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 6\nTraining loss: 0.059471154487446734 | Validation loss: 0.050572953497370086\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 7\nTraining loss: 0.05677154197014476 | Validation loss: 0.048783741891384125\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 8\nTraining loss: 0.05208255586483957 | Validation loss: 0.05494089797139168\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 9\nTraining loss: 0.05054084868415406 | Validation loss: 0.042090740675727524\nValidation loss (ends of cycles): [0.11083461]\n------------------------------\nEpoch: 10\nTraining loss: 0.046194194313628895 | Validation loss: 0.0418690579632918\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 11\nTraining loss: 0.04912359110618893 | Validation loss: 0.03736517330010732\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 12\nTraining loss: 0.050159576250926444 | Validation loss: 0.03728324609498183\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 13\nTraining loss: 0.04813679552784091 | Validation loss: 0.06099647656083107\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 14\nTraining loss: 0.05095356658689285 | Validation loss: 0.042035351817806564\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 15\nTraining loss: 0.05249298383530818 | Validation loss: 0.08353547627727191\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 16\nTraining loss: 0.04622210692124147 | Validation loss: 0.05047812437017759\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 17\nTraining loss: 0.04407538248128012 | Validation loss: 0.03657640082140764\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 18\nTraining loss: 0.04126803819580298 | Validation loss: 0.045112963765859604\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 19\nTraining loss: 0.03931908642775134 | Validation loss: 0.03320226073265076\nValidation loss (ends of cycles): [0.11083461 0.04186906]\n------------------------------\nEpoch: 20\nTraining loss: 0.040036032294952555 | Validation loss: 0.034371147553126015\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 21\nTraining loss: 0.03779972744737997 | Validation loss: 0.034147227803866066\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 22\nTraining loss: 0.03736349481991247 | Validation loss: 0.03392460756003857\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 23\nTraining loss: 0.036734214554981964 | Validation loss: 0.05457255865136782\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 24\nTraining loss: 0.03623275096699791 | Validation loss: 0.041262177750468254\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 25\nTraining loss: 0.039135233744194635 | Validation loss: 0.06989621991912524\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 26\nTraining loss: 0.041533306927273146 | Validation loss: 0.08880491803089778\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 27\nTraining loss: 0.036787426638367926 | Validation loss: 0.05547218148907026\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 28\nTraining loss: 0.037730952602271974 | Validation loss: 0.04338609303037325\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 29\nTraining loss: 0.03741005190501088 | Validation loss: 0.03517623494068781\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n------------------------------\nEpoch: 30\nTraining loss: 0.03459033566085916 | Validation loss: 0.029467060541113217\nValidation loss (ends of cycles): [0.11083461 0.04186906 0.03437115 0.02946706]\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.14787319733908302 | Validation loss: 0.12134905159473419\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 1\nTraining loss: 0.13971965289429614 | Validation loss: 0.1133270300924778\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 2\nTraining loss: 0.12202531177746623 | Validation loss: 0.09741818159818649\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 3\nTraining loss: 0.09899224105634187 | Validation loss: 0.0680413693189621\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 4\nTraining loss: 0.07547486730312046 | Validation loss: 0.05193481966853142\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 5\nTraining loss: 0.059965328754563084 | Validation loss: 0.1478968784213066\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 6\nTraining loss: 0.058371061949353466 | Validation loss: 0.060820143669843674\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 7\nTraining loss: 0.0518569956092458 | Validation loss: 0.04691869765520096\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 8\nTraining loss: 0.04747700201053368 | Validation loss: 0.05568346567451954\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 9\nTraining loss: 0.04481482358747407 | Validation loss: 0.044492047280073166\nValidation loss (ends of cycles): [0.12134905]\n------------------------------\nEpoch: 10\nTraining loss: 0.04322212032581631 | Validation loss: 0.041569143533706665\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 11\nTraining loss: 0.04246603638718003 | Validation loss: 0.04150853492319584\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 12\nTraining loss: 0.04089725978280369 | Validation loss: 0.03653017058968544\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 13\nTraining loss: 0.041098617801540775 | Validation loss: 0.04898947477340698\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 14\nTraining loss: 0.04085124303635798 | Validation loss: 0.04935206472873688\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 15\nTraining loss: 0.041323604552369365 | Validation loss: 0.04239597171545029\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 16\nTraining loss: 0.039808995531577816 | Validation loss: 0.0840645469725132\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 17\nTraining loss: 0.03703288322216586 | Validation loss: 0.03245018795132637\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 18\nTraining loss: 0.03433209422387575 | Validation loss: 0.03348969016224146\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 19\nTraining loss: 0.03210164145811608 | Validation loss: 0.030444078147411346\nValidation loss (ends of cycles): [0.12134905 0.04156914]\n------------------------------\nEpoch: 20\nTraining loss: 0.03053837034263109 | Validation loss: 0.03115426003932953\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 21\nTraining loss: 0.031159722393280583 | Validation loss: 0.03271864727139473\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 22\nTraining loss: 0.03416347729140207 | Validation loss: 0.045318085700273514\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 23\nTraining loss: 0.03107438314902155 | Validation loss: 0.056804386898875237\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 24\nTraining loss: 0.033158473474414724 | Validation loss: 0.05310705862939358\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 25\nTraining loss: 0.03283156532990305 | Validation loss: 0.05297096632421017\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 26\nTraining loss: 0.033077974362592945 | Validation loss: 0.03136811312288046\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 27\nTraining loss: 0.03158471576477352 | Validation loss: 0.058228276669979095\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 28\nTraining loss: 0.02844038595886607 | Validation loss: 0.046033360064029694\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 29\nTraining loss: 0.026794205566770153 | Validation loss: 0.02960763592272997\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n------------------------------\nEpoch: 30\nTraining loss: 0.025866988360097532 | Validation loss: 0.030549601651728153\nValidation loss (ends of cycles): [0.12134905 0.04156914 0.03115426 0.0305496 ]\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.2685273501433824 | Validation loss: 0.2683800756931305\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 1\nTraining loss: 0.25670922037802246 | Validation loss: 0.2526206970214844\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 2\nTraining loss: 0.22681226542121485 | Validation loss: 0.21906188130378723\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 3\nTraining loss: 0.18514166144948258 | Validation loss: 0.16225957870483398\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 4\nTraining loss: 0.13773585306970695 | Validation loss: 0.0982358418405056\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 5\nTraining loss: 0.09757452673817936 | Validation loss: 0.16215666383504868\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 6\nTraining loss: 0.0740412101149559 | Validation loss: 0.055965250357985497\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 7\nTraining loss: 0.06161656583610334 | Validation loss: 0.062003035098314285\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 8\nTraining loss: 0.056791181431004874 | Validation loss: 0.05057838559150696\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 9\nTraining loss: 0.05035581047597684 | Validation loss: 0.04332583770155907\nValidation loss (ends of cycles): [0.26838008]\n------------------------------\nEpoch: 10\nTraining loss: 0.047245774467132594 | Validation loss: 0.04338065907359123\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 11\nTraining loss: 0.048102253459786116 | Validation loss: 0.04022406227886677\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 12\nTraining loss: 0.04582173936069012 | Validation loss: 0.03690587542951107\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 13\nTraining loss: 0.041917656352253335 | Validation loss: 0.04160160571336746\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 14\nTraining loss: 0.04336070749712618 | Validation loss: 0.0791056714951992\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 15\nTraining loss: 0.04261575992170133 | Validation loss: 0.053798090666532516\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 16\nTraining loss: 0.04363253075433405 | Validation loss: 0.040439238771796227\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 17\nTraining loss: 0.04248141987543357 | Validation loss: 0.03731362521648407\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 18\nTraining loss: 0.038024303209232654 | Validation loss: 0.041773609817028046\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 19\nTraining loss: 0.03374856283986255 | Validation loss: 0.03551979921758175\nValidation loss (ends of cycles): [0.26838008 0.04338066]\n------------------------------\nEpoch: 20\nTraining loss: 0.033305737042897625 | Validation loss: 0.03380383178591728\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 21\nTraining loss: 0.03389084388158823 | Validation loss: 0.03323566913604736\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 22\nTraining loss: 0.0329377921788316 | Validation loss: 0.03338051959872246\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 23\nTraining loss: 0.03327037482277343 | Validation loss: 0.032685402780771255\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 24\nTraining loss: 0.03479252481146863 | Validation loss: 0.0644093994051218\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 25\nTraining loss: 0.03432726771815827 | Validation loss: 0.049457062035799026\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 26\nTraining loss: 0.03283493897240413 | Validation loss: 0.03840087540447712\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 27\nTraining loss: 0.029340636769407673 | Validation loss: 0.036333074793219566\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 28\nTraining loss: 0.028881219558809932 | Validation loss: 0.03344106115400791\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 29\nTraining loss: 0.026324676251725146 | Validation loss: 0.029291590675711632\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n------------------------------\nEpoch: 30\nTraining loss: 0.02539735272722809 | Validation loss: 0.029340913519263268\nValidation loss (ends of cycles): [0.26838008 0.04338066 0.03380383 0.02934091]\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.0841492728183144 | Validation loss: 0.06075831688940525\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 1\nTraining loss: 0.08169916311376973 | Validation loss: 0.06054982356727123\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 2\nTraining loss: 0.07849763216156709 | Validation loss: 0.06086615286767483\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 3\nTraining loss: 0.07423726802593783 | Validation loss: 0.06112533435225487\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 4\nTraining loss: 0.06984295362704679 | Validation loss: 0.04825133830308914\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 5\nTraining loss: 0.06283724523688618 | Validation loss: 0.04184510372579098\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 6\nTraining loss: 0.058544049725720755 | Validation loss: 0.041651615872979164\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 7\nTraining loss: 0.05212017580082542 | Validation loss: 0.04360814392566681\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 8\nTraining loss: 0.052451391067159805 | Validation loss: 0.04157676547765732\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 9\nTraining loss: 0.04793435246928742 | Validation loss: 0.03575167618691921\nValidation loss (ends of cycles): [0.06075832]\n------------------------------\nEpoch: 10\nTraining loss: 0.04477887306558458 | Validation loss: 0.035469865426421165\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 11\nTraining loss: 0.045943650466046836 | Validation loss: 0.0336600337177515\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 12\nTraining loss: 0.04350362703400223 | Validation loss: 0.03219062741845846\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 13\nTraining loss: 0.04283701863728071 | Validation loss: 0.03756018541753292\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 14\nTraining loss: 0.04464582922427278 | Validation loss: 0.0390226636081934\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 15\nTraining loss: 0.046644166406047974 | Validation loss: 0.03757801093161106\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 16\nTraining loss: 0.04258477227076104 | Validation loss: 0.03985445946455002\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 17\nTraining loss: 0.04088323328055834 | Validation loss: 0.026787959039211273\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 18\nTraining loss: 0.03871416957362702 | Validation loss: 0.025218220427632332\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 19\nTraining loss: 0.03509655153672946 | Validation loss: 0.02421511523425579\nValidation loss (ends of cycles): [0.06075832 0.03546987]\n------------------------------\nEpoch: 20\nTraining loss: 0.035071449550358874 | Validation loss: 0.02311981562525034\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 21\nTraining loss: 0.034543048394353765 | Validation loss: 0.022894551046192646\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 22\nTraining loss: 0.03377740447850604 | Validation loss: 0.024682712741196156\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 23\nTraining loss: 0.03353702306355301 | Validation loss: 0.029586568474769592\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 24\nTraining loss: 0.03612418242387081 | Validation loss: 0.027507783845067024\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 25\nTraining loss: 0.03587194492942408 | Validation loss: 0.027790222316980362\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 26\nTraining loss: 0.03557291837703241 | Validation loss: 0.03710603527724743\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 27\nTraining loss: 0.033871732284560016 | Validation loss: 0.03677363134920597\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 28\nTraining loss: 0.03167560118201532 | Validation loss: 0.032423168420791626\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 29\nTraining loss: 0.02956923759101253 | Validation loss: 0.019483156502246857\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n------------------------------\nEpoch: 30\nTraining loss: 0.02946120588795135 | Validation loss: 0.018310876563191414\nValidation loss (ends of cycles): [0.06075832 0.03546987 0.02311982 0.01831088]\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08156146030676992 | Validation loss: 0.0705137016872565\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 1\nTraining loss: 0.07990784237259313 | Validation loss: 0.06988636900981267\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 2\nTraining loss: 0.07819349220708798 | Validation loss: 0.06863330366710822\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 3\nTraining loss: 0.07480346470286972 | Validation loss: 0.0666625127196312\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 4\nTraining loss: 0.06917970745187056 | Validation loss: 0.058160472040375076\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 5\nTraining loss: 0.06264691466563627 | Validation loss: 0.08878007034460704\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 6\nTraining loss: 0.06039848516842252 | Validation loss: 0.05209088449676832\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 7\nTraining loss: 0.05595046673950396 | Validation loss: 0.04798128828406334\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 8\nTraining loss: 0.05155527258389875 | Validation loss: 0.041437882309158645\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 9\nTraining loss: 0.04763707479363993 | Validation loss: 0.043172294894854225\nValidation loss (ends of cycles): [0.0705137]\n------------------------------\nEpoch: 10\nTraining loss: 0.04461290363810564 | Validation loss: 0.04293485110004743\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 11\nTraining loss: 0.04245269229929698 | Validation loss: 0.03906371258199215\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 12\nTraining loss: 0.043166923287667726 | Validation loss: 0.04080717754550278\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 13\nTraining loss: 0.042604215247066396 | Validation loss: 0.03541523963212967\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 14\nTraining loss: 0.04206815513929254 | Validation loss: 0.03434837299088637\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 15\nTraining loss: 0.040814362760437164 | Validation loss: 0.034936813535750844\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 16\nTraining loss: 0.04256990846050413 | Validation loss: 0.05865098908543587\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 17\nTraining loss: 0.038062922558502146 | Validation loss: 0.032220245649417244\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 18\nTraining loss: 0.03491674932210069 | Validation loss: 0.03063141368329525\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 19\nTraining loss: 0.032406675727351716 | Validation loss: 0.031299490481615067\nValidation loss (ends of cycles): [0.0705137  0.04293485]\n------------------------------\nEpoch: 20\nTraining loss: 0.030975149993441607 | Validation loss: 0.03276701706151167\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 21\nTraining loss: 0.03167034911089822 | Validation loss: 0.031768561651309334\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 22\nTraining loss: 0.02994556332889356 | Validation loss: 0.030046330144008\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 23\nTraining loss: 0.03042360659884779 | Validation loss: 0.02618786444266637\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 24\nTraining loss: 0.033261800380913836 | Validation loss: 0.03181804623454809\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 25\nTraining loss: 0.03491548017451638 | Validation loss: 0.03237322314331929\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 26\nTraining loss: 0.03453618023348482 | Validation loss: 0.030259561104079086\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 27\nTraining loss: 0.02996809525709403 | Validation loss: 0.02521776221692562\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 28\nTraining loss: 0.029768657135336024 | Validation loss: 0.027145131180683773\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 29\nTraining loss: 0.02678287053774846 | Validation loss: 0.02985484277208646\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n------------------------------\nEpoch: 30\nTraining loss: 0.02542775310575962 | Validation loss: 0.029113321254650753\nValidation loss (ends of cycles): [0.0705137  0.04293485 0.03276702 0.02911332]\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.18578452575537893 | Validation loss: 0.17607577641805014\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 1\nTraining loss: 0.17636994024117789 | Validation loss: 0.16514772176742554\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 2\nTraining loss: 0.15567576388518015 | Validation loss: 0.14257992307345072\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 3\nTraining loss: 0.1261256126066049 | Validation loss: 0.11006870617469151\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 4\nTraining loss: 0.09516125255160862 | Validation loss: 0.07898629705111186\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 5\nTraining loss: 0.07224813931518131 | Validation loss: 0.07449610034624736\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 6\nTraining loss: 0.06180248782038689 | Validation loss: 0.059722560147444405\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 7\nTraining loss: 0.05495261256065634 | Validation loss: 0.05743814756472906\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 8\nTraining loss: 0.0541409340997537 | Validation loss: 0.05083008110523224\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 9\nTraining loss: 0.0495873944212993 | Validation loss: 0.05104871218403181\nValidation loss (ends of cycles): [0.17607578]\n------------------------------\nEpoch: 10\nTraining loss: 0.04791343791617288 | Validation loss: 0.049677314857641854\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 11\nTraining loss: 0.048474044952955514 | Validation loss: 0.04772906253735224\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 12\nTraining loss: 0.04561085191865762 | Validation loss: 0.04465216274062792\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 13\nTraining loss: 0.04276795002321402 | Validation loss: 0.044079518566528954\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 14\nTraining loss: 0.046003543875283666 | Validation loss: 0.05234615504741669\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 15\nTraining loss: 0.04754653500599994 | Validation loss: 0.057119290033976235\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 16\nTraining loss: 0.04541455095426904 | Validation loss: 0.051875809828440346\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 17\nTraining loss: 0.04212984825587935 | Validation loss: 0.061386716862519584\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 18\nTraining loss: 0.04025129984236426 | Validation loss: 0.0377594760308663\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 19\nTraining loss: 0.037910942195190325 | Validation loss: 0.03676092314223448\nValidation loss (ends of cycles): [0.17607578 0.04967731]\n------------------------------\nEpoch: 20\nTraining loss: 0.035172241946889296 | Validation loss: 0.03536786511540413\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 21\nTraining loss: 0.03531419661723905 | Validation loss: 0.03570879126588503\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 22\nTraining loss: 0.0355098739059435 | Validation loss: 0.033749821285406746\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 23\nTraining loss: 0.03708219093581041 | Validation loss: 0.034131928657492004\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 24\nTraining loss: 0.036728250690632396 | Validation loss: 0.08373745282491048\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 25\nTraining loss: 0.037703154815567866 | Validation loss: 0.16881321867307028\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 26\nTraining loss: 0.04052116773608658 | Validation loss: 0.035450027945140995\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 27\nTraining loss: 0.03702345759504371 | Validation loss: 0.04713146264354388\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 28\nTraining loss: 0.033382929033703275 | Validation loss: 0.033809199929237366\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 29\nTraining loss: 0.030483958828780387 | Validation loss: 0.03201883099973202\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n------------------------------\nEpoch: 30\nTraining loss: 0.028836593238843813 | Validation loss: 0.029221948857108753\nValidation loss (ends of cycles): [0.17607578 0.04967731 0.03536787 0.02922195]\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.08226940329921872 | Validation loss: 0.08771190047264099\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 1\nTraining loss: 0.08080815602290004 | Validation loss: 0.08779796585440636\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 2\nTraining loss: 0.07893975392768257 | Validation loss: 0.08810674771666527\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 3\nTraining loss: 0.07525817559737909 | Validation loss: 0.08791105076670647\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 4\nTraining loss: 0.06839138034142946 | Validation loss: 0.0787418819963932\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 5\nTraining loss: 0.06047361873482403 | Validation loss: 0.057681020349264145\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 6\nTraining loss: 0.05439754487260392 | Validation loss: 0.10234533622860909\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 7\nTraining loss: 0.048915984697247804 | Validation loss: 0.09288699552416801\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 8\nTraining loss: 0.05011697858572006 | Validation loss: 0.07140225917100906\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 9\nTraining loss: 0.04586619089700674 | Validation loss: 0.054284341633319855\nValidation loss (ends of cycles): [0.0877119]\n------------------------------\nEpoch: 10\nTraining loss: 0.04105820320546627 | Validation loss: 0.05086086876690388\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 11\nTraining loss: 0.040411756717060744 | Validation loss: 0.05188886821269989\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 12\nTraining loss: 0.04021171529434229 | Validation loss: 0.052854619920253754\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 13\nTraining loss: 0.039001153291840306 | Validation loss: 0.053517796099185944\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 14\nTraining loss: 0.03992528409550065 | Validation loss: 0.11048462241888046\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 15\nTraining loss: 0.0428880665843424 | Validation loss: 0.048895107582211494\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 16\nTraining loss: 0.03952205651684811 | Validation loss: 0.09723616763949394\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 17\nTraining loss: 0.03598170609850632 | Validation loss: 0.054928792640566826\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 18\nTraining loss: 0.034703530959392846 | Validation loss: 0.04207434877753258\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 19\nTraining loss: 0.03246637443570714 | Validation loss: 0.04125319607555866\nValidation loss (ends of cycles): [0.0877119  0.05086087]\n------------------------------\nEpoch: 20\nTraining loss: 0.03022692215285803 | Validation loss: 0.03823002055287361\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 21\nTraining loss: 0.031208522029613193 | Validation loss: 0.04041556641459465\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 22\nTraining loss: 0.029294442140350218 | Validation loss: 0.0428590402007103\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 23\nTraining loss: 0.0297195372220717 | Validation loss: 0.04256322421133518\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 24\nTraining loss: 0.03125792741775513 | Validation loss: 0.10660433769226074\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 25\nTraining loss: 0.03019392794292224 | Validation loss: 0.06545785069465637\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 26\nTraining loss: 0.033481411537841746 | Validation loss: 0.03471088223159313\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 27\nTraining loss: 0.027845038640263834 | Validation loss: 0.07449803873896599\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 28\nTraining loss: 0.027143595397080247 | Validation loss: 0.03891387768089771\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 29\nTraining loss: 0.025757619964056892 | Validation loss: 0.041501617059111595\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n------------------------------\nEpoch: 30\nTraining loss: 0.02374906617363817 | Validation loss: 0.04013838246464729\nValidation loss (ends of cycles): [0.0877119  0.05086087 0.03823002 0.04013838]\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.16255763173103333 | Validation loss: 0.14109364648660025\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 1\nTraining loss: 0.15516356999675432 | Validation loss: 0.13523547103007635\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 2\nTraining loss: 0.14172234551774132 | Validation loss: 0.12201743572950363\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 3\nTraining loss: 0.12399974134233263 | Validation loss: 0.09632302448153496\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 4\nTraining loss: 0.10233049053284857 | Validation loss: 0.06885181864102681\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 5\nTraining loss: 0.0784348054892487 | Validation loss: 0.05762290582060814\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 6\nTraining loss: 0.06391300840510263 | Validation loss: 0.0725318193435669\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 7\nTraining loss: 0.05626908710433377 | Validation loss: 0.04346885159611702\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 8\nTraining loss: 0.051691514543361135 | Validation loss: 0.04434716080625852\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 9\nTraining loss: 0.04746196946750084 | Validation loss: 0.04489594325423241\nValidation loss (ends of cycles): [0.14109365]\n------------------------------\nEpoch: 10\nTraining loss: 0.043907886577977076 | Validation loss: 0.04307339588801066\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.04457749778197871 | Validation loss: 0.04406307637691498\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.044939831313159734 | Validation loss: 0.08915746957063675\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.044208405539393425 | Validation loss: 0.036763026068607964\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.04433420538488361 | Validation loss: 0.04416805567840735\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.04537590737971994 | Validation loss: 0.4176284372806549\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.04425143709199296 | Validation loss: 0.18371537327766418\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.03873702914764484 | Validation loss: 0.0807472715775172\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.03545353727208243 | Validation loss: 0.042597355941931404\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.032974150549206466 | Validation loss: 0.033211088428894676\nValidation loss (ends of cycles): [0.14109365 0.0430734 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.030094111027816933 | Validation loss: 0.033144605035583176\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 21\nTraining loss: 0.03217791558967696 | Validation loss: 0.03846348077058792\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 22\nTraining loss: 0.031263780780136585 | Validation loss: 0.030576524635155995\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 23\nTraining loss: 0.030434809832109347 | Validation loss: 0.04343028490742048\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 24\nTraining loss: 0.032326566986739635 | Validation loss: 0.13629954804976782\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 25\nTraining loss: 0.03342853072616789 | Validation loss: 0.06467030942440033\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 26\nTraining loss: 0.03306691503773133 | Validation loss: 0.041692071904738746\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 27\nTraining loss: 0.030296926179693803 | Validation loss: 0.049537912011146545\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 28\nTraining loss: 0.026243075573196013 | Validation loss: 0.031099140644073486\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 29\nTraining loss: 0.024759062979784276 | Validation loss: 0.026985854531327885\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n------------------------------\nEpoch: 30\nTraining loss: 0.02487910890744792 | Validation loss: 0.029790397733449936\nValidation loss (ends of cycles): [0.14109365 0.0430734  0.03314461 0.0297904 ]\n\n\n\n# Replace following Paths with yours\nsrc_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/models')\norder = 10\nseeds = range(20)\nlearners = Learners(Model, tax_lookup, seeds=seeds, device=device)\nperfs_local_vertisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                  order = order,\n                                                  src_dir_model=src_dir_model)\n\nperfs_local_vertisols.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.777816\n      2.448334\n      0.669128\n      0.800569\n      0.328906\n      0.110845\n      0.213575\n      30.958227\n      -0.001265\n      -0.003632\n    \n    \n      std\n      0.184440\n      0.352951\n      0.069418\n      0.041342\n      0.052977\n      0.037720\n      0.023262\n      4.237755\n      0.024421\n      0.065963\n    \n    \n      min\n      1.473109\n      1.825536\n      0.532598\n      0.716435\n      0.252563\n      0.063788\n      0.179316\n      24.304497\n      -0.041362\n      -0.115813\n    \n    \n      25%\n      1.615667\n      2.245532\n      0.611233\n      0.773600\n      0.292217\n      0.085391\n      0.197175\n      28.024661\n      -0.016770\n      -0.041078\n    \n    \n      50%\n      1.788574\n      2.443511\n      0.683310\n      0.807425\n      0.318792\n      0.101629\n      0.209331\n      29.388569\n      0.000355\n      0.000905\n    \n    \n      75%\n      1.925041\n      2.621849\n      0.726185\n      0.828116\n      0.349539\n      0.122219\n      0.231883\n      34.220246\n      0.018196\n      0.041607\n    \n    \n      max\n      2.113319\n      3.132748\n      0.772750\n      0.861943\n      0.458266\n      0.210007\n      0.255553\n      40.589359\n      0.043430\n      0.129690\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nChecking losses\n\nfrom mirzai.training.core import load_dumps\n\n# Replace following Paths with yours\ndest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/losses')\nlosses = load_dumps(dest_dir_loss)\n\n\npd.DataFrame(losses[4]['valid']).plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCompile metrics for “local vs. global” Fig. 6\n\ndef format_metrics(*dfs):\n    perfs = {'r2': {'mean': [], 'std': []},\n             'mape': {'mean': [], 'std': []}}\n    for df in dfs:\n        for metric in ['r2', 'mape']:\n            mean, std = df.describe().loc[['mean', 'std'], metric].items()\n            perfs[metric]['mean'].append(mean[1])\n            perfs[metric]['std'].append(std[1])\n    return perfs\n\n\nperfs = {}\nperfs['global'] = format_metrics(perfs_global_mollisols, perfs_global_gelisols, perfs_global_vertisols)\nperfs['local'] = format_metrics(perfs_local_mollisols, perfs_local_gelisols, perfs_local_vertisols)\n\n\nperfs\n\n{'global': {'r2': {'mean': [0.7676331517935623,\n    0.7426116984444495,\n    0.7447822285232857],\n   'std': [0.022648288190552323, 0.0767026908137805, 0.06303329177546012]},\n  'mape': {'mean': [27.362060844898224, 47.88334220647812, 26.8556547164917],\n   'std': [1.1716483447190629, 10.194857908308927, 3.2722567422732105]}},\n 'local': {'r2': {'mean': [0.7727136496500038,\n    0.7461132281184896,\n    0.7569868331639625],\n   'std': [0.01908464680719597, 0.08643306096790201, 0.05496486944699407]},\n  'mape': {'mean': [26.96375846862793, 43.78511905670166, 25.207100808620453],\n   'std': [1.352271118990351, 10.660068154956656, 2.427168862994315]}}}\n\n\n\ndest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn')\nwith open(dest_dir/'global_vs_local.pickle', 'wb') as f:\n    pickle.dump(perfs, f)"
  },
  {
    "objectID": "paper/plsr_train_eval.html#load-and-transform",
    "href": "paper/plsr_train_eval.html#load-and-transform",
    "title": "3.2. Train & evaluate (PLSR)",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = 'data'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (40132, 1764)\ny shape: (40132,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[43.  2.]\n [ 0.  0.]\n [ 0.  1.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}"
  },
  {
    "objectID": "paper/plsr_train_eval.html#experiment",
    "href": "paper/plsr_train_eval.html#experiment",
    "title": "3.2. Train & evaluate (PLSR)",
    "section": "Experiment",
    "text": "Experiment\n\nSetup\n\nsplit_ratio = 0.1\nseeds = range(20)\ndest_dir = Path('files/dumps/plsr/train_eval')\n\n\n\nTrain on all Soil Taxonomic Orders\n\ndest_dir_model = Path('files/dumps/plsr/train_eval/all/models')\nseeds = range(20)\nlearners = Learners(tax_lookup, seeds=seeds)\nlearners.train((X, y, depth_order[:, -1]), \n               n_cpts_range=range(40, 70, 2),\n               delta=2e-3,\n               dest_dir_model=dest_dir_model)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n# of components chosen: 50\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n# of components chosen: 52\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n# of components chosen: 54\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n# of components chosen: 56\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n# of components chosen: 52\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n# of components chosen: 54\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n# of components chosen: 58\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n# of components chosen: 62\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n# of components chosen: 48\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n# of components chosen: 52\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n# of components chosen: 58\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n# of components chosen: 48\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n# of components chosen: 48\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n# of components chosen: 48\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n# of components chosen: 44\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n# of components chosen: 68\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n# of components chosen: 44\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n# of components chosen: 50\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n# of components chosen: 64\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n# of components chosen: 58\n\n\n\nEvaluate on all\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/all/models')\nseeds = range(20)\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_global_all, y_hats_all, y_trues_all = learners.evaluate((X, y, depth_order[:, -1]),\n                                                              src_dir_model=src_dir_model)\n\n\n# Save spectific seed y_hat, y_true to plot \"Observed vs. predicted\" scatterplots\ndest_dir_predicted = Path('files/dumps/')\nseed = 1\nwith open(dest_dir_predicted/f'predicted-true-plsr-seed-{seed}.pickle', 'wb') as f: \n    pickle.dump((y_hats_all[seed].to_numpy(), y_trues_all[seed].to_numpy()), f)\n\n\nperfs_global_all.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.658763\n      2.272073\n      0.636279\n      0.780492\n      1.057159\n      1.306802\n      0.335821\n      135.026036\n      -0.000259\n      -0.000547\n    \n    \n      std\n      0.022627\n      0.033917\n      0.009834\n      0.005444\n      0.446290\n      1.222881\n      0.016914\n      3.283663\n      0.003848\n      0.007405\n    \n    \n      min\n      1.623276\n      2.204548\n      0.620402\n      0.771287\n      0.671250\n      0.450576\n      0.308898\n      130.007174\n      -0.006300\n      -0.012485\n    \n    \n      25%\n      1.640177\n      2.256584\n      0.628185\n      0.777881\n      0.756833\n      0.572822\n      0.325999\n      132.585609\n      -0.003009\n      -0.005756\n    \n    \n      50%\n      1.660981\n      2.267233\n      0.637440\n      0.780165\n      0.861632\n      0.742537\n      0.331193\n      134.434034\n      -0.001383\n      -0.002745\n    \n    \n      75%\n      1.666828\n      2.296689\n      0.639975\n      0.784414\n      1.093822\n      1.216215\n      0.344576\n      136.850054\n      0.002217\n      0.004202\n    \n    \n      max\n      1.707217\n      2.328779\n      0.656813\n      0.793939\n      2.242110\n      5.027059\n      0.372859\n      140.504871\n      0.007079\n      0.013396\n    \n  \n\n\n\n\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/all/models')\n\nseeds = range(20)\n\nfor k, v in {k: v for k, v in tax_lookup.items() if k != 'oxisols'}.items():\n    print(80*'-')\n    print(f'Test metrics on {k}')\n    print(80*'-')\n    learners = Learners(tax_lookup, seeds=seeds)\n    perfs_global, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                           order=v,\n                                           src_dir_model=src_dir_model)\n    print(perfs_global.describe())\n\n--------------------------------------------------------------------------------\nTest metrics on alfisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.488581   2.023110   0.544065   0.727878   0.411130   0.198526   \nstd     0.077245   0.111888   0.048141   0.028838   0.176213   0.187989   \nmin     1.335787   1.825327   0.438167   0.673686   0.207121   0.042899   \n25%     1.429706   1.971203   0.509535   0.713507   0.282230   0.079724   \n50%     1.493973   2.024866   0.550797   0.731100   0.390250   0.152373   \n75%     1.533459   2.092083   0.573742   0.749183   0.462969   0.214501   \nmax     1.625880   2.267384   0.620742   0.769198   0.845685   0.715184   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.162636  82.627329  -0.001926  -0.004745  \nstd     0.020796   4.371945   0.008761   0.023061  \nmin     0.128730  73.976273  -0.018855  -0.047651  \n25%     0.146705  79.772143  -0.009633  -0.025287  \n50%     0.159680  83.069107  -0.000834  -0.002193  \n75%     0.179517  85.662533   0.003653   0.009967  \nmax     0.195241  93.340300   0.011908   0.032238  \n--------------------------------------------------------------------------------\nTest metrics on mollisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.582966   2.090417   0.599952   0.742203   0.769248   0.629664   \nstd     0.034982   0.078073   0.017652   0.012356   0.199794   0.322337   \nmin     1.525933   1.972214   0.570089   0.722419   0.526864   0.277585   \n25%     1.558545   2.042070   0.587883   0.734876   0.600345   0.360454   \n50%     1.579016   2.077462   0.598502   0.738004   0.735055   0.541195   \n75%     1.610838   2.141203   0.614204   0.752421   0.917944   0.842758   \nmax     1.642984   2.250635   0.629145   0.764131   1.123730   1.262768   \n\n             mae       mape       bias        stb  \ncount  20.000000  20.000000  20.000000  20.000000  \nmean    0.307466  94.298259   0.001662   0.003734  \nstd     0.018656   3.052491   0.008008   0.018876  \nmin     0.279430  87.394573  -0.013899  -0.033922  \n25%     0.289349  92.537801  -0.004175  -0.009866  \n50%     0.308846  94.409650   0.001370   0.003153  \n75%     0.320209  96.313669   0.008152   0.018857  \nmax     0.340558  98.688058   0.014374   0.032833  \n--------------------------------------------------------------------------------\nTest metrics on inceptisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.484931   2.062821   0.543190   0.715217   0.560004   0.327634   \nstd     0.052383   0.113698   0.032576   0.021247   0.121523   0.141674   \nmin     1.393604   1.834631   0.483181   0.680782   0.383973   0.147435   \n25%     1.443211   1.980935   0.518045   0.696649   0.462424   0.214457   \n50%     1.496851   2.072912   0.552023   0.716683   0.563604   0.317676   \n75%     1.515227   2.150964   0.562994   0.733597   0.621687   0.386500   \nmax     1.588288   2.274688   0.602151   0.752080   0.802342   0.643753   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.258099  117.611669  -0.025303  -0.051641  \nstd     0.030971    6.840199   0.011152   0.023835  \nmin     0.206618  105.015942  -0.046778  -0.093141  \n25%     0.236181  114.370854  -0.033645  -0.068934  \n50%     0.253719  116.712206  -0.022784  -0.045203  \n75%     0.276829  119.444905  -0.017516  -0.032937  \nmax     0.332711  134.933979  -0.007504  -0.014204  \n--------------------------------------------------------------------------------\nTest metrics on entisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.495344   2.102047   0.535041   0.733857   0.442698   0.204343   \nstd     0.160012   0.348114   0.099822   0.051184   0.093817   0.089328   \nmin     1.229760   1.616803   0.334548   0.616401   0.310090   0.096156   \n25%     1.435865   1.802874   0.511824   0.723841   0.396742   0.157491   \n50%     1.505428   2.070569   0.555707   0.748509   0.417861   0.174608   \n75%     1.587539   2.257373   0.600692   0.761758   0.506173   0.256319   \nmax     1.916463   2.839117   0.725927   0.830845   0.663214   0.439853   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.227277  124.306496  -0.013363  -0.027085  \nstd     0.033105   15.203068   0.022993   0.048705  \nmin     0.172270  102.336085  -0.045416  -0.111028  \n25%     0.209802  113.533998  -0.030312  -0.054147  \n50%     0.226905  119.691934  -0.020139  -0.041128  \n75%     0.247482  131.740359  -0.003590  -0.006261  \nmax     0.287580  161.354752   0.034848   0.076445  \n--------------------------------------------------------------------------------\nTest metrics on spodosols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.884385   2.713797   0.699472   0.824446   0.482583   0.249341   \nstd     0.238908   0.639696   0.079492   0.043133   0.131607   0.130359   \nmin     1.390288   1.803467   0.474431   0.721559   0.276331   0.076359   \n25%     1.712985   2.360031   0.652982   0.792727   0.385248   0.148728   \n50%     1.870642   2.527430   0.709356   0.835642   0.477664   0.228171   \n75%     2.015491   3.008323   0.749113   0.844006   0.557888   0.312005   \nmax     2.398842   4.506760   0.823003   0.895621   0.706393   0.498991   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.262912  157.167552   0.014543   0.020080  \nstd     0.058003   19.733897   0.028305   0.043673  \nmin     0.171519  126.395882  -0.019549  -0.047892  \n25%     0.219475  142.879484  -0.001157  -0.002089  \n50%     0.263831  156.169654   0.007113   0.011952  \n75%     0.296229  171.764491   0.019708   0.037011  \nmax     0.359890  188.821685   0.101276   0.143619  \n--------------------------------------------------------------------------------\nTest metrics on undefined\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.699452   2.327902   0.653167   0.788484   1.152385   1.582740   \nstd     0.032766   0.070709   0.013227   0.007203   0.517839   1.675655   \nmin     1.648992   2.217120   0.632006   0.777364   0.766317   0.587241   \n25%     1.677440   2.275579   0.644376   0.781214   0.880365   0.775066   \n50%     1.692550   2.328214   0.650700   0.788912   0.935093   0.874408   \n75%     1.724165   2.352930   0.663386   0.795110   1.060103   1.132949   \nmax     1.757505   2.475099   0.676041   0.798610   2.462503   6.063923   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.424512  152.988067   0.004325   0.007633  \nstd     0.029141    7.248857   0.006910   0.012443  \nmin     0.362205  141.274022  -0.006812  -0.012853  \n25%     0.407466  148.161140  -0.001129  -0.002094  \n50%     0.424653  151.755239   0.003296   0.005683  \n75%     0.443348  157.515627   0.009594   0.017346  \nmax     0.475289  168.416540   0.016393   0.029619  \n--------------------------------------------------------------------------------\nTest metrics on ultisols\n--------------------------------------------------------------------------------\n\n\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.201589   1.665164   0.296985   0.581148   0.326529   0.109017   \nstd     0.067696   0.152486   0.081464   0.056106   0.050224   0.034916   \nmin     1.079312   1.352102   0.136519   0.489327   0.252949   0.063983   \n25%     1.151943   1.560671   0.242226   0.546748   0.295477   0.087307   \n50%     1.207474   1.676421   0.310236   0.584230   0.321999   0.103689   \n75%     1.237247   1.789812   0.343037   0.616695   0.350845   0.123099   \nmax     1.329557   1.890884   0.430578   0.701262   0.450793   0.203214   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.181925   80.909836   0.039877   0.094603  \nstd     0.025022    9.029538   0.014663   0.031388  \nmin     0.150119   67.915820   0.013417   0.031652  \n25%     0.164187   74.465798   0.025832   0.068632  \n50%     0.174361   80.450314   0.037480   0.100163  \n75%     0.197729   88.393075   0.050413   0.112495  \nmax     0.249080  101.466523   0.065988   0.147605  \n--------------------------------------------------------------------------------\nTest metrics on andisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.561501   2.013977   0.584894   0.728846   0.592610   0.428996   \nstd     0.054676   0.214017   0.028844   0.027988   0.286190   0.451123   \nmin     1.462280   1.607348   0.528559   0.677337   0.344830   0.118907   \n25%     1.522535   1.939558   0.564517   0.712462   0.397968   0.158418   \n50%     1.560284   2.031585   0.585664   0.727166   0.479231   0.229689   \n75%     1.594774   2.100454   0.603265   0.746108   0.630045   0.397882   \nmax     1.659770   2.348595   0.633790   0.781835   1.299265   1.688089   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.281092  106.394236   0.015788   0.033443  \nstd     0.049266   11.345530   0.017747   0.037467  \nmin     0.205634   84.861332  -0.015052  -0.033935  \n25%     0.241596   98.732944   0.005526   0.013816  \n50%     0.288964  106.024153   0.013751   0.028963  \n75%     0.307314  112.704328   0.029538   0.064234  \nmax     0.390000  130.842886   0.043541   0.094777  \n--------------------------------------------------------------------------------\nTest metrics on histosols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.711375   2.500980   0.637454   0.778795   1.165003   1.492735   \nstd     0.214934   0.635935   0.090172   0.055099   0.377669   1.060323   \nmin     1.374536   1.609684   0.462316   0.677843   0.634321   0.402363   \n25%     1.566681   2.034666   0.586609   0.753988   0.927356   0.860062   \n50%     1.681201   2.410043   0.641241   0.776610   1.093495   1.195750   \n75%     1.820641   2.907901   0.693132   0.811247   1.278496   1.634970   \nmax     2.161752   3.628460   0.782561   0.867816   2.175235   4.731648   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.610852  243.155805   0.001554  -0.003005  \nstd     0.133948   46.102071   0.035374   0.045302  \nmin     0.380170  172.238410  -0.064465  -0.110639  \n25%     0.546008  210.254481  -0.010255  -0.015323  \n50%     0.567238  231.883135  -0.000548  -0.000847  \n75%     0.674668  284.666143   0.009428   0.010923  \nmax     0.865779  324.890279   0.077207   0.078856  \n--------------------------------------------------------------------------------\nTest metrics on vertisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.524148   2.101732   0.547234   0.732481   0.390509   0.161691   \nstd     0.175537   0.331334   0.101618   0.061666   0.098375   0.090253   \nmin     1.233878   1.556100   0.333783   0.607804   0.279383   0.078055   \n25%     1.428539   1.912700   0.503573   0.695811   0.327822   0.107521   \n50%     1.478822   2.143325   0.536402   0.728507   0.361795   0.130898   \n75%     1.632835   2.287894   0.619066   0.781692   0.423892   0.179688   \nmax     1.895009   2.787665   0.717768   0.831755   0.673910   0.454155   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.238076   98.744299  -0.052767  -0.134787  \nstd     0.035953   12.347859   0.015789   0.043975  \nmin     0.183381   83.009898  -0.075401  -0.206095  \n25%     0.206193   89.681748  -0.061264  -0.162066  \n50%     0.231768   94.428492  -0.055714  -0.138102  \n75%     0.259825  106.099271  -0.046249  -0.110102  \nmax     0.309477  127.327009  -0.015200  -0.042732  \n--------------------------------------------------------------------------------\nTest metrics on aridisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse         mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000   20.000000   \nmean    1.378008   1.827588   0.444671   0.686068   2.131294   11.685892   \nstd     0.164302   0.273668   0.154683   0.067033   2.742162   28.829063   \nmin     1.015784   1.353244   0.024153   0.533021   0.484385    0.234629   \n25%     1.294828   1.616959   0.398816   0.656175   0.657982    0.433422   \n50%     1.420750   1.811119   0.501060   0.704963   1.069559    1.148925   \n75%     1.471227   2.008619   0.534730   0.727574   1.995471    4.232994   \nmax     1.643348   2.293749   0.626948   0.771557  10.787938  116.379602   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.500131  145.908265  -0.010957  -0.025541  \nstd     0.250221   56.170231   0.018981   0.046494  \nmin     0.294676   96.568375  -0.047555  -0.126001  \n25%     0.352105  113.472506  -0.021256  -0.045881  \n50%     0.403889  124.989228  -0.007535  -0.013668  \n75%     0.562606  154.904255   0.004738   0.008801  \nmax     1.257594  322.681936   0.015741   0.036257  \n--------------------------------------------------------------------------------\nTest metrics on gelisols\n--------------------------------------------------------------------------------\n             rpd       rpiq         r2       lccc       rmse        mse  \\\ncount  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \nmean    1.692878   2.534223   0.626330   0.766703   0.646289   0.435929   \nstd     0.200619   0.761655   0.096257   0.059995   0.138561   0.188936   \nmin     1.274587   1.094882   0.368254   0.619306   0.418552   0.175186   \n25%     1.607986   2.005345   0.602193   0.747244   0.552801   0.305750   \n50%     1.651799   2.517481   0.624528   0.765263   0.631649   0.398982   \n75%     1.849051   2.810388   0.698211   0.819691   0.723203   0.523037   \nmax     2.033034   3.926091   0.752298   0.864522   0.954032   0.910176   \n\n             mae        mape       bias        stb  \ncount  20.000000   20.000000  20.000000  20.000000  \nmean    0.389866  193.838336  -0.088982  -0.145424  \nstd     0.084392   31.399886   0.034638   0.082395  \nmin     0.255306  134.782910  -0.161743  -0.406283  \n25%     0.324760  169.096695  -0.107498  -0.178604  \n50%     0.390097  191.552850  -0.092811  -0.129146  \n75%     0.459532  213.219983  -0.058120  -0.106705  \nmax     0.542090  257.645578  -0.023931  -0.030365  \n\n\n\n\nEvaluate on Mollisols\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/all/models')\nseeds = range(20)\norder = 1\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_global_mollisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                 order=1,\n                                                 src_dir_model=src_dir_model)\n\n\nperfs_global_mollisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.582966\n      2.090417\n      0.599952\n      0.742203\n      0.769248\n      0.629664\n      0.307466\n      94.298259\n      0.001662\n      0.003734\n    \n    \n      std\n      0.034982\n      0.078073\n      0.017652\n      0.012356\n      0.199794\n      0.322337\n      0.018656\n      3.052491\n      0.008008\n      0.018876\n    \n    \n      min\n      1.525933\n      1.972214\n      0.570089\n      0.722419\n      0.526864\n      0.277585\n      0.279430\n      87.394573\n      -0.013899\n      -0.033922\n    \n    \n      25%\n      1.558545\n      2.042070\n      0.587883\n      0.734876\n      0.600345\n      0.360454\n      0.289349\n      92.537801\n      -0.004175\n      -0.009866\n    \n    \n      50%\n      1.579016\n      2.077462\n      0.598502\n      0.738004\n      0.735055\n      0.541195\n      0.308846\n      94.409650\n      0.001370\n      0.003153\n    \n    \n      75%\n      1.610838\n      2.141203\n      0.614204\n      0.752421\n      0.917944\n      0.842758\n      0.320209\n      96.313669\n      0.008152\n      0.018857\n    \n    \n      max\n      1.642984\n      2.250635\n      0.629145\n      0.764131\n      1.123730\n      1.262768\n      0.340558\n      98.688058\n      0.014374\n      0.032833\n    \n  \n\n\n\n\n\n\nEvaluate on Gelisols\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/all/models')\nseeds = range(20)\norder = 12\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_global_gelisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                order=order,\n                                                src_dir_model=src_dir_model)\n\n\nperfs_global_gelisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.692878\n      2.534223\n      0.626330\n      0.766703\n      0.646289\n      0.435929\n      0.389866\n      193.838336\n      -0.088982\n      -0.145424\n    \n    \n      std\n      0.200619\n      0.761655\n      0.096257\n      0.059995\n      0.138561\n      0.188936\n      0.084392\n      31.399886\n      0.034638\n      0.082395\n    \n    \n      min\n      1.274587\n      1.094882\n      0.368254\n      0.619306\n      0.418552\n      0.175186\n      0.255306\n      134.782910\n      -0.161743\n      -0.406283\n    \n    \n      25%\n      1.607986\n      2.005345\n      0.602193\n      0.747244\n      0.552801\n      0.305750\n      0.324760\n      169.096695\n      -0.107498\n      -0.178604\n    \n    \n      50%\n      1.651799\n      2.517481\n      0.624528\n      0.765263\n      0.631649\n      0.398982\n      0.390097\n      191.552850\n      -0.092811\n      -0.129146\n    \n    \n      75%\n      1.849051\n      2.810388\n      0.698211\n      0.819691\n      0.723203\n      0.523037\n      0.459532\n      213.219983\n      -0.058120\n      -0.106705\n    \n    \n      max\n      2.033034\n      3.926091\n      0.752298\n      0.864522\n      0.954032\n      0.910176\n      0.542090\n      257.645578\n      -0.023931\n      -0.030365\n    \n  \n\n\n\n\n\n\nEvaluate on Vertisols\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/all/models')\nseeds = range(20)\norder = 10\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_global_vertisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                 order=order,\n                                                 src_dir_model=src_dir_model)\n\n\nperfs_global_vertisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.524148\n      2.101732\n      0.547234\n      0.732481\n      0.390509\n      0.161691\n      0.238076\n      98.744299\n      -0.052767\n      -0.134787\n    \n    \n      std\n      0.175537\n      0.331334\n      0.101618\n      0.061666\n      0.098375\n      0.090253\n      0.035953\n      12.347859\n      0.015789\n      0.043975\n    \n    \n      min\n      1.233878\n      1.556100\n      0.333783\n      0.607804\n      0.279383\n      0.078055\n      0.183381\n      83.009898\n      -0.075401\n      -0.206095\n    \n    \n      25%\n      1.428539\n      1.912700\n      0.503573\n      0.695811\n      0.327822\n      0.107521\n      0.206193\n      89.681748\n      -0.061264\n      -0.162066\n    \n    \n      50%\n      1.478822\n      2.143325\n      0.536402\n      0.728507\n      0.361795\n      0.130898\n      0.231768\n      94.428492\n      -0.055714\n      -0.138102\n    \n    \n      75%\n      1.632835\n      2.287894\n      0.619066\n      0.781692\n      0.423892\n      0.179688\n      0.259825\n      106.099271\n      -0.046249\n      -0.110102\n    \n    \n      max\n      1.895009\n      2.787665\n      0.717768\n      0.831755\n      0.673910\n      0.454155\n      0.309477\n      127.327009\n      -0.015200\n      -0.042732\n    \n  \n\n\n\n\n\n\n\nTrain and test on Mollisols\n\ndest_dir_model = Path('files/dumps/plsr/train_eval/mollisols/models')\nseeds = range(20)\norder = 1\nlearners = Learners(tax_lookup, seeds=seeds)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               n_cpts_range=range(30, 60, 2),\n               delta=2e-3,\n               dest_dir_model=dest_dir_model)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n# of components chosen: 46\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n# of components chosen: 52\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n# of components chosen: 40\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n# of components chosen: 46\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n# of components chosen: 40\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n# of components chosen: 40\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n# of components chosen: 40\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n# of components chosen: 54\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n# of components chosen: 56\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n# of components chosen: 34\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n# of components chosen: 40\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n# of components chosen: 50\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n# of components chosen: 34\n\n\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/mollisols/models')\nseeds = range(20)\norder = 1\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_local_mollisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                order=order,\n                                                src_dir_model=src_dir_model)\nperfs_local_mollisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      1.729961\n      2.284160\n      0.664719\n      0.803311\n      0.682758\n      0.501425\n      0.271352\n      102.920252\n      0.000623\n      0.001149\n    \n    \n      std\n      0.049480\n      0.085176\n      0.019570\n      0.011727\n      0.192671\n      0.281706\n      0.014817\n      3.289161\n      0.008043\n      0.018940\n    \n    \n      min\n      1.620689\n      2.157094\n      0.618909\n      0.776560\n      0.431012\n      0.185771\n      0.243688\n      95.089908\n      -0.016145\n      -0.039405\n    \n    \n      25%\n      1.704100\n      2.221866\n      0.655256\n      0.801153\n      0.535298\n      0.286687\n      0.261261\n      100.989098\n      -0.005376\n      -0.012943\n    \n    \n      50%\n      1.734100\n      2.282031\n      0.667110\n      0.803882\n      0.609356\n      0.371682\n      0.273628\n      103.076593\n      -0.001222\n      -0.002843\n    \n    \n      75%\n      1.759684\n      2.322080\n      0.676704\n      0.809385\n      0.793591\n      0.630660\n      0.275906\n      105.139386\n      0.007436\n      0.017265\n    \n    \n      max\n      1.827361\n      2.468705\n      0.700207\n      0.826625\n      1.028421\n      1.057650\n      0.303412\n      108.383273\n      0.013749\n      0.033221\n    \n  \n\n\n\n\n\n\nTrain and test on Gelisols\n\ndest_dir_model = Path('files/dumps/plsr/train_eval/gelisols/models')\nseeds = range(20)\norder = 12\nlearners = Learners(tax_lookup, seeds=seeds)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               n_cpts_range=range(2, 10, 1),\n               delta=1e-2,\n               dest_dir_model=dest_dir_model)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n# of components chosen: 8\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n# of components chosen: 7\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n# of components chosen: 7\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n# of components chosen: 7\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n# of components chosen: 9\n\n\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/gelisols/models')\nseeds = range(20)\norder = 12\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_local_gelisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                order=order,\n                                                src_dir_model=src_dir_model)\nperfs_local_gelisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.077886\n      3.084523\n      0.746774\n      0.858587\n      0.646908\n      0.452289\n      0.319878\n      205.707385\n      -0.002118\n      -0.006263\n    \n    \n      std\n      0.308216\n      0.896429\n      0.076636\n      0.042218\n      0.188620\n      0.248623\n      0.069856\n      44.668977\n      0.040502\n      0.064437\n    \n    \n      min\n      1.547630\n      1.263899\n      0.570562\n      0.773000\n      0.348828\n      0.121681\n      0.208846\n      122.575289\n      -0.067213\n      -0.125266\n    \n    \n      25%\n      1.823879\n      2.564071\n      0.692319\n      0.827514\n      0.530327\n      0.281372\n      0.269832\n      172.185662\n      -0.025272\n      -0.038202\n    \n    \n      50%\n      2.053352\n      3.090948\n      0.756956\n      0.857239\n      0.614913\n      0.378319\n      0.308076\n      204.408200\n      -0.007451\n      -0.014036\n    \n    \n      75%\n      2.299863\n      3.404348\n      0.805802\n      0.899488\n      0.817420\n      0.669407\n      0.361622\n      227.222367\n      0.016758\n      0.020807\n    \n    \n      max\n      2.652731\n      5.047128\n      0.851434\n      0.914563\n      0.913832\n      0.835089\n      0.452944\n      302.505106\n      0.103336\n      0.170306\n    \n  \n\n\n\n\n\n\nTrain and test on Vertisols\n\ndest_dir_model = Path('files/dumps/plsr/train_eval/vertisols/models')\nseeds = range(20)\norder = 10\nlearners = Learners(tax_lookup, seeds=seeds)\nlearners.train((X, y, depth_order[:, -1]), \n               order=order,\n               n_cpts_range=range(2, 30, 1),\n               delta=1e-2,\n               dest_dir_model=dest_dir_model)\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n# of components chosen: 11\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n# of components chosen: 11\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n# of components chosen: 20\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n# of components chosen: 18\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n# of components chosen: 12\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n# of components chosen: 10\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n# of components chosen: 17\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n# of components chosen: 8\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n# of components chosen: 15\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n# of components chosen: 11\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n# of components chosen: 9\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n# of components chosen: 12\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n# of components chosen: 19\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n# of components chosen: 11\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n# of components chosen: 12\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n# of components chosen: 15\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n# of components chosen: 14\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n# of components chosen: 12\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n# of components chosen: 8\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n# of components chosen: 11\n\n\n\nsrc_dir_model = Path('files/dumps/plsr/train_eval/vertisols/models')\nseeds = range(20)\norder = 10\nlearners = Learners(tax_lookup, seeds=seeds)\nperfs_local_vertisols, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n                                                order=order,\n                                                src_dir_model=src_dir_model)\nperfs_local_vertisols.describe()\n\n\n\n\n\n  \n    \n      \n      rpd\n      rpiq\n      r2\n      lccc\n      rmse\n      mse\n      mae\n      mape\n      bias\n      stb\n    \n  \n  \n    \n      count\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n      20.000000\n    \n    \n      mean\n      2.009008\n      2.772106\n      0.737918\n      0.855452\n      0.285306\n      0.084297\n      0.187213\n      93.891373\n      -0.004065\n      -0.011017\n    \n    \n      std\n      0.233000\n      0.461799\n      0.071342\n      0.035866\n      0.055230\n      0.033674\n      0.024124\n      11.393788\n      0.020619\n      0.053431\n    \n    \n      min\n      1.410521\n      1.902507\n      0.490198\n      0.741338\n      0.182039\n      0.033138\n      0.129494\n      76.538749\n      -0.041485\n      -0.121708\n    \n    \n      25%\n      1.894380\n      2.447912\n      0.717390\n      0.849941\n      0.247126\n      0.061071\n      0.173790\n      85.490789\n      -0.016195\n      -0.042687\n    \n    \n      50%\n      2.024185\n      2.805916\n      0.752705\n      0.860249\n      0.283139\n      0.080169\n      0.188396\n      91.373366\n      -0.002301\n      -0.006435\n    \n    \n      75%\n      2.081399\n      3.036756\n      0.766082\n      0.870251\n      0.299627\n      0.089804\n      0.204240\n      102.965234\n      0.007755\n      0.018465\n    \n    \n      max\n      2.549723\n      3.779666\n      0.843884\n      0.916982\n      0.418275\n      0.174954\n      0.223513\n      113.539148\n      0.036139\n      0.086563\n    \n  \n\n\n\n\n\n\nCompile metrics for “local vs. global” Fig. 6\n\ndef format_metrics(*dfs):\n    perfs = {'r2': {'mean': [], 'std': []},\n             'mape': {'mean': [], 'std': []}}\n    for df in dfs:\n        for metric in ['r2', 'mape']:\n            mean, std = df.describe().loc[['mean', 'std'], metric].items()\n            perfs[metric]['mean'].append(mean[1])\n            perfs[metric]['std'].append(std[1])\n    return perfs\n\n\nperfs = {}\nperfs['global'] = format_metrics(perfs_global_mollisols, perfs_global_gelisols, perfs_global_vertisols)\nperfs['local'] = format_metrics(perfs_local_mollisols, perfs_local_gelisols, perfs_local_vertisols)\n\n\nperfs\n\n{'global': {'r2': {'mean': [0.5999520975577833,\n    0.6263296407122689,\n    0.5472338139745212],\n   'std': [0.01765197454826005, 0.09625659382541236, 0.10161782452206956]},\n  'mape': {'mean': [94.29825920533892, 193.83833562261376, 98.74429900717739],\n   'std': [3.0524914231457974, 31.399886042493737, 12.347859402802634]}},\n 'local': {'r2': {'mean': [0.6647188332987304,\n    0.7467735410966299,\n    0.737918199125363],\n   'std': [0.01957008650036342, 0.07663555355310155, 0.07134199041747523]},\n  'mape': {'mean': [102.92025183047231, 205.70738511998024, 93.89137312512031],\n   'std': [3.289160840803382, 44.668977271871476, 11.393787965933955]}}}\n\n\n\ndest_dir = Path('files/dumps/plsr')\nwith open(dest_dir/'global_vs_local.pickle', 'wb') as f: \n                pickle.dump(perfs, f)"
  },
  {
    "objectID": "paper/eda.html#loading-data",
    "href": "paper/eda.html#loading-data",
    "title": "1. EDA (Exploratory Data Analysis)",
    "section": "1. Loading data",
    "text": "1. Loading data\n\nsrc_dir = 'data' # Or any folder or mounted drive folder\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (50494, 1764)\ny shape: (50494,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[43.  2.]\n [ 0.  0.]\n [ 0.  1.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}"
  },
  {
    "objectID": "paper/eda.html#overview",
    "href": "paper/eda.html#overview",
    "title": "1. EDA (Exploratory Data Analysis)",
    "section": "2. Overview",
    "text": "2. Overview\n\n2.1 Target variable (exchangeable potassium)\n\nstats.describe(y)\n\nDescribeResult(nobs=50494, minmax=(0.0, 32.3309691), mean=0.663862353167703, variance=1.1346917228463702, skewness=7.074360641969034, kurtosis=103.08921041432097)\n\n\n\nq1, median, q3 = np.percentile(y, [25, 50, 75])\nprint(f'First quartile: {q1:.2f}, Median: {median:.2f}, Third quartile: {q3:.2f}')\n\nFirst quartile: 0.16, Median: 0.36, Third quartile: 0.74\n\n\n\n# Histogram of exchangeabme potassium\nfig, ax = plt.subplots(figsize=(10, 6))\nplt.xlabel('Potassium, NH4OAc Extractable, 2M KCl displacement in cmol(+)/kg')\nplt.ylabel('# of samples (Log scale)')\nax.grid(True, which='both')\nax.set_axisbelow(True)\nax.hist(y, bins=20, log=True, histtype='bar', cumulative=False);\n\n\n\n\n\n\n2.2 Features (Mid-Infrared spectra)\n\nplot_spectra(X, X_names, figsize=(12, 4), sample=20)\n\n\n\n\n\n\n2.3 Count and distribution by Soil Taxonomy order\n\nsummary_plot(y, depth_order, tax_lookup)"
  },
  {
    "objectID": "paper/plsr_learning_curve.html#load-and-transform",
    "href": "paper/plsr_learning_curve.html#load-and-transform",
    "title": "3.1. Learning Curve (PLSR)",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = 'data' # Replace by the path your downloaded the data to\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (40132, 1764)\ny shape: (40132,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[43.  2.]\n [ 0.  0.]\n [ 0.  1.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}"
  },
  {
    "objectID": "paper/plsr_learning_curve.html#setup",
    "href": "paper/plsr_learning_curve.html#setup",
    "title": "3.1. Learning Curve (PLSR)",
    "section": "Setup",
    "text": "Setup\n\ntraining_size = [500, 1000, 2000, 5000, 10000, 20000, 30000, X.shape[0]]\nsplit_ratio = 0.1\n\ndef n_cpts_range(size):\n    if size <= 500:\n        return range(2, 10, 1)\n    elif size <= 2000:\n        return range(10, 16, 2)\n    elif size <= 5000:\n        return range(16, 30, 2)\n    else:\n        return range(40, 60, 5)\n        \nseeds = range(20)\ndest_dir = Path('files/dumps/plsr/learning_curve')"
  },
  {
    "objectID": "paper/plsr_learning_curve.html#experiment",
    "href": "paper/plsr_learning_curve.html#experiment",
    "title": "3.1. Learning Curve (PLSR)",
    "section": "Experiment",
    "text": "Experiment\n\nfor seed in seeds:\n    perfs_by_size = OrderedDict({'seed': [], 'n_samples': [], 'test_score': [], 'n_cpts': []})\n    for size in training_size:\n        print(80*'-')\n        print(f'Seed: {seed} | Size: {size}')\n        print(80*'-')\n        idx = np.random.choice(len(X), size, replace=False)\n        # train/test\n        X_train, X_test, y_train, y_test = train_test_split(X[idx, :], \n                                                            y[idx], \n                                                            test_size=split_ratio,\n                                                            random_state=seed) \n        # Further train/valid\n        X_train, X_valid, y_train, y_valid = train_test_split(X_train, \n                                                              y_train, \n                                                              test_size=split_ratio, \n                                                              random_state=seed)\n        model = Pipeline([\n            ('snv', SNV()),\n            ('derivative', TakeDerivative(window_length=11, polyorder=1, deriv=1)),\n            ('dropper', DropSpectralRegions(X_names, regions=CO2_REGION)),\n            ('model', PLSRegression())])\n\n        learner = Learner((X_train, X_valid, y_train, y_valid), model)\n        \n        learner.find_hp(n_cpts_range=n_cpts_range(size), delta=1e-2, verbose=False)\n        learner.fit()\n        perfs_by_size['seed'].append(seed)\n        perfs_by_size['n_samples'].append(size)\n        perfs_by_size['n_cpts'].append(learner.n_cpts_best)\n        perfs_by_size['test_score'].append(learner.evaluate(X_test, y_test))\n    with open(dest_dir/f'plsr-lc-seed-{seed}.pickle', 'wb') as f: \n        pickle.dump(perfs_by_size, f)\n\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 0 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 1 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 2 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 3 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 4 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 10000\n--------------------------------------------------------------------------------\n\n\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 5 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 6 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 7 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 8 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 1000\n--------------------------------------------------------------------------------\n\n\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 11 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 12 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 13 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 14 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 15 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 30000\n--------------------------------------------------------------------------------\n\n\n--------------------------------------------------------------------------------\nSeed: 16 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 17 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 18 | Size: 40132\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 500\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 1000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 2000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 5000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 10000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 20000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 30000\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nSeed: 19 | Size: 40132\n--------------------------------------------------------------------------------\n\n\n\nsrc_dir = Path('files/dumps/plsr/learning_curve')\ndumps = load_dumps(src_dir)\n\n\ndf = pd.concat([pd.DataFrame(perf) for perf in dumps])\ngrps = df[df['seed'] != 15].groupby(['n_samples']).agg({'test_score':['mean','std'], 'n_cpts':['mean','std']})\ngrps.head(10)\n\n\n\n\n\n  \n    \n      \n      test_score\n      n_cpts\n    \n    \n      \n      mean\n      std\n      mean\n      std\n    \n    \n      n_samples\n      \n      \n      \n      \n    \n  \n  \n    \n      500\n      0.363073\n      0.170815\n      8.421053\n      1.574653\n    \n    \n      1000\n      0.448904\n      0.064789\n      14.000000\n      0.000000\n    \n    \n      2000\n      0.479573\n      0.065036\n      14.000000\n      0.000000\n    \n    \n      5000\n      0.563470\n      0.080684\n      25.578947\n      3.501044\n    \n    \n      10000\n      0.619376\n      0.031239\n      51.315789\n      2.262070\n    \n    \n      20000\n      0.634018\n      0.016620\n      50.526316\n      1.576509\n    \n    \n      30000\n      0.633537\n      0.013978\n      50.526316\n      1.576509\n    \n    \n      40132\n      0.639718\n      0.012636\n      51.052632\n      2.094270\n    \n  \n\n\n\n\n\ndf = pd.concat([pd.DataFrame(perf) for perf in dumps])\ngrps = df.groupby(['n_samples']).agg({'test_score':['mean','std'], 'n_cpts':['mean','std']})\ngrps.head(10)\n\n\n\n\n\n  \n    \n      \n      test_score\n      n_cpts\n    \n    \n      \n      mean\n      std\n      mean\n      std\n    \n    \n      n_samples\n      \n      \n      \n      \n    \n  \n  \n    \n      500\n      0.357241\n      0.168292\n      8.45\n      1.538112\n    \n    \n      1000\n      0.446311\n      0.064118\n      14.00\n      0.000000\n    \n    \n      2000\n      0.484049\n      0.066392\n      14.00\n      0.000000\n    \n    \n      5000\n      0.563827\n      0.078549\n      25.30\n      3.628832\n    \n    \n      10000\n      0.619452\n      0.030407\n      51.50\n      2.350812\n    \n    \n      20000\n      0.634474\n      0.016305\n      50.50\n      1.538968\n    \n    \n      30000\n      0.633108\n      0.013739\n      50.50\n      1.538968\n    \n    \n      40132\n      0.639170\n      0.012540\n      51.00\n      2.051957\n    \n  \n\n\n\n\n\ngrps.columns = grps.columns.map('_'.join)\ngrps.reset_index().plot(x='n_samples', y='test_score_mean', logx=True);"
  },
  {
    "objectID": "paper/figures_global_vs_local.html#input-data",
    "href": "paper/figures_global_vs_local.html#input-data",
    "title": "5.3. Global vs. local modelling",
    "section": "Input data",
    "text": "Input data\nTo generate the learning curves for both the PLSR and CNN models, run the following notebooks: * PLSR training & evaluation * CNN training & evaluation\nInstead, we load already generated and saved data: global_vs_local.pickle.\n\nsrc_dir = Path('dumps')\n\n\nfname = 'global_vs_local.pickle'\n\nplsr_eval = pickle.load(open(src_dir/'plsr'/fname, \"rb\"))\ncnn_eval = pickle.load(open(src_dir/'cnn'/fname, \"rb\"))\n\n\ncnn_eval\n\n{'global': {'r2': {'mean': [0.7676331478787332,\n    0.7426117015170665,\n    0.7447822268117752],\n   'std': [0.02264828790526842, 0.07670269031321013, 0.06303328480007991]},\n  'mape': {'mean': [27.362060993909836, 47.8833418753412, 26.85565337538719],\n   'std': [1.171648213272994, 10.194858798150458, 3.2722564855592804]}},\n 'local': {'r2': {'mean': [0.7727136487719102,\n    0.691142579693905,\n    0.6691275530909808],\n   'std': [0.019084647824623407, 0.08809393616324758, 0.06941770320711035]},\n  'mape': {'mean': [26.96375921368599, 51.508686112033, 30.958226919174194],\n   'std': [1.3522711409175694, 12.377707161654318, 4.237754935060679]}}}"
  },
  {
    "objectID": "paper/figures_global_vs_local.html#plot",
    "href": "paper/figures_global_vs_local.html#plot",
    "title": "5.3. Global vs. local modelling",
    "section": "Plot",
    "text": "Plot\n\ndef plot_global_local_metric(data_global, data_local, \n                             labels=['Mollisols', 'Gelisols', 'Vertisols'], \n                             ax=None, delta=0.04, \n                             global_kwargs={}, local_kwargs={}):\n    x = np.arange(len(labels)) \n    ax.errorbar(x-delta, data_global['mean'], yerr=data_global['std'], label='Global', **global_kwargs)\n    ax.errorbar(x+delta, data_local['mean'], yerr=data_local['std'], label='Local', **local_kwargs)\n    ax.set_xticks(x, labels)\n    return(ax)\n\ndef plot_global_vs_local(plsr, cnn, labels,\n                         figsize=(16*centimeter,6*centimeter), dpi=600):\n    # Adjust styles\n    p = plt.rcParams\n    p[\"xtick.minor.visible\"] = False\n\n    # Layout \n    fig = plt.figure(figsize=figsize, dpi=600)\n    gs = GridSpec(nrows=2, ncols=2)\n\n    ax0 = fig.add_subplot(gs[0, 0])\n    ax0.set_title('(a) PLSR', loc='left')\n\n    ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n    ax1.set_title('(b) CNN', loc='left')\n\n    ax2 = fig.add_subplot(gs[1, 0])\n    ax3 = fig.add_subplot(gs[1, 1], sharey=ax2)\n\n    # Plots\n    global_params = {'fmt':'o', 'mfc':'w', 'ms': 3, 'c': 'C0', 'ecolor': 'C0', 'elinewidth': 1}\n    local_params = {'fmt':'o', 'mfc':'w', 'ms': 3, 'c': 'C1', 'ecolor': 'C1', 'elinewidth': 1}\n    plot_global_local_metric(plsr['global']['r2'], \n                             plsr['local']['r2'],\n                             labels, ax=ax0, global_kwargs=global_params, \n                             local_kwargs=local_params)\n\n    plot_global_local_metric(cnn['global']['r2'], \n                             cnn['local']['r2'],\n                             labels, ax=ax1, global_kwargs=global_params, \n                             local_kwargs=local_params)\n\n    plot_global_local_metric(plsr['global']['mape'], \n                             plsr['local']['mape'],\n                             labels, ax=ax2, global_kwargs=global_params, \n                             local_kwargs=local_params)\n\n    plot_global_local_metric(cnn['global']['mape'], \n                             cnn['local']['mape'],\n                             labels, ax=ax3, global_kwargs=global_params, \n                             local_kwargs=local_params)\n        \n\n    # Ornaments\n    ax0.set_ylabel('$R^2$ →', loc='top')\n    ax2.set_ylabel('MAPE (%) →', loc='top')\n    ax0.set_xticklabels([])\n    ax1.set_xticklabels([])\n    \n    handles, labs = ax0.get_legend_handles_labels()\n    fig.legend(handles, labs,\n               frameon=False, ncol=2, loc='upper center',  \n               bbox_to_anchor=(0.4, 0.99))\n \n    for ax in [ax0, ax1, ax2, ax3]:\n        ax.grid(True, \"minor\", color=\"0.85\", linewidth=0.2, zorder=-2)\n        ax.grid(True, \"major\", color=\"0.65\", linewidth=0.4, zorder=-1) \n\n    plt.tight_layout()\n\n\n#FIG_PATH = Path('nameofyourfolder')\nFIG_PATH = Path('images')\n\nset_style(DEFAULT_STYLE)\nplot_global_vs_local(plsr_eval, cnn_eval, ['Mollisols', 'Gelisols', 'Vertisols'],\n                     figsize=(16*centimeter,9*centimeter), dpi=600)\n\n# To save/export it\nplt.savefig(FIG_PATH/'global_vs_local.png', dpi=600, transparent=True, format='png')"
  },
  {
    "objectID": "paper/figures_observed_vs_predicted.html#input-data",
    "href": "paper/figures_observed_vs_predicted.html#input-data",
    "title": "5.2. Observed vs. predicted scatterplots",
    "section": "Input data",
    "text": "Input data\nTo predict exchangeable potassium content with both the PLSR and CNN models, run the following notebooks: * PLSR training & evaluation * CNN training & evaluation\nInstead, we load already predicted and saved values.\n\nsrc_dir = Path('dumps')\ny_hat_plsr, y_true_plsr = pickle.load(open(src_dir/'predicted-true-plsr-seed-1.pickle', \"rb\"))\ny_hat_cnn, y_true_cnn = pickle.load(open(src_dir/'predicted-true-cnn-seed-1.pickle', \"rb\"))\nprint(f'y_hat_plsr shape: {y_hat_plsr.shape}, y_hat_cnn shape: {y_hat_cnn.shape}')\n\ny_hat_plsr shape: (4014,), y_hat_cnn shape: (4014,)"
  },
  {
    "objectID": "paper/figures_observed_vs_predicted.html#plot",
    "href": "paper/figures_observed_vs_predicted.html#plot",
    "title": "5.2. Observed vs. predicted scatterplots",
    "section": "Plot",
    "text": "Plot\n\ndef plot_hexbin_scatter(x, Y, ax=None, hb_kwargs={}):\n    if ax is None:\n        ax = plt.gca()\n    hb = ax.hexbin(x, Y, **hb_kwargs)\n    ax.set_aspect('equal')\n    return (ax, hb)\n\ndef get_color_norm(x, Y, ax, n_bins=5, hb_kwargs={}):\n    hb = ax.hexbin(x, Y, cmap='viridis', **hb_kwargs)\n    bounds = np.rint(np.histogram_bin_edges(hb.get_array(), n_bins))\n    bounds[0] = bounds[0] + 1\n    norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=n_bins+1, extend='min')\n    return norm\n\ndef plot_obs_vs_pred(data_plsr, data_cnn,\n                     gridsize=35, \n                     figsize=(16*centimeter,8*centimeter), \n                     dpi=600):\n    # Styles\n    p = plt.rcParams\n    p[\"grid.color\"] = \"0.65\"\n    p[\"grid.linewidth\"] = 0.4\n\n    # Layout \n    fig = plt.figure(figsize=figsize, dpi=dpi)\n    nrows, ncols = 1, 3\n    w1, w2, w3 = 20, 20, 2\n    gs = GridSpec(nrows=nrows, ncols=ncols, figure=fig, width_ratios=[w1, w2, w3])\n    ax0 = fig.add_subplot(gs[0, 0])\n    ax0.set_title('(a)', loc='left')\n    ax1 = fig.add_subplot(gs[0, 1]) \n    ax1.set_title('(b)', loc='left')\n\n    # color norm based on cnn perfs\n    params_hb = {'gridsize': gridsize, 'mincnt': 1, 'alpha': 0}\n    x, Y = data_cnn\n    norm = get_color_norm(x, Y, ax1, hb_kwargs=params_hb)\n\n    # Calculate color scale adapted to grid resolution\n    for ax, (x, Y) in zip([ax0, ax1], [data_plsr, data_cnn]):\n        params_hb = {'gridsize': gridsize, 'cmap': cm.get_cmap('Spectral_r', norm.N), \n                    'mincnt': 1, 'norm': norm, 'linewidths': 0.2}\n        _, hb = plot_hexbin_scatter(x, Y, ax=ax, hb_kwargs=params_hb)\n        ax.plot([-1.25, 1.25], [-1.25, 1.25], 'k--', lw=0.75)\n\n    ax_clb = plt.subplot(gs[0, 2], aspect=w1)\n    ax_clb\n    p[\"xtick.direction\"] = \"out\"\n    clb = plt.colorbar(hb, cax=ax_clb)\n    clb.ax.tick_params(axis='y', direction='out')\n    clb.ax.set_title('Counts', size=8)\n\n    # Ornaments\n    ax0.set_ylabel('Observed ex-K ($log_{10}(cmol(+)kg^{-1})$) →', loc='top')\n    ax0.set_xlabel('Predicted ex-K ($log_{10}(cmol(+)kg^{-1})$) →', loc='right')\n    ax1.set_xlabel('Predicted ex-K ($log_{10}(cmol(+)kg^{-1})$) →', loc='right')\n\n    plt.tight_layout()\n\n\n#FIG_PATH = Path('nameofyourfolder')\nFIG_PATH = Path('images/')\nset_style(DEFAULT_STYLE)\nplot_obs_vs_pred((y_hat_plsr, y_true_plsr), (y_hat_cnn, y_true_cnn))\n\n# To save/export it\nplt.savefig(FIG_PATH/'observed-vs-predicted.png', dpi=600, transparent=True, format='png')"
  },
  {
    "objectID": "paper/interpretation_gradshap.html#load-and-transform",
    "href": "paper/interpretation_gradshap.html#load-and-transform",
    "title": "6.1. GradientShap values",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\n\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)"
  },
  {
    "objectID": "paper/interpretation_gradshap.html#experiment-gpu-required",
    "href": "paper/interpretation_gradshap.html#experiment-gpu-required",
    "title": "6.1. GradientShap values",
    "section": "Experiment (GPU required)",
    "text": "Experiment (GPU required)\n\nUtilities\n\ndef X_to_torch(X, preprocessing_fn=SNV(), device='cuda:0'):\n    if preprocessing_fn:\n        X = preprocessing_fn.fit_transform(X)\n    X = X.reshape(X.shape[-2], 1, -1)\n    return torch.tensor(X).to(device)  \n\ndef gradShap(X_baseline, X_inspect, model, n_baseline=100, \n             kwargs={'n_samples': 5, 'return_convergence_delta': True}):\n\n    idx_baseline = randint(X_baseline.shape[0], size=n_baseline)\n    X_baseline = X_baseline[idx_baseline, :]\n\n    gs = GradientShap(model, multiply_by_inputs=True)\n    shaps = []\n    for i in tqdm(range(len(X_inspect))):\n        gs_attr_test, delta = gs.attribute(X_to_torch(X_inspect[[i],:]), \n                                           baselines=X_to_torch(X_baseline), \n                                           **kwargs)\n        shaps.append(gs_attr_test.cpu().detach().numpy().ravel())\n\n    return np.array(shaps)\n\n\ndef reduce(dfs, colname='shap'):\n    df = pd.concat(dfs)\n    df.index.name = 'wn'\n    df = df.reset_index().groupby(['order', 'wn']).median()\n    df.reset_index(inplace=True)\n    df_reduced = []\n    for name, group in df.groupby('order'):\n        df_reduced.append(OrderedDict(\n            {colname: group.sort_values(by='wn', ascending=False)[colname].to_numpy(), \n            'order': name}))\n    return df_reduced\n\n\n\nSetup\n\n# Is a GPU available?\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(f'Runtime is: {device}')\n\nRuntime is: cuda:0\n\n\n\n\nRun\nComputes GradientShap values by Soil Taxonomy order.\n\nseeds = range(20)\nsplit_ratio = 0.1\nsrc_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n\nshap_by_order = []\nX_mean_by_order = []\n\nfor seed in seeds:\n    print(80*'-')\n    print(f'Seed: {seed}')\n    print(80*'-')\n\n    # Train/test split\n    data = train_test_split(X, y, depth_order, test_size=split_ratio, random_state=seed)\n    X_train, X_test, y_train, y_test, depth_order_train, depth_order_test = data\n\n    # Further Train/Valid split \n    data = train_test_split(X_train, y_train, depth_order_train, \n                            test_size=split_ratio, random_state=seed)\n    X_train, X_valid, y_train, y_valid, depth_order_train, depth_order_valid = data    \n\n    # load model\n    model = Model(X.shape[1], out_channel=16).to(device)\n    fname = f'model-seed-{seed}.pt'\n    if device.type == 'cpu':\n        model.load_state_dict(torch.load(src_dir/fname, map_location=torch.device('cpu')))\n    else:\n        model.load_state_dict(torch.load(src_dir/fname))\n    # Params are not learnable in \"eval\" model & Dropout is disabled\n    model.eval()\n\n    orders_label = ['all'] + list(tax_lookup.keys())\n\n    # Compute mean GradientShap value by orders\n    for order in tqdm(orders_label):\n        if order == 'all': \n            X_inspect = X_valid\n        else:\n            idx = tax_lookup[order]\n            mask = depth_order_valid[:, 1] == idx\n            X_inspect = X_valid[mask, :]\n        \n        X_mean = np.mean(X_inspect, axis=0)\n        shaps = gradShap(X_train, X_inspect, model, n_baseline=100)\n        shap_by_order.append(pd.DataFrame({'shap': np.mean(shaps, axis=0), 'order': order}, \n                                          index=X_names))\n        X_mean_by_order.append(pd.DataFrame({'X': X_mean, 'order': order}, \n                                         index=X_names))\n\n--------------------------------------------------------------------------------\nSeed: 0\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 1\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 2\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 3\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 4\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 5\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 6\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 7\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 8\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 9\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 10\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 11\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 12\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 13\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 14\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 15\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 16\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 17\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 18\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\nSeed: 19\n--------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshap_by_order, X_mean_by_order = reduce(shap_by_order), reduce(X_mean_by_order, colname='X')\n\n\n# Save it if required\n#dest_dir = Path('your_dumps_path')\ndest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/shaps')\nfname = 'shap_by_orders_02_09_2022.pickle'\nwith open(dest_dir/fname, 'wb') as f: \n    pickle.dump((shap_by_order, X_mean_by_order), f)"
  },
  {
    "objectID": "paper/interpretation_gradshap.html#plot",
    "href": "paper/interpretation_gradshap.html#plot",
    "title": "6.1. GradientShap values",
    "section": "Plot",
    "text": "Plot\n\nUtilities\n\ndef prettify_label(label, tax_lut):\n    return tax_lut[label].capitalize()\n\n\n\nReload\n\nsrc_dir = Path('./files/dumps')\nsrc_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/shaps')\nfname = 'shap_by_orders_02_09_2022.pickle'\nshap_by_order, X_mean_by_order  = pickle.load(open(src_dir/fname, \"rb\"))\n\n\ntax_pretty_lut = OrderedDict({'all': 'all', \n                              'undefined': 'undefined', \n                              'mollisols': 'molli.', \n                              'alfisols': 'alfi.', \n                              'inceptisols': 'incepti.', \n                              'ultisols': 'ulti.', \n                              'entisols': 'enti.', \n                              'aridisols': 'aridi.',   \n                              'andisols': 'andi.',\n                              'vertisols': 'verti.',\n                              'histosols': 'histo.',\n                              'spodosols': 'spodo.',\n                              'gelisols': 'geli.', \n                              'oxisols': 'oxi.'})\n\n\ndef plot_shaps_by_orders(attr_values, X, X_names, tax_pretty, diverging=False,\n                         annotate=True, figsize=(16*centimeter,6*centimeter), dpi=600):\n    # Styles\n    p = plt.rcParams\n    p[\"axes.spines.bottom\"] = False\n    p[\"axes.grid\"] = False\n    p[\"xtick.labelsize\"] = 6\n    p[\"xtick.direction\"] = \"in\"\n    p[\"xtick.major.size\"] = 3\n    p[\"xtick.major.width\"] = 0.5\n    p[\"xtick.minor.size\"] = 1\n    p[\"xtick.minor.width\"] = 0.25\n    p[\"ytick.left\"] = False\n    p[\"ytick.labelleft\"] = False\n    p[\"ytick.labelright\"] = False\n    p[\"ytick.major.size\"] = 3\n    p[\"ytick.major.width\"] = 0.5\n    p[\"ytick.minor.size\"] = 1\n    p[\"ytick.minor.width\"] = 0.25\n    p[\"ytick.minor.visible\"] = False\n\n    # Layout \n    fig, axes = plt.subplots(ncols=1, nrows=len(attr_values),\n                             sharey=False, figsize=figsize, dpi=dpi) \n    \n    # Calculate color scale adapted to grid resolution\n    for i, (label, values) in enumerate(tax_pretty.items()):\n        shap = list(filter(lambda x: x['order'] == label, attr_values))[0]['shap']\n        if not diverging:\n            shap = np.absolute(shap)\n        axes[i].set_xlim(np.max(X_names), np.min(X_names))\n        title = prettify_label(label, tax_pretty_lut) \n\n        if diverging:\n            mask_pos = shap > 0\n            attr_pos = np.copy(shap)\n            attr_neg = np.copy(shap)\n            attr_pos[~mask_pos] = 0   \n            attr_neg[mask_pos] = 0   \n            axes[i].bar(X_names, attr_pos, width=3, color='#0571b0', label='GradientShap > 0')\n            axes[i].bar(X_names, attr_neg, width=3, color='#ca0020', label='GradientShap < 0')\n        else:\n            axes[i].bar(X_names, shap, width=2, color='black')\n        \n        axes[i].yaxis.set_major_formatter(FormatStrFormatter('%.2f')) \n        axes[i].set_ylabel(f'{title}')\n        axes[i].get_yaxis().set_ticks([])\n\n        ax_twin = axes[i].twinx()\n        X_mean = list(filter(lambda x: x['order'] == label, X))[0]['X']\n        ax_twin.plot(X_names, X_mean, c='#555', \n                     alpha=1, lw=0.5, ls='--', zorder=-1, \n                     label='Mean spectrum (Absorbance)') \n        ax_twin.get_yaxis().set_ticks([])\n\n        axes[i].xaxis.set_major_locator(ticker.MaxNLocator(20))\n        axes[i].xaxis.set_minor_locator(ticker.MaxNLocator(80))\n\n    handles_all = []\n    labels_all = []\n    for ax in [axes[0], ax_twin]:\n        handles, labs = ax.get_legend_handles_labels()\n        labels_all += labs\n        handles_all += handles\n    \n    fig.legend(handles_all, labels_all, \n               frameon=False, ncol=5, loc='upper center',  borderaxespad=0.1) \n   \n   # Ornaments\n    axes.flat[-1].set_xlabel('Wavenumber ($cm^{-1}$) →', loc='right')\n    plt.tight_layout()\n\n\n#FIG_PATH = Path('nameofyourfolder')\nFIG_PATH = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/img')\nfname = 'gradshap-order-02092022.png'\n\nset_style(DEFAULT_STYLE)\nplot_shaps_by_orders(shap_by_order, X_mean_by_order, X_names, tax_pretty_lut,\n                     figsize=(16*centimeter, 20*centimeter), diverging=True)\n\n\nplt.savefig(FIG_PATH/fname, dpi=600, transparent=True, format='png')"
  },
  {
    "objectID": "paper/interpretation_gradshap_corr.html#load-and-transform",
    "href": "paper/interpretation_gradshap_corr.html#load-and-transform",
    "title": "6.2. GradientShap values correlation",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = 'data'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)"
  },
  {
    "objectID": "paper/interpretation_gradshap_corr.html#setup",
    "href": "paper/interpretation_gradshap_corr.html#setup",
    "title": "6.2. GradientShap values correlation",
    "section": "Setup",
    "text": "Setup\nThe required GradShap values are computed (then saved) as shown in GradientShap values notebook. Here we will simply load them.\n\nsrc_dir = Path('dumps/cnn/shaps')\nshap_by_orders, _  = pickle.load(open(src_dir/'shap_by_orders_02_09_2022.pickle', \"rb\"))\n\nWe compute the Pearson correlation coefficient between the average GradientShap values of Soil Taxonomy Orders.\n\nshap_by_orders\n\n[OrderedDict([('shap',\n               array([ 1.8393685e-06, -3.4975756e-07,  1.0674637e-07, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'alfisols')]),\n OrderedDict([('shap',\n               array([-5.8400028e-06, -1.0966338e-05, -1.0213902e-06, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'all')]),\n OrderedDict([('shap',\n               array([ 2.3224013e-05,  1.5094614e-04, -2.2070333e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'andisols')]),\n OrderedDict([('shap',\n               array([2.0230336e-05, 9.7760840e-06, 9.9760737e-06, ..., 0.0000000e+00,\n                      0.0000000e+00, 0.0000000e+00], dtype=float32)),\n              ('order', 'aridisols')]),\n OrderedDict([('shap',\n               array([-7.0780785e-05, -1.4203938e-04,  1.2610339e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'entisols')]),\n OrderedDict([('shap',\n               array([ 4.5924044e-06,  4.4373726e-05, -1.3388793e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'gelisols')]),\n OrderedDict([('shap',\n               array([ 1.8504910e-05,  4.4637756e-05, -1.5806931e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'histosols')]),\n OrderedDict([('shap',\n               array([ 2.0495969e-05,  3.4390621e-05, -4.1421335e-06, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'inceptisols')]),\n OrderedDict([('shap',\n               array([1.8545827e-05, 1.5292404e-05, 3.3629021e-06, ..., 0.0000000e+00,\n                      0.0000000e+00, 0.0000000e+00], dtype=float32)),\n              ('order', 'mollisols')]),\n OrderedDict([('shap',\n               array([ 9.6384401e-04,  2.6268021e-03, -3.9017643e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'oxisols')]),\n OrderedDict([('shap',\n               array([-4.8168564e-05, -2.9730183e-05, -3.7888144e-06, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'spodosols')]),\n OrderedDict([('shap',\n               array([ 9.3750474e-05,  1.1947515e-04, -1.9311596e-05, ...,\n                       0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32)),\n              ('order', 'ultisols')]),\n OrderedDict([('shap',\n               array([-6.108464e-06, -9.995457e-06,  8.304874e-07, ...,  0.000000e+00,\n                       0.000000e+00,  0.000000e+00], dtype=float32)),\n              ('order', 'undefined')]),\n OrderedDict([('shap',\n               array([1.7481134e-05, 2.0790496e-05, 2.8002933e-07, ..., 0.0000000e+00,\n                      0.0000000e+00, 0.0000000e+00], dtype=float32)),\n              ('order', 'vertisols')])]\n\n\n\ntax_order_sorted = ['all', 'undefined', 'mollisols', 'alfisols', 'inceptisols', 'ultisols', \n                    'entisols', 'aridisols', 'andisols','vertisols', 'histosols',\n                    'spodosols', 'gelisols', 'oxisols']\n\n\nshap_by_orders_sorted = []\nfor tax in tax_order_sorted:\n    shap_by_orders_sorted.append(list(filter(lambda x: x['order'] == tax, shap_by_orders))[0])\n\n\n# Correlation\nshaps_corr = np.corrcoef(np.array([s['shap'] for s in shap_by_orders_sorted]))\ncorr_labels = [s['order'].capitalize() for s in shap_by_orders_sorted]"
  },
  {
    "objectID": "paper/interpretation_gradshap_corr.html#plot",
    "href": "paper/interpretation_gradshap_corr.html#plot",
    "title": "6.2. GradientShap values correlation",
    "section": "Plot",
    "text": "Plot\n\ndef plot_shaps_corr(shaps, labels, figsize=(6*centimeter, 6*centimeter), dpi=600):\n\n    # Styles\n    p = plt.rcParams\n    p[\"axes.spines.bottom\"] = False\n    p[\"axes.grid\"] = False\n    p[\"xtick.major.size\"] = 3\n    p[\"xtick.major.width\"] = 0.5  \n    p[\"xtick.minor.size\"] = 1\n    p[\"xtick.minor.width\"] = 0.25\n    p[\"xtick.minor.visible\"] = False\n    p[\"xtick.labelsize\"] = 4\n    p[\"ytick.labelsize\"] = 4\n    p[\"ytick.labelleft\"] = True\n    p[\"ytick.direction\"] = \"out\"\n    p[\"ytick.major.size\"] = 3\n    p[\"ytick.major.width\"] = 0.5\n    p[\"ytick.minor.size\"] = 1\n    p[\"ytick.minor.width\"] = 0.25\n    p[\"ytick.minor.visible\"] = False\n    \n    # Layout \n    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=figsize, dpi=dpi) \n    \n    norm = mcolors.TwoSlopeNorm(vmin=-1, vcenter=0., vmax=1)\n    props = dict(cmap='RdBu_r', norm=norm, interpolation=None)\n    \n    shaps = np.tril(shaps, k=0)\n    shaps[shaps == 0.0] = np.nan \n\n    im = ax.imshow(shaps, **props)\n    ax.set_xticks(np.arange(len(labels)), labels=labels)\n    ax.set_yticks(np.arange(len(labels)), labels=labels)\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\") \n   \n    for i in range(len(shaps)):\n        for j in range(len(shaps)):\n            if np.isnan(shaps[i, j]):\n                pass\n            else:\n                color = 'w' if np.abs(shaps[i, j]) > 0.3 else '#333'\n                text = ax.text(j, i, \"{:.2f}\".format(shaps[i, j]),\n                               ha=\"center\", va=\"center\", color=color, size=3)\n            \n    cax = fig.add_axes([0.5, 0.9, 0.3, 0.02])\n    clb = plt.colorbar(im, cax=cax, orientation='horizontal')\n    clb.ax.xaxis.set_ticks_position('bottom')\n    clb.ax.tick_params(labelsize=4) \n    clb.outline.set_visible(False)\n    clb.ax.set_title('Correlation Coefficient', size=4)\n\n    plt.tight_layout()\n\n\n#FIG_PATH = Path('nameofyourfolder')\nFIG_PATH = Path('images/')\nset_style(DEFAULT_STYLE)\nplot_shaps_corr(shaps_corr, corr_labels)\n\n# To save/export it\nplt.savefig(FIG_PATH/'gradshap-order-corr.png', dpi=600, transparent=True, format='png')"
  },
  {
    "objectID": "paper/cnn_learning_curve.html#load-and-transform",
    "href": "paper/cnn_learning_curve.html#load-and-transform",
    "title": "4.2. Learning curve (CNN)",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (40132, 1764)\ny shape: (40132,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[43.  2.]\n [ 0.  0.]\n [ 0.  1.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}"
  },
  {
    "objectID": "paper/cnn_learning_curve.html#experiment",
    "href": "paper/cnn_learning_curve.html#experiment",
    "title": "4.2. Learning curve (CNN)",
    "section": "Experiment",
    "text": "Experiment\n\nSetup\n\n# Is a GPU available?\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(f'Runtime is: {device}')\n\nn_epochs = 151\nstep_size_up = 5\ncriterion = MSELoss() # Mean Squared Error loss\nbase_lr, max_lr = 3e-5, 1e-3 # Based on learning rate finder\ndelta = 1e-3 # Loss difference threshold for early stopping\n\ndest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/learning_curve')\n\nRuntime is: cuda:0\n\n\n\ntraining_size = [500, 1000, 2000, 5000, 10000, 20000, 30000, X.shape[0]]\nsplit_ratio = 0.1\nseeds = range(20)\n\n\nfor seed in tqdm(seeds):\n    perfs_by_size = OrderedDict({'seed': [], 'n_samples': [], 'test_score': [], 'n_epochs': []})\n    \n    for size in training_size:\n        print(80*'-')\n        print(f'Seed: {seed} | Size: {size}')\n        print(80*'-')\n        idx = np.random.choice(len(X), size, replace=False)\n        \n        # Train/test split\n        data = train_test_split(X[idx, :], \n                                y[idx], \n                                depth_order[idx,1], \n                                test_size=split_ratio,\n                                random_state=seed)\n        X_train, X_test, y_train, y_test, tax_order_train, tax_order_test = data\n        data_test = X_test, y_test, tax_order_test\n\n        # Further train/valid split\n        data = train_test_split(X_train, \n                                y_train,\n                                tax_order_train,\n                                test_size=split_ratio, \n                                random_state=seed)\n        X_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = data\n        data_train = X_train, y_train, tax_order_train\n        data_valid = X_valid, y_valid, tax_order_valid\n        \n        dls = DataLoaders(data_train, \n                          data_valid,\n                          data_test,\n                          transform=SNV_transform(),\n                          batch_size=32)\n\n        training_generator, validation_generator, test_generator = dls.loaders()\n        \n        # Modeling\n        model = Model(X.shape[1], out_channel=16).to(device)\n        opt = Adam(model.parameters(), lr=1e-4)\n        model = model.apply(weights_init)\n        scheduler = CyclicLR(opt, base_lr=base_lr, max_lr=max_lr,\n                             step_size_up=step_size_up, mode='triangular',\n                             cycle_momentum=False)\n\n        early_stopper = partial(is_plateau, delta=delta, verbose=False)\n\n        learner = Learner(model, criterion, opt, n_epochs=n_epochs, \n                          scheduler=scheduler, early_stopper=early_stopper,\n                          tax_lookup=tax_lookup.values(), verbose=True)\n        model, losses = learner.fit(training_generator, validation_generator)\n\n        y_hat, y_true = learner.predict(test_generator)\n        perfs = eval_reg(y_true, y_hat)\n\n        perfs_by_size['seed'].append(seed)\n        perfs_by_size['n_samples'].append(size)\n        perfs_by_size['n_epochs'].append(len(losses['train']))\n        perfs_by_size['test_score'].append(perfs['r2'])\n\n    with open(dest_dir/f'cnn-lc-seed-{seed}.pickle', 'wb') as f: \n        pickle.dump(perfs_by_size, f)\n\n\n\n\nStreaming output truncated to the last 5000 lines.\n------------------------------\nEpoch: 6\nTraining loss: 0.06806543636222093 | Validation loss: 0.0665476806461811\nValidation loss (ends of cycles): [0.29326259]\n------------------------------\nEpoch: 7\nTraining loss: 0.06351911573793878 | Validation loss: 0.06200351657574637\nValidation loss (ends of cycles): [0.29326259]\n------------------------------\nEpoch: 8\nTraining loss: 0.05975192000159968 | Validation loss: 0.05620013448622143\nValidation loss (ends of cycles): [0.29326259]\n------------------------------\nEpoch: 9\nTraining loss: 0.05616931922190053 | Validation loss: 0.053450687412630045\nValidation loss (ends of cycles): [0.29326259]\n------------------------------\nEpoch: 10\nTraining loss: 0.05372029852261675 | Validation loss: 0.05114826099260857\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 11\nTraining loss: 0.054583898482796475 | Validation loss: 0.05386886536552195\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 12\nTraining loss: 0.05613075314673799 | Validation loss: 0.052798241344198846\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 13\nTraining loss: 0.0567820893501389 | Validation loss: 0.05200839581850328\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 14\nTraining loss: 0.057392622577485716 | Validation loss: 0.05300498377989259\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 15\nTraining loss: 0.05863403260737246 | Validation loss: 0.05642465720966197\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 16\nTraining loss: 0.05551970786097252 | Validation loss: 0.05141487086943367\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 17\nTraining loss: 0.05263045195791378 | Validation loss: 0.04939890414345683\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 18\nTraining loss: 0.049820137990884764 | Validation loss: 0.049233734019492804\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 19\nTraining loss: 0.04752113914422147 | Validation loss: 0.047601242239276566\nValidation loss (ends of cycles): [0.29326259 0.05114826]\n------------------------------\nEpoch: 20\nTraining loss: 0.04555318841097153 | Validation loss: 0.044789811596274376\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 21\nTraining loss: 0.04647652471265379 | Validation loss: 0.04712833372647302\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 22\nTraining loss: 0.0479339515235591 | Validation loss: 0.04763588224325264\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 23\nTraining loss: 0.048971499608465904 | Validation loss: 0.049810390854090975\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 24\nTraining loss: 0.05068343192618156 | Validation loss: 0.04795939717115017\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 25\nTraining loss: 0.05176023933558892 | Validation loss: 0.04928604261786269\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 26\nTraining loss: 0.04998573636603073 | Validation loss: 0.05063137840152832\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 27\nTraining loss: 0.04781438338274551 | Validation loss: 0.049298560344859174\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 28\nTraining loss: 0.044998955462840655 | Validation loss: 0.047076730155631116\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 29\nTraining loss: 0.04269748058948938 | Validation loss: 0.043681511099924124\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n------------------------------\nEpoch: 30\nTraining loss: 0.04123661962486583 | Validation loss: 0.04148694225832036\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 31\nTraining loss: 0.04170608106831592 | Validation loss: 0.04531938837547051\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 32\nTraining loss: 0.04312830294171969 | Validation loss: 0.046768018224260265\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 33\nTraining loss: 0.04452355271407368 | Validation loss: 0.051704760902283486\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 34\nTraining loss: 0.045750220440313426 | Validation loss: 0.04653846119579516\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 35\nTraining loss: 0.04793604665217256 | Validation loss: 0.04717006236968333\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 36\nTraining loss: 0.04609691420353258 | Validation loss: 0.05028725784729447\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 37\nTraining loss: 0.04414579540094327 | Validation loss: 0.046487056699238326\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 38\nTraining loss: 0.04129064368374308 | Validation loss: 0.04598317131922956\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 39\nTraining loss: 0.03918340381903526 | Validation loss: 0.046684552441563526\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n------------------------------\nEpoch: 40\nTraining loss: 0.03772568548212096 | Validation loss: 0.03923895272115866\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 41\nTraining loss: 0.038246894685121685 | Validation loss: 0.04450457794755174\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 42\nTraining loss: 0.03958957887546727 | Validation loss: 0.04442280089776767\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 43\nTraining loss: 0.04075290252044944 | Validation loss: 0.0414913749616397\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 44\nTraining loss: 0.04319122280095221 | Validation loss: 0.04472611011251023\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 45\nTraining loss: 0.044247216745270546 | Validation loss: 0.04369983495327465\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 46\nTraining loss: 0.04255689712355182 | Validation loss: 0.05006042926719314\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 47\nTraining loss: 0.04059595685577134 | Validation loss: 0.04566892837746102\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 48\nTraining loss: 0.03882334366095431 | Validation loss: 0.04446525734506155\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 49\nTraining loss: 0.036582596561480205 | Validation loss: 0.04001459044714769\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n------------------------------\nEpoch: 50\nTraining loss: 0.03509080934814036 | Validation loss: 0.0384905424884014\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 51\nTraining loss: 0.03569372744653999 | Validation loss: 0.039927559855737184\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 52\nTraining loss: 0.037237670164209966 | Validation loss: 0.04519388896592876\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 53\nTraining loss: 0.03838471407750419 | Validation loss: 0.0458477276066939\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 54\nTraining loss: 0.03979467994788812 | Validation loss: 0.04402068803054199\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 55\nTraining loss: 0.041505508775100904 | Validation loss: 0.04638113081455231\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 56\nTraining loss: 0.04012122852048108 | Validation loss: 0.042935863954194804\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 57\nTraining loss: 0.03744441115240432 | Validation loss: 0.04187299245805071\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 58\nTraining loss: 0.03562752796884413 | Validation loss: 0.04327832126434435\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 59\nTraining loss: 0.033714442978625934 | Validation loss: 0.03990272723399756\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n------------------------------\nEpoch: 60\nTraining loss: 0.03269251721821123 | Validation loss: 0.037310676289755\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 61\nTraining loss: 0.03308601810962432 | Validation loss: 0.03998835182242226\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 62\nTraining loss: 0.034110041697099955 | Validation loss: 0.043654320042645724\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 63\nTraining loss: 0.03643204273630293 | Validation loss: 0.042565111332295236\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 64\nTraining loss: 0.037886392731124006 | Validation loss: 0.04307818798380986\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 65\nTraining loss: 0.039455032082645614 | Validation loss: 0.050652103890713895\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 66\nTraining loss: 0.03784991302610149 | Validation loss: 0.05323615209444573\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 67\nTraining loss: 0.03613430108963209 | Validation loss: 0.0495827330374404\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 68\nTraining loss: 0.03396667248185511 | Validation loss: 0.05136706980696896\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 69\nTraining loss: 0.03226531962671223 | Validation loss: 0.03999559313320277\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068]\n------------------------------\nEpoch: 70\nTraining loss: 0.03079099055284109 | Validation loss: 0.03639809499707138\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 71\nTraining loss: 0.03151882326329601 | Validation loss: 0.037147419167715204\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 72\nTraining loss: 0.032178009986965614 | Validation loss: 0.04826249839051774\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 73\nTraining loss: 0.03405666328724143 | Validation loss: 0.04936403996850315\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 74\nTraining loss: 0.03541494072718028 | Validation loss: 0.04829113718056888\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 75\nTraining loss: 0.03756211765103145 | Validation loss: 0.04617483740705147\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 76\nTraining loss: 0.03559289298973669 | Validation loss: 0.04658271506298007\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 77\nTraining loss: 0.03390944113907141 | Validation loss: 0.04650598888595899\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 78\nTraining loss: 0.03154865592806297 | Validation loss: 0.043829943690645065\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 79\nTraining loss: 0.03073635195699728 | Validation loss: 0.03725755015355453\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809]\n------------------------------\nEpoch: 80\nTraining loss: 0.0294124557080089 | Validation loss: 0.03590158830609238\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 81\nTraining loss: 0.02953582158756738 | Validation loss: 0.037577209700095024\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 82\nTraining loss: 0.030502555425291702 | Validation loss: 0.044688741823560314\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 83\nTraining loss: 0.03210949529278325 | Validation loss: 0.05308352276813565\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 84\nTraining loss: 0.03396462016468923 | Validation loss: 0.04562949912067045\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 85\nTraining loss: 0.03592904591179752 | Validation loss: 0.045200854474515245\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 86\nTraining loss: 0.03457508122463962 | Validation loss: 0.050139864481854854\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 87\nTraining loss: 0.03230217536562352 | Validation loss: 0.04341460270970537\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 88\nTraining loss: 0.03038559539548509 | Validation loss: 0.04495927831974991\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 89\nTraining loss: 0.02869941036104862 | Validation loss: 0.03822105756977148\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159]\n------------------------------\nEpoch: 90\nTraining loss: 0.02780957057285885 | Validation loss: 0.03527055269009188\nValidation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n 0.03731068 0.03639809 0.03590159 0.03527055]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 30000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.147111305290539 | Validation loss: 0.1315017819404602\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 1\nTraining loss: 0.09975460252577537 | Validation loss: 0.0852299884399947\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 2\nTraining loss: 0.08679056662674013 | Validation loss: 0.08812916081617861\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 3\nTraining loss: 0.08041144743267643 | Validation loss: 0.0723718693589463\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 4\nTraining loss: 0.07665490742870852 | Validation loss: 0.06908508214880438\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 5\nTraining loss: 0.07295063323782462 | Validation loss: 0.064691921192057\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 6\nTraining loss: 0.06718138600944688 | Validation loss: 0.058289176620104736\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 7\nTraining loss: 0.06265752670159074 | Validation loss: 0.05494928048814044\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 8\nTraining loss: 0.05877749496139586 | Validation loss: 0.05109663200290764\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 9\nTraining loss: 0.05578988529496679 | Validation loss: 0.04918082448489526\nValidation loss (ends of cycles): [0.13150178]\n------------------------------\nEpoch: 10\nTraining loss: 0.05317496091960684 | Validation loss: 0.04732894088853808\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 11\nTraining loss: 0.05387537818843205 | Validation loss: 0.04777663764269913\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 12\nTraining loss: 0.05503765009588709 | Validation loss: 0.049427153915166853\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 13\nTraining loss: 0.05634034368160524 | Validation loss: 0.048755222164532715\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 14\nTraining loss: 0.0574380914239507 | Validation loss: 0.05096663817325059\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 15\nTraining loss: 0.0579996428844568 | Validation loss: 0.05888870698125923\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 16\nTraining loss: 0.05525187691183467 | Validation loss: 0.048332023379557276\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 17\nTraining loss: 0.05298996464913025 | Validation loss: 0.04795002014759709\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 18\nTraining loss: 0.050095231600693964 | Validation loss: 0.04421709517345709\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 19\nTraining loss: 0.04788641859042017 | Validation loss: 0.04255331138915876\nValidation loss (ends of cycles): [0.13150178 0.04732894]\n------------------------------\nEpoch: 20\nTraining loss: 0.04585613830162114 | Validation loss: 0.0418057950980523\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.046981786802950266 | Validation loss: 0.042653957995421744\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.04777146606019845 | Validation loss: 0.044101270704584965\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.04940902760350391 | Validation loss: 0.04385254315155394\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.05038880116649364 | Validation loss: 0.04633155581267441\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.05155313427461997 | Validation loss: 0.046872320508255676\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.0495440785699573 | Validation loss: 0.045494657645330706\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.047308244940971855 | Validation loss: 0.04535501838168677\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.04507004849147052 | Validation loss: 0.04133476996246506\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.042914133947832805 | Validation loss: 0.04083044673590099\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.040776250611892655 | Validation loss: 0.03877844817059881\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 31\nTraining loss: 0.04152484470733294 | Validation loss: 0.039483094083912235\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 32\nTraining loss: 0.04290025286670578 | Validation loss: 0.040904018326717265\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 33\nTraining loss: 0.044265624253373395 | Validation loss: 0.04122531755882151\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 34\nTraining loss: 0.046046443509035986 | Validation loss: 0.04211513080141124\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 35\nTraining loss: 0.047449978132193024 | Validation loss: 0.04790202175431392\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 36\nTraining loss: 0.04531032362903811 | Validation loss: 0.046201490764232245\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 37\nTraining loss: 0.04320118561950757 | Validation loss: 0.040469889671486965\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 38\nTraining loss: 0.0409529635376346 | Validation loss: 0.039225977001821295\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 39\nTraining loss: 0.039055508706032444 | Validation loss: 0.03890094250878867\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n------------------------------\nEpoch: 40\nTraining loss: 0.03767110928525462 | Validation loss: 0.03655863406465334\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 41\nTraining loss: 0.03817510909621457 | Validation loss: 0.03774878807804164\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 42\nTraining loss: 0.0393792031259325 | Validation loss: 0.03817844322937376\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 43\nTraining loss: 0.04070846887648498 | Validation loss: 0.04050976456526448\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 44\nTraining loss: 0.0424697090467242 | Validation loss: 0.041637066970853245\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 45\nTraining loss: 0.043809773583014154 | Validation loss: 0.04080562661675846\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 46\nTraining loss: 0.04217543903811786 | Validation loss: 0.03963810448699138\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 47\nTraining loss: 0.040034622934303786 | Validation loss: 0.03918085889342953\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 48\nTraining loss: 0.03804198447492366 | Validation loss: 0.03835276941604474\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 49\nTraining loss: 0.036407624813728036 | Validation loss: 0.03730193979161627\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n------------------------------\nEpoch: 50\nTraining loss: 0.03499086681557329 | Validation loss: 0.035120617181939234\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 51\nTraining loss: 0.03556273958253625 | Validation loss: 0.03798426411607686\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 52\nTraining loss: 0.03691161809562656 | Validation loss: 0.037254741152419765\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 53\nTraining loss: 0.03808860194648763 | Validation loss: 0.03941758612499518\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 54\nTraining loss: 0.03961590641227208 | Validation loss: 0.03795143950949697\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 55\nTraining loss: 0.041122380242143805 | Validation loss: 0.047902766505585\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 56\nTraining loss: 0.039530974404739315 | Validation loss: 0.03792385430458714\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 57\nTraining loss: 0.03773507730373622 | Validation loss: 0.037603619760450194\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 58\nTraining loss: 0.03599328672219264 | Validation loss: 0.037073982134461406\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 59\nTraining loss: 0.03398567610665371 | Validation loss: 0.03748219501884545\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n------------------------------\nEpoch: 60\nTraining loss: 0.03271165021670688 | Validation loss: 0.03435478813069708\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 61\nTraining loss: 0.03338361110685295 | Validation loss: 0.036062022934065144\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 62\nTraining loss: 0.034301962170406784 | Validation loss: 0.037316511702888154\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 63\nTraining loss: 0.03564163815401691 | Validation loss: 0.0389238071792266\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 64\nTraining loss: 0.0370434378567887 | Validation loss: 0.04136042470002876\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 65\nTraining loss: 0.03914995556819792 | Validation loss: 0.04118646508192315\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 66\nTraining loss: 0.03734212646040281 | Validation loss: 0.039161085907150714\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 67\nTraining loss: 0.03560297669058567 | Validation loss: 0.0376554255538127\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 68\nTraining loss: 0.03367881167091821 | Validation loss: 0.03781347434748621\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 69\nTraining loss: 0.03207173739608966 | Validation loss: 0.03692457919173381\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479]\n------------------------------\nEpoch: 70\nTraining loss: 0.03081994304237397 | Validation loss: 0.0337879949632813\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 71\nTraining loss: 0.03131349330498396 | Validation loss: 0.03656278537476764\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 72\nTraining loss: 0.032406741111098154 | Validation loss: 0.036839769627241524\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 73\nTraining loss: 0.03356948233773246 | Validation loss: 0.04138847341870561\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 74\nTraining loss: 0.03554210637352968 | Validation loss: 0.03746587980319472\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 75\nTraining loss: 0.03756678962197743 | Validation loss: 0.04222078783547177\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 76\nTraining loss: 0.03534613802269297 | Validation loss: 0.03830132688231328\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 77\nTraining loss: 0.03376366636887389 | Validation loss: 0.04012722719241591\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 78\nTraining loss: 0.03233612979468154 | Validation loss: 0.037108558964203384\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 79\nTraining loss: 0.03046056817800395 | Validation loss: 0.036080013193628364\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799]\n------------------------------\nEpoch: 80\nTraining loss: 0.029702923919907524 | Validation loss: 0.03293276488342706\nValidation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n 0.03435479 0.03378799 0.03293276]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 9 | Size: 40132\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.26767727328596386 | Validation loss: 0.20993191188415594\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 1\nTraining loss: 0.11062715910595115 | Validation loss: 0.09041815471992029\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 2\nTraining loss: 0.08798102279171699 | Validation loss: 0.083032216448172\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 3\nTraining loss: 0.0831462372530721 | Validation loss: 0.08245944924059168\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 4\nTraining loss: 0.0791002170138765 | Validation loss: 0.07641426572757484\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 5\nTraining loss: 0.07637509745985681 | Validation loss: 0.07529779728007528\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 6\nTraining loss: 0.07119164006293231 | Validation loss: 0.074695995984088\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 7\nTraining loss: 0.06693422179815807 | Validation loss: 0.06877124391957722\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 8\nTraining loss: 0.06382161711423298 | Validation loss: 0.06549916515308143\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 9\nTraining loss: 0.06060796096099643 | Validation loss: 0.06079392095582675\nValidation loss (ends of cycles): [0.20993191]\n------------------------------\nEpoch: 10\nTraining loss: 0.05856810573774471 | Validation loss: 0.057926995514900284\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 11\nTraining loss: 0.05894209550121638 | Validation loss: 0.06261971423122208\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 12\nTraining loss: 0.059919283873819576 | Validation loss: 0.06526916124651917\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 13\nTraining loss: 0.060730206374973644 | Validation loss: 0.06995311323388488\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 14\nTraining loss: 0.06151795924428528 | Validation loss: 0.07613310333242458\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 15\nTraining loss: 0.061776850015057 | Validation loss: 0.07298920982706864\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 16\nTraining loss: 0.05926943988920608 | Validation loss: 0.06051817781959487\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 17\nTraining loss: 0.0569572605478658 | Validation loss: 0.06914879089897186\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 18\nTraining loss: 0.05450942960391775 | Validation loss: 0.06324264410454615\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 19\nTraining loss: 0.052381872527638644 | Validation loss: 0.0565843051052199\nValidation loss (ends of cycles): [0.20993191 0.057927  ]\n------------------------------\nEpoch: 20\nTraining loss: 0.050279307501815904 | Validation loss: 0.05087755673227057\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 21\nTraining loss: 0.05125819389913671 | Validation loss: 0.055446823069875216\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 22\nTraining loss: 0.05232975868449554 | Validation loss: 0.06173066887180362\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 23\nTraining loss: 0.05332614857828113 | Validation loss: 0.07285340931431382\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 24\nTraining loss: 0.05472646032526033 | Validation loss: 0.07538743981415719\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 25\nTraining loss: 0.05555287960506096 | Validation loss: 0.06203558999814291\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 26\nTraining loss: 0.05368010373602761 | Validation loss: 0.06663409501841638\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 27\nTraining loss: 0.05148195093277636 | Validation loss: 0.06459290692500308\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 28\nTraining loss: 0.049331870113360134 | Validation loss: 0.05822600282530869\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 29\nTraining loss: 0.04742172895197382 | Validation loss: 0.05136353392318814\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n------------------------------\nEpoch: 30\nTraining loss: 0.04593015410094045 | Validation loss: 0.04718089631173463\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 31\nTraining loss: 0.04645789550767669 | Validation loss: 0.05242063497415686\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 32\nTraining loss: 0.04760350834387611 | Validation loss: 0.0537388277099987\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 33\nTraining loss: 0.04892369482930251 | Validation loss: 0.05456192056294036\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 34\nTraining loss: 0.05001625064833779 | Validation loss: 0.053328140350305926\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 35\nTraining loss: 0.05126057072417942 | Validation loss: 0.05397078264669507\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 36\nTraining loss: 0.04982032163039319 | Validation loss: 0.05863784631071365\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 37\nTraining loss: 0.04786631208661152 | Validation loss: 0.06205748732045161\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 38\nTraining loss: 0.045445062373178156 | Validation loss: 0.05508627135932973\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 39\nTraining loss: 0.04398632775368829 | Validation loss: 0.05074058398934065\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n------------------------------\nEpoch: 40\nTraining loss: 0.04237788485722455 | Validation loss: 0.04485161622277403\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 41\nTraining loss: 0.04334347072886083 | Validation loss: 0.048917631247797896\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 42\nTraining loss: 0.04402575698153182 | Validation loss: 0.05648849759481649\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 43\nTraining loss: 0.0455412358104244 | Validation loss: 0.05538065110094252\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 44\nTraining loss: 0.04707928725488953 | Validation loss: 0.058348986625143914\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 45\nTraining loss: 0.04844906317495455 | Validation loss: 0.052721294714550004\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 46\nTraining loss: 0.04671046115993339 | Validation loss: 0.06256921447615708\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 47\nTraining loss: 0.044951911013771405 | Validation loss: 0.06044869774342638\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 48\nTraining loss: 0.04295778853404446 | Validation loss: 0.05060311258498546\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 49\nTraining loss: 0.041293553852175514 | Validation loss: 0.04906327166981929\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n------------------------------\nEpoch: 50\nTraining loss: 0.03999335623683599 | Validation loss: 0.043351529924347335\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 51\nTraining loss: 0.04056466871131886 | Validation loss: 0.049954215410800105\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 52\nTraining loss: 0.04156503059505302 | Validation loss: 0.0522121147343279\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 53\nTraining loss: 0.04284362089416877 | Validation loss: 0.05801308550665864\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 54\nTraining loss: 0.0443785713149572 | Validation loss: 0.05014750747158464\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 55\nTraining loss: 0.04580919714882882 | Validation loss: 0.06235515744944589\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 56\nTraining loss: 0.044337539317629 | Validation loss: 0.05064553986674389\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 57\nTraining loss: 0.04247408428625387 | Validation loss: 0.05216136716504013\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 58\nTraining loss: 0.040845778049336465 | Validation loss: 0.044927841662305644\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 59\nTraining loss: 0.03896903427222406 | Validation loss: 0.0456106700167983\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n------------------------------\nEpoch: 60\nTraining loss: 0.03817753170022932 | Validation loss: 0.041548734382454273\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 61\nTraining loss: 0.0385868555479353 | Validation loss: 0.0453451023626644\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 62\nTraining loss: 0.03971990962663123 | Validation loss: 0.05200313806401945\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 63\nTraining loss: 0.04082011654753032 | Validation loss: 0.05165595175550047\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 64\nTraining loss: 0.04232948364260951 | Validation loss: 0.04828151575891317\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 65\nTraining loss: 0.04398110482519067 | Validation loss: 0.0522270905331964\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 66\nTraining loss: 0.04246964514636853 | Validation loss: 0.04821099462894212\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 67\nTraining loss: 0.04050930707319456 | Validation loss: 0.05413374576750582\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 68\nTraining loss: 0.03861887257832183 | Validation loss: 0.045728032221704455\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 69\nTraining loss: 0.03748417487477986 | Validation loss: 0.04113612045426812\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873]\n------------------------------\nEpoch: 70\nTraining loss: 0.036482414228870996 | Validation loss: 0.03996460937556967\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 71\nTraining loss: 0.036949899838687336 | Validation loss: 0.04259959054467952\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 72\nTraining loss: 0.03779611252358286 | Validation loss: 0.05049621508316656\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 73\nTraining loss: 0.03913594540263578 | Validation loss: 0.046180599443284814\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 74\nTraining loss: 0.04059691341890858 | Validation loss: 0.05154817353571411\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 75\nTraining loss: 0.04208160265473517 | Validation loss: 0.05156806101445603\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 76\nTraining loss: 0.0406858466682941 | Validation loss: 0.05118079272519171\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 77\nTraining loss: 0.039154747262685086 | Validation loss: 0.04794440525625132\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 78\nTraining loss: 0.03741198778152466 | Validation loss: 0.045822479308838336\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 79\nTraining loss: 0.03600720611793231 | Validation loss: 0.041700294241309166\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461]\n------------------------------\nEpoch: 80\nTraining loss: 0.034843648902111224 | Validation loss: 0.03910587075273547\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 81\nTraining loss: 0.03538154784450674 | Validation loss: 0.041343334930396713\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 82\nTraining loss: 0.036538641037250776 | Validation loss: 0.04486508937799825\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 83\nTraining loss: 0.03746334522573908 | Validation loss: 0.04573550967054557\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 84\nTraining loss: 0.03906626951345426 | Validation loss: 0.04793903403050077\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 85\nTraining loss: 0.04067838922652643 | Validation loss: 0.04968288084244834\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 86\nTraining loss: 0.03920144755136603 | Validation loss: 0.04930177286465084\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 87\nTraining loss: 0.03762676272317621 | Validation loss: 0.05426667930673709\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 88\nTraining loss: 0.0362003557594845 | Validation loss: 0.045433557897278695\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 89\nTraining loss: 0.034699298914404604 | Validation loss: 0.041561927959586666\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587]\n------------------------------\nEpoch: 90\nTraining loss: 0.03369743091248592 | Validation loss: 0.038345360271303\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 91\nTraining loss: 0.034240884567046256 | Validation loss: 0.04051130935879408\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 92\nTraining loss: 0.03505190609088974 | Validation loss: 0.04553813698281229\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 93\nTraining loss: 0.036325072192249626 | Validation loss: 0.04458336504093841\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 94\nTraining loss: 0.038167144151727166 | Validation loss: 0.05034423378848397\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 95\nTraining loss: 0.03953559986795995 | Validation loss: 0.04722861444528124\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 96\nTraining loss: 0.038086684345539044 | Validation loss: 0.04692284245274763\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 97\nTraining loss: 0.036556421646180996 | Validation loss: 0.042711537298375526\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 98\nTraining loss: 0.035050427166224404 | Validation loss: 0.04218942583002876\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 99\nTraining loss: 0.033719378285198 | Validation loss: 0.04147126843773686\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536]\n------------------------------\nEpoch: 100\nTraining loss: 0.03284708765889041 | Validation loss: 0.03807422106640529\nValidation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n 0.04154873 0.03996461 0.03910587 0.03834536 0.03807422]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 500\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.28623229150588697 | Validation loss: 0.2876668721437454\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 1\nTraining loss: 0.2732484845014719 | Validation loss: 0.2794637233018875\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 2\nTraining loss: 0.24852608488156244 | Validation loss: 0.26532696187496185\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 3\nTraining loss: 0.21454166219784662 | Validation loss: 0.244778074324131\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 4\nTraining loss: 0.1760728319103901 | Validation loss: 0.227938711643219\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 5\nTraining loss: 0.13927876548125193 | Validation loss: 0.2178543582558632\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 6\nTraining loss: 0.1165735016648586 | Validation loss: 0.1460997760295868\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 7\nTraining loss: 0.1044723718212201 | Validation loss: 0.11919640749692917\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 8\nTraining loss: 0.09750367930302253 | Validation loss: 0.10581953823566437\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 9\nTraining loss: 0.09256000587573418 | Validation loss: 0.105472881346941\nValidation loss (ends of cycles): [0.28766687]\n------------------------------\nEpoch: 10\nTraining loss: 0.08769601182295726 | Validation loss: 0.10440703853964806\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 11\nTraining loss: 0.08512237668037415 | Validation loss: 0.10394519194960594\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 12\nTraining loss: 0.08663272571105224 | Validation loss: 0.10312925651669502\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 13\nTraining loss: 0.08722404648478214 | Validation loss: 0.10252366960048676\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 14\nTraining loss: 0.08525520849686402 | Validation loss: 0.16640019416809082\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 15\nTraining loss: 0.09381065976161224 | Validation loss: 0.12001441791653633\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 16\nTraining loss: 0.09383911123642555 | Validation loss: 0.13745518773794174\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 17\nTraining loss: 0.09079675748944283 | Validation loss: 0.11658180505037308\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 18\nTraining loss: 0.0856318806226437 | Validation loss: 0.10903191938996315\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 19\nTraining loss: 0.08159097685263707 | Validation loss: 0.10695850476622581\nValidation loss (ends of cycles): [0.28766687 0.10440704]\n------------------------------\nEpoch: 20\nTraining loss: 0.08112306835559699 | Validation loss: 0.10882006213068962\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 21\nTraining loss: 0.08342313222013988 | Validation loss: 0.11046990752220154\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 22\nTraining loss: 0.08023825918252651 | Validation loss: 0.10669990256428719\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 23\nTraining loss: 0.08050067350268364 | Validation loss: 0.11306899785995483\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 24\nTraining loss: 0.08110488578677177 | Validation loss: 0.10976016893982887\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 25\nTraining loss: 0.0816067812534479 | Validation loss: 0.10279102995991707\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 26\nTraining loss: 0.08068692168364158 | Validation loss: 0.12279709428548813\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 27\nTraining loss: 0.07764218289118546 | Validation loss: 0.10749772936105728\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 28\nTraining loss: 0.07489511055442002 | Validation loss: 0.10868991166353226\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 29\nTraining loss: 0.07361829338165429 | Validation loss: 0.10824761912226677\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n------------------------------\nEpoch: 30\nTraining loss: 0.07218144604792961 | Validation loss: 0.10803351551294327\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 31\nTraining loss: 0.07381307562956443 | Validation loss: 0.10806496813893318\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 32\nTraining loss: 0.0742212373476762 | Validation loss: 0.11222716048359871\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 33\nTraining loss: 0.07452364036670098 | Validation loss: 0.10684147849678993\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 34\nTraining loss: 0.07637788527286969 | Validation loss: 0.1159149706363678\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 35\nTraining loss: 0.0818782881475412 | Validation loss: 0.10239430144429207\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 36\nTraining loss: 0.07602285765684567 | Validation loss: 0.11685087531805038\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 37\nTraining loss: 0.07531523704528809 | Validation loss: 0.12537145614624023\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 38\nTraining loss: 0.07239608867810322 | Validation loss: 0.10199717059731483\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 39\nTraining loss: 0.06922513475784889 | Validation loss: 0.10814457386732101\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n------------------------------\nEpoch: 40\nTraining loss: 0.06899523219236961 | Validation loss: 0.1046244278550148\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 41\nTraining loss: 0.06658756675628516 | Validation loss: 0.1013033427298069\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 42\nTraining loss: 0.06902540790346953 | Validation loss: 0.10858236253261566\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 43\nTraining loss: 0.06836868163484794 | Validation loss: 0.1024441346526146\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 44\nTraining loss: 0.07284962471861106 | Validation loss: 0.10861896350979805\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 45\nTraining loss: 0.07337845202821952 | Validation loss: 0.11208184063434601\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 46\nTraining loss: 0.07648678697072543 | Validation loss: 0.10931962355971336\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 47\nTraining loss: 0.06974261884505932 | Validation loss: 0.13788466900587082\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 48\nTraining loss: 0.06619262351439549 | Validation loss: 0.11117371916770935\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 49\nTraining loss: 0.06400956414066829 | Validation loss: 0.10756627842783928\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n------------------------------\nEpoch: 50\nTraining loss: 0.06536509698400131 | Validation loss: 0.10626817867159843\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 51\nTraining loss: 0.06240379179899509 | Validation loss: 0.10504582896828651\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 52\nTraining loss: 0.0661077255812975 | Validation loss: 0.10221891477704048\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 53\nTraining loss: 0.06295406073331833 | Validation loss: 0.11193358525633812\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 54\nTraining loss: 0.06823232712653968 | Validation loss: 0.09892533719539642\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 55\nTraining loss: 0.06969980093149039 | Validation loss: 0.10674293711781502\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 56\nTraining loss: 0.07000684480254467 | Validation loss: 0.1276056095957756\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 57\nTraining loss: 0.07000288166678868 | Validation loss: 0.10634531080722809\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 58\nTraining loss: 0.06597055552097467 | Validation loss: 0.10050374269485474\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 59\nTraining loss: 0.06105187821846742 | Validation loss: 0.09869374707341194\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n------------------------------\nEpoch: 60\nTraining loss: 0.06096246236791977 | Validation loss: 0.10020382329821587\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 61\nTraining loss: 0.06011223420500755 | Validation loss: 0.10638590157032013\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 62\nTraining loss: 0.06001588931450477 | Validation loss: 0.10041772574186325\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 63\nTraining loss: 0.05770640648328341 | Validation loss: 0.10693498328328133\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 64\nTraining loss: 0.06082365203362245 | Validation loss: 0.10736185312271118\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 65\nTraining loss: 0.06504335684271959 | Validation loss: 0.10069963335990906\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 66\nTraining loss: 0.06329144451480645 | Validation loss: 0.12001533806324005\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 67\nTraining loss: 0.060021109879016876 | Validation loss: 0.13025841116905212\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 68\nTraining loss: 0.05991259618447377 | Validation loss: 0.1112692691385746\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 69\nTraining loss: 0.051523296878888056 | Validation loss: 0.10381150618195534\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382]\n------------------------------\nEpoch: 70\nTraining loss: 0.05670847801061777 | Validation loss: 0.10544414073228836\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 71\nTraining loss: 0.05006153193803934 | Validation loss: 0.1040196605026722\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 72\nTraining loss: 0.053257113752456814 | Validation loss: 0.10596621409058571\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 73\nTraining loss: 0.052096135914325714 | Validation loss: 0.10098161175847054\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 74\nTraining loss: 0.05899054528428958 | Validation loss: 0.15640872716903687\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 75\nTraining loss: 0.06283879165466015 | Validation loss: 0.11133874207735062\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 76\nTraining loss: 0.06475053956875435 | Validation loss: 0.1269955411553383\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 77\nTraining loss: 0.05856200307607651 | Validation loss: 0.1232197992503643\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 78\nTraining loss: 0.0522953188763215 | Validation loss: 0.10317911207675934\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 79\nTraining loss: 0.050779332335178666 | Validation loss: 0.10657905414700508\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414]\n------------------------------\nEpoch: 80\nTraining loss: 0.05063657720501606 | Validation loss: 0.10991430655121803\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 81\nTraining loss: 0.046154147443863064 | Validation loss: 0.12377838045358658\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 82\nTraining loss: 0.04866070572573405 | Validation loss: 0.11358434334397316\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 83\nTraining loss: 0.04941498688780344 | Validation loss: 0.1076432652771473\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 84\nTraining loss: 0.05149089373075045 | Validation loss: 0.1143205277621746\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 85\nTraining loss: 0.054084719373629644 | Validation loss: 0.1442350260913372\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 86\nTraining loss: 0.0523872788135822 | Validation loss: 0.14174965769052505\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 87\nTraining loss: 0.049203543995435424 | Validation loss: 0.10664897784590721\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 88\nTraining loss: 0.04649185045407368 | Validation loss: 0.11035742238163948\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 89\nTraining loss: 0.04299872726775133 | Validation loss: 0.11943933367729187\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431]\n------------------------------\nEpoch: 90\nTraining loss: 0.04394056762640293 | Validation loss: 0.11714010313153267\nValidation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n 0.10020382 0.10544414 0.10991431 0.1171401 ]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 1000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.27114631694096786 | Validation loss: 0.3013191173473994\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 1\nTraining loss: 0.251861857680174 | Validation loss: 0.28458447754383087\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 2\nTraining loss: 0.21546687586949423 | Validation loss: 0.284496545791626\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 3\nTraining loss: 0.1699894041969226 | Validation loss: 0.21031304200490317\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 4\nTraining loss: 0.13458524501094452 | Validation loss: 0.08553920437892278\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 5\nTraining loss: 0.11567006145532314 | Validation loss: 0.14232793947060904\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 6\nTraining loss: 0.10996172233269765 | Validation loss: 0.10632145653168361\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 7\nTraining loss: 0.1118621499492572 | Validation loss: 0.08670407781998317\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 8\nTraining loss: 0.10521372522299106 | Validation loss: 0.09393906593322754\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 9\nTraining loss: 0.10288918677430886 | Validation loss: 0.08820493270953496\nValidation loss (ends of cycles): [0.30131912]\n------------------------------\nEpoch: 10\nTraining loss: 0.10211293800519063 | Validation loss: 0.088161401450634\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 11\nTraining loss: 0.10041159792588307 | Validation loss: 0.08792162934939067\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 12\nTraining loss: 0.10152667130415256 | Validation loss: 0.08688186854124069\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 13\nTraining loss: 0.10256221446280296 | Validation loss: 0.10419053584337234\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 14\nTraining loss: 0.1035890977543134 | Validation loss: 0.0830913856625557\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 15\nTraining loss: 0.10162635978597861 | Validation loss: 0.10780499627192815\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 16\nTraining loss: 0.10166545398533344 | Validation loss: 0.09875310709079106\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 17\nTraining loss: 0.10068163562279481 | Validation loss: 0.11237562447786331\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 18\nTraining loss: 0.09774628596810195 | Validation loss: 0.08589743822813034\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 19\nTraining loss: 0.09629813982890202 | Validation loss: 0.08379857987165451\nValidation loss (ends of cycles): [0.30131912 0.0881614 ]\n------------------------------\nEpoch: 20\nTraining loss: 0.0934074344829871 | Validation loss: 0.08416049679120381\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 21\nTraining loss: 0.09412454641782321 | Validation loss: 0.08251916865507762\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 22\nTraining loss: 0.09572084749547335 | Validation loss: 0.08353349069754283\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 23\nTraining loss: 0.09290660158372842 | Validation loss: 0.0896812950571378\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 24\nTraining loss: 0.09400351259570855 | Validation loss: 0.11157957216103871\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 25\nTraining loss: 0.09808041814428109 | Validation loss: 0.09437836209932964\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 26\nTraining loss: 0.09618092586214726 | Validation loss: 0.07646445681651433\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 27\nTraining loss: 0.09161735111131118 | Validation loss: 0.08044230192899704\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 28\nTraining loss: 0.08888172100369747 | Validation loss: 0.08159844825665157\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 29\nTraining loss: 0.08454665813881618 | Validation loss: 0.0776438241203626\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n------------------------------\nEpoch: 30\nTraining loss: 0.08237259147258905 | Validation loss: 0.07378115753332774\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 31\nTraining loss: 0.08362751325162557 | Validation loss: 0.07990996291240056\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 32\nTraining loss: 0.083226439757989 | Validation loss: 0.10914879540602367\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 33\nTraining loss: 0.08491195523394988 | Validation loss: 0.07139277209838231\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 34\nTraining loss: 0.08520169427188543 | Validation loss: 0.08888460695743561\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 35\nTraining loss: 0.0905728626709718 | Validation loss: 0.08219081163406372\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 36\nTraining loss: 0.08954568350544342 | Validation loss: 0.07995042701562245\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 37\nTraining loss: 0.08245866330197224 | Validation loss: 0.10690229882796605\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 38\nTraining loss: 0.08004418946802616 | Validation loss: 0.0655438502629598\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 39\nTraining loss: 0.07748245619810544 | Validation loss: 0.07324954122304916\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n------------------------------\nEpoch: 40\nTraining loss: 0.07241358321446639 | Validation loss: 0.06766582901279132\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 41\nTraining loss: 0.07349490660887498 | Validation loss: 0.08220083763202031\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 42\nTraining loss: 0.07459167777918853 | Validation loss: 0.06816854948798816\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 43\nTraining loss: 0.075133565813303 | Validation loss: 0.06550999979178111\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 44\nTraining loss: 0.07938563565795238 | Validation loss: 0.17636031409104666\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 45\nTraining loss: 0.08100382424890995 | Validation loss: 0.14928263425827026\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 46\nTraining loss: 0.08009787734884483 | Validation loss: 0.19112283488114676\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 47\nTraining loss: 0.07215518289460586 | Validation loss: 0.06750130653381348\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 48\nTraining loss: 0.06909940305810708 | Validation loss: 0.06105370571215948\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 49\nTraining loss: 0.06814099848270416 | Validation loss: 0.06708964208761851\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n------------------------------\nEpoch: 50\nTraining loss: 0.0647653667972638 | Validation loss: 0.06590079019467036\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 51\nTraining loss: 0.06436607255958594 | Validation loss: 0.06134747465451559\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 52\nTraining loss: 0.06368346626941974 | Validation loss: 0.05555027723312378\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 53\nTraining loss: 0.06528307368549016 | Validation loss: 0.10233555485804875\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 54\nTraining loss: 0.06762558365097412 | Validation loss: 0.06919782360394795\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 55\nTraining loss: 0.0683538823460157 | Validation loss: 0.09502405176560084\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 56\nTraining loss: 0.07198400795459747 | Validation loss: 0.14405548572540283\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 57\nTraining loss: 0.06737143417390493 | Validation loss: 0.07190001259247462\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 58\nTraining loss: 0.06260795833972785 | Validation loss: 0.062167766193548836\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 59\nTraining loss: 0.05951648993560901 | Validation loss: 0.07571763545274734\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n------------------------------\nEpoch: 60\nTraining loss: 0.05508949851187376 | Validation loss: 0.0668637715280056\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 61\nTraining loss: 0.052382624636475854 | Validation loss: 0.09922036528587341\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 62\nTraining loss: 0.056416381580325276 | Validation loss: 0.09052975972493489\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 63\nTraining loss: 0.057161668745371014 | Validation loss: 0.07923702895641327\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 64\nTraining loss: 0.06068163279157419 | Validation loss: 0.07965557028849919\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 65\nTraining loss: 0.06255157041148497 | Validation loss: 0.30685415863990784\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 66\nTraining loss: 0.06131344331571689 | Validation loss: 0.0656268410384655\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 67\nTraining loss: 0.06062774546444416 | Validation loss: 0.08304284016291301\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 68\nTraining loss: 0.05327921833556432 | Validation loss: 0.14299276967843375\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 69\nTraining loss: 0.05078572436020924 | Validation loss: 0.06972592696547508\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377]\n------------------------------\nEpoch: 70\nTraining loss: 0.04483967346067612 | Validation loss: 0.07364171991745631\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 71\nTraining loss: 0.047769702469500214 | Validation loss: 0.08219071726004283\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 72\nTraining loss: 0.04863498230966238 | Validation loss: 0.15322100122769675\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 73\nTraining loss: 0.04958262351843027 | Validation loss: 0.08630643784999847\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 74\nTraining loss: 0.05238688429101156 | Validation loss: 0.12523938218752542\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 75\nTraining loss: 0.05482054688036442 | Validation loss: 0.07240167011817296\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 76\nTraining loss: 0.05516412023168344 | Validation loss: 0.08454832683006923\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 77\nTraining loss: 0.05065158023857153 | Validation loss: 0.08039050673445065\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 78\nTraining loss: 0.047002577007963106 | Validation loss: 0.12330764532089233\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 79\nTraining loss: 0.04479805251153616 | Validation loss: 0.06863002230723698\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172]\n------------------------------\nEpoch: 80\nTraining loss: 0.042003152605432734 | Validation loss: 0.06680252775549889\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 81\nTraining loss: 0.040736994276253075 | Validation loss: 0.06924128532409668\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 82\nTraining loss: 0.04364607404344357 | Validation loss: 0.13677890847126642\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 83\nTraining loss: 0.04239509634387035 | Validation loss: 0.06394252677758534\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 84\nTraining loss: 0.048944419536453024 | Validation loss: 0.12318740785121918\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 85\nTraining loss: 0.050307344358700976 | Validation loss: 0.07770912845929463\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 86\nTraining loss: 0.0510884178085969 | Validation loss: 0.12004852046569188\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 87\nTraining loss: 0.044719476682635456 | Validation loss: 0.08277116964260738\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 88\nTraining loss: 0.0436299704015255 | Validation loss: 0.09767195334037145\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 89\nTraining loss: 0.038958583815166585 | Validation loss: 0.06731322159369786\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253]\n------------------------------\nEpoch: 90\nTraining loss: 0.03566374486455551 | Validation loss: 0.06604948143164317\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 91\nTraining loss: 0.03600069758697198 | Validation loss: 0.06159008666872978\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 92\nTraining loss: 0.03836567363200279 | Validation loss: 0.0633199227352937\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 93\nTraining loss: 0.037643212968340285 | Validation loss: 0.12101396421591441\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 94\nTraining loss: 0.04071113252295898 | Validation loss: 0.09344382832447688\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 95\nTraining loss: 0.044794661566041984 | Validation loss: 0.16241360704104105\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 96\nTraining loss: 0.04553248654477871 | Validation loss: 0.087947316467762\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 97\nTraining loss: 0.04601407094070545 | Validation loss: 0.06197854007283846\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 98\nTraining loss: 0.037852445259117164 | Validation loss: 0.07366656263669331\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 99\nTraining loss: 0.03565156388168152 | Validation loss: 0.0590630459288756\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948]\n------------------------------\nEpoch: 100\nTraining loss: 0.03224476340871591 | Validation loss: 0.0645342580974102\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 101\nTraining loss: 0.03184994045071877 | Validation loss: 0.06282762189706166\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 102\nTraining loss: 0.032962807740729586 | Validation loss: 0.06401825199524562\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 103\nTraining loss: 0.0340998714359907 | Validation loss: 0.07556230823198955\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 104\nTraining loss: 0.03651317825111059 | Validation loss: 0.07191591709852219\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 105\nTraining loss: 0.03656492310647781 | Validation loss: 0.1600353717803955\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 106\nTraining loss: 0.04045305587351322 | Validation loss: 0.08703135202328365\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 107\nTraining loss: 0.03643202502280474 | Validation loss: 0.07275040199359258\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 108\nTraining loss: 0.03348856677229588 | Validation loss: 0.06988890593250592\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 109\nTraining loss: 0.030310515171060197 | Validation loss: 0.061166045566399894\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n------------------------------\nEpoch: 110\nTraining loss: 0.027753474978873365 | Validation loss: 0.06391521046559016\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 111\nTraining loss: 0.02818346696977432 | Validation loss: 0.05979646369814873\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 112\nTraining loss: 0.02923572829996164 | Validation loss: 0.0680171325802803\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 113\nTraining loss: 0.030005195894493505 | Validation loss: 0.06576111788551013\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 114\nTraining loss: 0.03573801576231535 | Validation loss: 0.056423703829447426\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 115\nTraining loss: 0.036100911119809516 | Validation loss: 0.07376208404699962\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 116\nTraining loss: 0.03690812213776203 | Validation loss: 0.0762839342157046\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 117\nTraining loss: 0.03343923941541176 | Validation loss: 0.061920154839754105\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 118\nTraining loss: 0.02978512293730791 | Validation loss: 0.07674552748600642\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 119\nTraining loss: 0.029101334512233734 | Validation loss: 0.06387490406632423\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n------------------------------\nEpoch: 120\nTraining loss: 0.026124813271543153 | Validation loss: 0.06534932677944501\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 121\nTraining loss: 0.025870294095231935 | Validation loss: 0.06444216519594193\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 122\nTraining loss: 0.025658548558847263 | Validation loss: 0.06507259979844093\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 123\nTraining loss: 0.028533661523117468 | Validation loss: 0.06774731601277988\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 124\nTraining loss: 0.02891709805967716 | Validation loss: 0.0724047174056371\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 125\nTraining loss: 0.03162456948596697 | Validation loss: 0.06366992741823196\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 126\nTraining loss: 0.03268601781187149 | Validation loss: 0.0641860527296861\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 127\nTraining loss: 0.03037001765691317 | Validation loss: 0.07524159799019496\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 128\nTraining loss: 0.02665410556185704 | Validation loss: 0.06862633923689525\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 129\nTraining loss: 0.02404931803735403 | Validation loss: 0.06612421944737434\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933]\n------------------------------\nEpoch: 130\nTraining loss: 0.023632952441962864 | Validation loss: 0.06416553010543187\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 131\nTraining loss: 0.023884588565963965 | Validation loss: 0.06173599263032278\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 132\nTraining loss: 0.02424594580840606 | Validation loss: 0.06624979277451833\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 133\nTraining loss: 0.025196982762561396 | Validation loss: 0.061767312387625374\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 134\nTraining loss: 0.02775773348716589 | Validation loss: 0.09692556907733281\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 135\nTraining loss: 0.032594263123778194 | Validation loss: 0.08224088450272878\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 136\nTraining loss: 0.029487412207974836 | Validation loss: 0.07490686575571696\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 137\nTraining loss: 0.02741387515113904 | Validation loss: 0.06623867899179459\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 138\nTraining loss: 0.027188565940237962 | Validation loss: 0.06423299262921016\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 139\nTraining loss: 0.022550587363254566 | Validation loss: 0.06976987918217976\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553]\n------------------------------\nEpoch: 140\nTraining loss: 0.022771345952955577 | Validation loss: 0.06576516106724739\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 141\nTraining loss: 0.02289973869203375 | Validation loss: 0.0628873569269975\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 142\nTraining loss: 0.023526311493836917 | Validation loss: 0.07104157408078511\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 143\nTraining loss: 0.024283254232544165 | Validation loss: 0.08539532621701558\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 144\nTraining loss: 0.026763396409268562 | Validation loss: 0.06554212172826131\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 145\nTraining loss: 0.03026583532874401 | Validation loss: 0.11454411596059799\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 146\nTraining loss: 0.028293991676316813 | Validation loss: 0.08389495313167572\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 147\nTraining loss: 0.02770799584686756 | Validation loss: 0.06585045903921127\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 148\nTraining loss: 0.023221183926440202 | Validation loss: 0.06650333975752194\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 149\nTraining loss: 0.02247353306470009 | Validation loss: 0.08126651247342427\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516]\n------------------------------\nEpoch: 150\nTraining loss: 0.019623086751940157 | Validation loss: 0.06778747588396072\nValidation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n 0.06534933 0.06416553 0.06576516 0.06778748]\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 2000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.1618801434250439 | Validation loss: 0.1752209154268106\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 1\nTraining loss: 0.14574233997686237 | Validation loss: 0.15481133138140044\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 2\nTraining loss: 0.11868300873274897 | Validation loss: 0.11752228438854218\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 3\nTraining loss: 0.10331503433339737 | Validation loss: 0.11742908507585526\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 4\nTraining loss: 0.09970792346433097 | Validation loss: 0.10259843369325002\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 5\nTraining loss: 0.09703811111987806 | Validation loss: 0.103461354970932\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 6\nTraining loss: 0.09149395820556902 | Validation loss: 0.10043870781858762\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 7\nTraining loss: 0.09101413066188495 | Validation loss: 0.12704111635684967\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 8\nTraining loss: 0.08387162901607215 | Validation loss: 0.10820480187733968\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 9\nTraining loss: 0.0832572073913088 | Validation loss: 0.1032949797809124\nValidation loss (ends of cycles): [0.17522092]\n------------------------------\nEpoch: 10\nTraining loss: 0.07924356764438105 | Validation loss: 0.09691472351551056\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 11\nTraining loss: 0.08008078003630918 | Validation loss: 0.0957989643017451\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 12\nTraining loss: 0.08209384254672948 | Validation loss: 0.11568646629651387\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 13\nTraining loss: 0.08243749655929267 | Validation loss: 0.09991084039211273\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 14\nTraining loss: 0.08606560250707701 | Validation loss: 0.1379929060737292\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 15\nTraining loss: 0.0824899130738249 | Validation loss: 0.09170163857440154\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 16\nTraining loss: 0.0803560282961995 | Validation loss: 0.09212791919708252\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 17\nTraining loss: 0.07607325809259041 | Validation loss: 0.09057196353872617\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 18\nTraining loss: 0.07439241616749297 | Validation loss: 0.09052857694526513\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 19\nTraining loss: 0.07024018408036699 | Validation loss: 0.09308399073779583\nValidation loss (ends of cycles): [0.17522092 0.09691472]\n------------------------------\nEpoch: 20\nTraining loss: 0.06798860613329738 | Validation loss: 0.08868985312680404\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 21\nTraining loss: 0.06862006760111042 | Validation loss: 0.09170038687686126\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 22\nTraining loss: 0.06869713041712255 | Validation loss: 0.08607339983185132\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 23\nTraining loss: 0.07040536199130264 | Validation loss: 0.08300078163544337\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 24\nTraining loss: 0.06987133454166207 | Validation loss: 0.09090227695802848\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 25\nTraining loss: 0.0716366690090474 | Validation loss: 0.09582946076989174\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 26\nTraining loss: 0.06975229413193815 | Validation loss: 0.08876596515377362\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 27\nTraining loss: 0.06483151796547805 | Validation loss: 0.08653237794836362\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 28\nTraining loss: 0.06122077154178245 | Validation loss: 0.08586440918346246\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 29\nTraining loss: 0.059484159245210534 | Validation loss: 0.09282346442341805\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n------------------------------\nEpoch: 30\nTraining loss: 0.05785673750820113 | Validation loss: 0.0816469881683588\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 31\nTraining loss: 0.0590196952004643 | Validation loss: 0.09680157403151195\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 32\nTraining loss: 0.05919191201089644 | Validation loss: 0.08778925302127998\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 33\nTraining loss: 0.05938986396672679 | Validation loss: 0.09211701527237892\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 34\nTraining loss: 0.061850913380290945 | Validation loss: 0.0910467089464267\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 35\nTraining loss: 0.06223887621479876 | Validation loss: 0.10479206964373589\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 36\nTraining loss: 0.06194849370741377 | Validation loss: 0.08416965107123058\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 37\nTraining loss: 0.05916060693562031 | Validation loss: 0.09831625409424305\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 38\nTraining loss: 0.05644529353023744 | Validation loss: 0.0976157213250796\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 39\nTraining loss: 0.05409847791580593 | Validation loss: 0.07766535754005115\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n------------------------------\nEpoch: 40\nTraining loss: 0.05131756507006346 | Validation loss: 0.07610398282607396\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 41\nTraining loss: 0.05128737677838288 | Validation loss: 0.07781509744624297\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 42\nTraining loss: 0.052724012411108204 | Validation loss: 0.07516794838011265\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 43\nTraining loss: 0.05487711676487736 | Validation loss: 0.10514147579669952\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 44\nTraining loss: 0.054588435363827965 | Validation loss: 0.08979047338167827\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 45\nTraining loss: 0.05867484726888292 | Validation loss: 0.094602112347881\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 46\nTraining loss: 0.056937527313244106 | Validation loss: 0.08283252144853274\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 47\nTraining loss: 0.05434086024030751 | Validation loss: 0.11367898931105931\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 48\nTraining loss: 0.05096290885087322 | Validation loss: 0.08029752473036449\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 49\nTraining loss: 0.049206791336045545 | Validation loss: 0.07943053916096687\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n------------------------------\nEpoch: 50\nTraining loss: 0.046280208025492875 | Validation loss: 0.07776264660060406\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 51\nTraining loss: 0.045652902703367026 | Validation loss: 0.079695966715614\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 52\nTraining loss: 0.04672223851815158 | Validation loss: 0.07726358249783516\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 53\nTraining loss: 0.048943011930175855 | Validation loss: 0.10645054529110591\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 54\nTraining loss: 0.050548168921879695 | Validation loss: 0.08036942842106025\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 55\nTraining loss: 0.052031887001266666 | Validation loss: 0.10596027101079623\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 56\nTraining loss: 0.053062821793205595 | Validation loss: 0.10486301655570666\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 57\nTraining loss: 0.049803431091063166 | Validation loss: 0.10336205114920934\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 58\nTraining loss: 0.04522683836665808 | Validation loss: 0.09069225067893665\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 59\nTraining loss: 0.04493647981800285 | Validation loss: 0.07809621468186378\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n------------------------------\nEpoch: 60\nTraining loss: 0.04065243574772395 | Validation loss: 0.07773153235514958\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 61\nTraining loss: 0.04221714835833101 | Validation loss: 0.07856796185175578\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 62\nTraining loss: 0.04279230407201776 | Validation loss: 0.08424294305344422\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 63\nTraining loss: 0.04577502148116336 | Validation loss: 0.09170131136973698\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 64\nTraining loss: 0.046164985600055435 | Validation loss: 0.08783026970922947\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 65\nTraining loss: 0.047662596930475795 | Validation loss: 0.08518970385193825\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 66\nTraining loss: 0.04445842315680256 | Validation loss: 0.07751050901909669\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 67\nTraining loss: 0.043508671991088814 | Validation loss: 0.08918310577670734\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 68\nTraining loss: 0.04179470343332665 | Validation loss: 0.09014023592074712\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 69\nTraining loss: 0.04049620671453429 | Validation loss: 0.07655073702335358\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153]\n------------------------------\nEpoch: 70\nTraining loss: 0.0396222028443042 | Validation loss: 0.07584572024643421\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 71\nTraining loss: 0.03717943269978551 | Validation loss: 0.07880508278807004\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 72\nTraining loss: 0.03973317442133146 | Validation loss: 0.08252802553276221\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 73\nTraining loss: 0.04049243540594391 | Validation loss: 0.09000153529147308\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 74\nTraining loss: 0.04049573047999658 | Validation loss: 0.10002372041344643\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 75\nTraining loss: 0.044400962179197985 | Validation loss: 0.07380459271371365\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 76\nTraining loss: 0.043414482737288755 | Validation loss: 0.07823999598622322\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 77\nTraining loss: 0.038739253839879646 | Validation loss: 0.08262713812291622\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 78\nTraining loss: 0.03809597438164786 | Validation loss: 0.08333441180487473\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 79\nTraining loss: 0.035754169465280046 | Validation loss: 0.08620268727342288\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572]\n------------------------------\nEpoch: 80\nTraining loss: 0.03467226956112712 | Validation loss: 0.07951171075304349\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 81\nTraining loss: 0.03466332311212432 | Validation loss: 0.07921988517045975\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 82\nTraining loss: 0.03592760305778653 | Validation loss: 0.0850036001453797\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 83\nTraining loss: 0.036594210959532684 | Validation loss: 0.07627586275339127\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 84\nTraining loss: 0.04111481717258107 | Validation loss: 0.07233001788457234\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 85\nTraining loss: 0.040537830971765755 | Validation loss: 0.09824792668223381\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 86\nTraining loss: 0.03904353648278059 | Validation loss: 0.08698291952411334\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 87\nTraining loss: 0.03920765275902608 | Validation loss: 0.0835754635433356\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 88\nTraining loss: 0.03601997488123529 | Validation loss: 0.08621295168995857\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 89\nTraining loss: 0.03377638346351245 | Validation loss: 0.07983815545837085\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171]\n------------------------------\nEpoch: 90\nTraining loss: 0.0327324680801408 | Validation loss: 0.0762726366519928\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 91\nTraining loss: 0.03165368520307774 | Validation loss: 0.08496354147791862\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 92\nTraining loss: 0.03344015656586956 | Validation loss: 0.07823167617122333\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 93\nTraining loss: 0.03262264915175882 | Validation loss: 0.0848035675783952\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 94\nTraining loss: 0.03703135953230016 | Validation loss: 0.07858358137309551\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 95\nTraining loss: 0.03976100869476795 | Validation loss: 0.10528316410879295\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 96\nTraining loss: 0.03727892081381059 | Validation loss: 0.07412890965739886\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 97\nTraining loss: 0.03417914097799974 | Validation loss: 0.07945749287803967\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 98\nTraining loss: 0.0325212050229311 | Validation loss: 0.07739517899851005\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 99\nTraining loss: 0.03150020338887093 | Validation loss: 0.08356203138828278\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264]\n------------------------------\nEpoch: 100\nTraining loss: 0.029827139632520722 | Validation loss: 0.07726814846197765\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 101\nTraining loss: 0.029131623405013598 | Validation loss: 0.07960248303910096\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 102\nTraining loss: 0.029651211191187885 | Validation loss: 0.077345651263992\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 103\nTraining loss: 0.03169410891246562 | Validation loss: 0.07760117140909036\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 104\nTraining loss: 0.03362071163514081 | Validation loss: 0.08115199704964955\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 105\nTraining loss: 0.03455840410920335 | Validation loss: 0.09261715412139893\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 106\nTraining loss: 0.034202438523518106 | Validation loss: 0.08733148748675983\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 107\nTraining loss: 0.031660995080920996 | Validation loss: 0.08463405569394429\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 108\nTraining loss: 0.030696076669675464 | Validation loss: 0.07534050444761912\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 109\nTraining loss: 0.028993837371030274 | Validation loss: 0.08378856008251508\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n------------------------------\nEpoch: 110\nTraining loss: 0.027884817408288225 | Validation loss: 0.07827197946608067\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 111\nTraining loss: 0.026954872278021832 | Validation loss: 0.0768817663192749\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 112\nTraining loss: 0.027306516892185398 | Validation loss: 0.07335130125284195\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 113\nTraining loss: 0.02984478463437043 | Validation loss: 0.07736792415380478\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 114\nTraining loss: 0.030624039343320857 | Validation loss: 0.07951478908459346\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 115\nTraining loss: 0.03410996135105105 | Validation loss: 0.09334786732991536\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 116\nTraining loss: 0.03268755740467824 | Validation loss: 0.08859992027282715\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 117\nTraining loss: 0.0311201336592728 | Validation loss: 0.07669213910897572\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 118\nTraining loss: 0.02850773309667905 | Validation loss: 0.076536084835728\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 119\nTraining loss: 0.026045327659185026 | Validation loss: 0.07779801202317078\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n------------------------------\nEpoch: 120\nTraining loss: 0.025879897213741846 | Validation loss: 0.07725021553536256\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 121\nTraining loss: 0.025006736263486685 | Validation loss: 0.08035719518860181\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 122\nTraining loss: 0.02521455225845178 | Validation loss: 0.07811126050849755\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 123\nTraining loss: 0.02737539444191783 | Validation loss: 0.07882043470939\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 124\nTraining loss: 0.02847288306072062 | Validation loss: 0.08231854687134425\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 125\nTraining loss: 0.03284748344152581 | Validation loss: 0.0926503340403239\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 126\nTraining loss: 0.029583644267975117 | Validation loss: 0.09233872592449188\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 127\nTraining loss: 0.02809212899164242 | Validation loss: 0.08092666106919448\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 128\nTraining loss: 0.02684425281397268 | Validation loss: 0.07736630427340667\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 129\nTraining loss: 0.026525435640531426 | Validation loss: 0.08286652341485023\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022]\n------------------------------\nEpoch: 130\nTraining loss: 0.026220443165477586 | Validation loss: 0.07740283012390137\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 131\nTraining loss: 0.024342383404134537 | Validation loss: 0.08926018575827281\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 132\nTraining loss: 0.025032186953752648 | Validation loss: 0.09302532176176707\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 133\nTraining loss: 0.0252848844244784 | Validation loss: 0.07852401832739513\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 134\nTraining loss: 0.02585682207170655 | Validation loss: 0.08621748288472493\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 135\nTraining loss: 0.027297223589437848 | Validation loss: 0.09372994552055995\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 136\nTraining loss: 0.029613328915016324 | Validation loss: 0.07932470242182414\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 137\nTraining loss: 0.027742074400770898 | Validation loss: 0.08915846919020017\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 138\nTraining loss: 0.024932185973168586 | Validation loss: 0.09161436433593433\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 139\nTraining loss: 0.02556919655306082 | Validation loss: 0.09213371202349663\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283]\n------------------------------\nEpoch: 140\nTraining loss: 0.024974070051137137 | Validation loss: 0.07853892631828785\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 141\nTraining loss: 0.022545989748894 | Validation loss: 0.09043761218587558\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 142\nTraining loss: 0.02345672288142583 | Validation loss: 0.08282394955555598\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 143\nTraining loss: 0.024325473727110553 | Validation loss: 0.11688203240434329\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 144\nTraining loss: 0.026514568840902225 | Validation loss: 0.09078371028105418\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 145\nTraining loss: 0.027508027358528447 | Validation loss: 0.08296988283594449\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 146\nTraining loss: 0.027422879256454168 | Validation loss: 0.11777617782354355\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 147\nTraining loss: 0.024141497910022736 | Validation loss: 0.09811192005872726\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 148\nTraining loss: 0.024877404100170322 | Validation loss: 0.0829291803141435\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 149\nTraining loss: 0.02423349623659662 | Validation loss: 0.08197802864015102\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893]\n------------------------------\nEpoch: 150\nTraining loss: 0.02233779888746201 | Validation loss: 0.08017101387182872\nValidation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n 0.07725022 0.07740283 0.07853893 0.08017101]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 5000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.21679930271595482 | Validation loss: 0.20318472385406494\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 1\nTraining loss: 0.1668136041230104 | Validation loss: 0.11816636696457863\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 2\nTraining loss: 0.1115807911131795 | Validation loss: 0.11339729763567448\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 3\nTraining loss: 0.10118959852912295 | Validation loss: 0.12549301534891127\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 4\nTraining loss: 0.09859782768281426 | Validation loss: 0.10448834672570229\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 5\nTraining loss: 0.09670952816061147 | Validation loss: 0.10199640865127245\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 6\nTraining loss: 0.09333653369639802 | Validation loss: 0.1617287278175354\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 7\nTraining loss: 0.090622306662047 | Validation loss: 0.0932340698937575\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 8\nTraining loss: 0.08455809977228247 | Validation loss: 0.08920024012525876\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 9\nTraining loss: 0.08212190136078774 | Validation loss: 0.08311140177150568\nValidation loss (ends of cycles): [0.20318472]\n------------------------------\nEpoch: 10\nTraining loss: 0.0791853968728715 | Validation loss: 0.08080732847253481\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 11\nTraining loss: 0.07991005939350823 | Validation loss: 0.08217759616672993\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 12\nTraining loss: 0.07952956224637707 | Validation loss: 0.09287703956166903\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 13\nTraining loss: 0.08107227450750006 | Validation loss: 0.09761275698741277\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 14\nTraining loss: 0.0815588255214879 | Validation loss: 0.09132462789614995\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 15\nTraining loss: 0.08069409350827923 | Validation loss: 0.1021335686246554\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 16\nTraining loss: 0.07824799048853671 | Validation loss: 0.09792800272504489\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 17\nTraining loss: 0.07535720156051043 | Validation loss: 0.09600358655055365\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 18\nTraining loss: 0.07134035119684193 | Validation loss: 0.07297346604367097\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 19\nTraining loss: 0.06730523676149489 | Validation loss: 0.06935091987252236\nValidation loss (ends of cycles): [0.20318472 0.08080733]\n------------------------------\nEpoch: 20\nTraining loss: 0.06681856215293483 | Validation loss: 0.06999336344500383\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 21\nTraining loss: 0.06591901843120733 | Validation loss: 0.0713375985622406\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 22\nTraining loss: 0.06646224384115437 | Validation loss: 0.07043899595737457\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 23\nTraining loss: 0.06865779084600801 | Validation loss: 0.0846476435661316\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 24\nTraining loss: 0.06966026428001602 | Validation loss: 0.08409541994333267\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 25\nTraining loss: 0.06914794953965296 | Validation loss: 0.07454115003347397\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 26\nTraining loss: 0.06687090094164601 | Validation loss: 0.12878460387388865\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 27\nTraining loss: 0.065276972186847 | Validation loss: 0.08846696764230728\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 28\nTraining loss: 0.06052851867605382 | Validation loss: 0.06936634952823321\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 29\nTraining loss: 0.05739320233816237 | Validation loss: 0.06372248244782289\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n------------------------------\nEpoch: 30\nTraining loss: 0.056034141623481054 | Validation loss: 0.06305947179595629\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 31\nTraining loss: 0.05621948521437607 | Validation loss: 0.0632335669050614\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 32\nTraining loss: 0.05801960817120207 | Validation loss: 0.06638292744755744\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 33\nTraining loss: 0.059636385510052285 | Validation loss: 0.07299174045523008\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 34\nTraining loss: 0.06075522271314944 | Validation loss: 0.06601005693276724\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 35\nTraining loss: 0.06310464410976631 | Validation loss: 0.07540631766120592\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 36\nTraining loss: 0.061312416993726894 | Validation loss: 0.0751885324716568\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 37\nTraining loss: 0.05711426153251036 | Validation loss: 0.07411545490225156\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 38\nTraining loss: 0.05412266294904581 | Validation loss: 0.06427605276306471\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 39\nTraining loss: 0.05176926941031546 | Validation loss: 0.060552603627244635\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n------------------------------\nEpoch: 40\nTraining loss: 0.0486671953777394 | Validation loss: 0.06134359017014503\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 41\nTraining loss: 0.05110002699212765 | Validation loss: 0.05918064539631208\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 42\nTraining loss: 0.051530711455490645 | Validation loss: 0.060740908980369566\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 43\nTraining loss: 0.05298666653084004 | Validation loss: 0.07054131577412287\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 44\nTraining loss: 0.05645980859013993 | Validation loss: 0.07371640031536421\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 45\nTraining loss: 0.05717385922536606 | Validation loss: 0.08709836999575298\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 46\nTraining loss: 0.05534336075010732 | Validation loss: 0.0652000146607558\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 47\nTraining loss: 0.05249954535677208 | Validation loss: 0.06557390540838241\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 48\nTraining loss: 0.04942367346150669 | Validation loss: 0.0619909405708313\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 49\nTraining loss: 0.04723699968748205 | Validation loss: 0.05703059149285158\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n------------------------------\nEpoch: 50\nTraining loss: 0.04512434300240569 | Validation loss: 0.057147355874379475\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 51\nTraining loss: 0.04565585554232748 | Validation loss: 0.058827751750747365\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 52\nTraining loss: 0.04680207665041676 | Validation loss: 0.06423094595472018\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 53\nTraining loss: 0.04919423680664517 | Validation loss: 0.07155880083640416\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 54\nTraining loss: 0.05087821399719696 | Validation loss: 0.06875351990262667\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 55\nTraining loss: 0.053621325627204 | Validation loss: 0.12530262917280197\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 56\nTraining loss: 0.05069674629219404 | Validation loss: 0.0649179848531882\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 57\nTraining loss: 0.04963773845394296 | Validation loss: 0.0683489166200161\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 58\nTraining loss: 0.046637567097511816 | Validation loss: 0.05715753951420387\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 59\nTraining loss: 0.04308643450183192 | Validation loss: 0.05415662241478761\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n------------------------------\nEpoch: 60\nTraining loss: 0.04191293368688014 | Validation loss: 0.05487368342777093\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 61\nTraining loss: 0.04219510956982693 | Validation loss: 0.056085166583458586\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 62\nTraining loss: 0.04427421400983503 | Validation loss: 0.061968022212386134\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 63\nTraining loss: 0.045907203914729626 | Validation loss: 0.06518676156798998\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 64\nTraining loss: 0.04781815905387946 | Validation loss: 0.07564251323541006\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 65\nTraining loss: 0.04938608843569211 | Validation loss: 0.08950053478280703\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 66\nTraining loss: 0.047823689848653914 | Validation loss: 0.05974354532857736\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 67\nTraining loss: 0.0452634461928071 | Validation loss: 0.05870025667051474\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 68\nTraining loss: 0.042983035578971776 | Validation loss: 0.0612156942486763\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 69\nTraining loss: 0.040601052402511356 | Validation loss: 0.055788303911685946\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368]\n------------------------------\nEpoch: 70\nTraining loss: 0.03814817611568087 | Validation loss: 0.05492573517064254\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 71\nTraining loss: 0.03847261846769513 | Validation loss: 0.05461364078025023\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 72\nTraining loss: 0.03995766030169848 | Validation loss: 0.05567261067529519\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 73\nTraining loss: 0.04270468235719861 | Validation loss: 0.06666113883256912\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 74\nTraining loss: 0.043942760514814085 | Validation loss: 0.059602606544891995\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 75\nTraining loss: 0.04800796254325335 | Validation loss: 0.06048930970331033\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 76\nTraining loss: 0.044324284302085404 | Validation loss: 0.058276225626468656\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 77\nTraining loss: 0.04246668216193051 | Validation loss: 0.06340857942899068\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 78\nTraining loss: 0.039575646829417374 | Validation loss: 0.06758413364489874\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 79\nTraining loss: 0.037527156377753876 | Validation loss: 0.0539743257065614\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574]\n------------------------------\nEpoch: 80\nTraining loss: 0.03588872146976041 | Validation loss: 0.05481154695153236\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 81\nTraining loss: 0.03649109230912107 | Validation loss: 0.054990808169047035\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 82\nTraining loss: 0.03719184185400253 | Validation loss: 0.06046265438199043\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 83\nTraining loss: 0.03993967960141306 | Validation loss: 0.05727590061724186\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 84\nTraining loss: 0.040751541143385916 | Validation loss: 0.06313362121582031\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 85\nTraining loss: 0.04366428489378822 | Validation loss: 0.06349676909546058\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 86\nTraining loss: 0.04226917906950309 | Validation loss: 0.056340149914224945\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 87\nTraining loss: 0.03940869762202886 | Validation loss: 0.06956625630458196\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 88\nTraining loss: 0.037179867033003355 | Validation loss: 0.056858163326978683\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 89\nTraining loss: 0.03474178528926504 | Validation loss: 0.054771875590085985\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155]\n------------------------------\nEpoch: 90\nTraining loss: 0.03365579730735754 | Validation loss: 0.05424569162229697\nValidation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n 0.05487368 0.05492574 0.05481155 0.05424569]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 10000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.3208344729514573 | Validation loss: 0.318280567382944\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 1\nTraining loss: 0.19803376548637555 | Validation loss: 0.13973277755852404\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 2\nTraining loss: 0.10369020112328173 | Validation loss: 0.1150948526016597\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 3\nTraining loss: 0.09644094917659216 | Validation loss: 0.10184345497139569\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 4\nTraining loss: 0.09403398349939839 | Validation loss: 0.1028368263665972\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 5\nTraining loss: 0.09193474704062375 | Validation loss: 0.1109759640590898\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 6\nTraining loss: 0.08814766762528832 | Validation loss: 0.09330829733918453\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 7\nTraining loss: 0.0837278733484623 | Validation loss: 0.08856733438783679\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 8\nTraining loss: 0.08128497258239374 | Validation loss: 0.08120295495308678\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 9\nTraining loss: 0.07837615451064166 | Validation loss: 0.07797302755302396\nValidation loss (ends of cycles): [0.31828057]\n------------------------------\nEpoch: 10\nTraining loss: 0.07498718809893751 | Validation loss: 0.07726067600065264\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 11\nTraining loss: 0.07560104029033128 | Validation loss: 0.07749038266724553\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 12\nTraining loss: 0.07756251360722415 | Validation loss: 0.08580444444870126\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 13\nTraining loss: 0.07727028810837137 | Validation loss: 0.0858236273814892\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 14\nTraining loss: 0.07767378302716364 | Validation loss: 0.08244718315786329\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 15\nTraining loss: 0.07831902486660819 | Validation loss: 0.08229162158637211\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 16\nTraining loss: 0.07570499881339354 | Validation loss: 0.07988743625324347\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 17\nTraining loss: 0.07201992783431463 | Validation loss: 0.07582339567357096\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 18\nTraining loss: 0.07011749405472532 | Validation loss: 0.07034371587736853\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 19\nTraining loss: 0.06716613902703045 | Validation loss: 0.0669062534539864\nValidation loss (ends of cycles): [0.31828057 0.07726068]\n------------------------------\nEpoch: 20\nTraining loss: 0.06499362923938223 | Validation loss: 0.06683127569227383\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 21\nTraining loss: 0.06468080149919499 | Validation loss: 0.06583795465272048\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 22\nTraining loss: 0.06689566351825327 | Validation loss: 0.06987852204976411\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 23\nTraining loss: 0.06782534108387203 | Validation loss: 0.07301809394667888\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 24\nTraining loss: 0.06932942601522123 | Validation loss: 0.07833132944230375\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 25\nTraining loss: 0.0697620217138388 | Validation loss: 0.07771215652083528\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 26\nTraining loss: 0.06711326695714645 | Validation loss: 0.07463829681791108\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 27\nTraining loss: 0.06510572683975452 | Validation loss: 0.07238342150531966\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 28\nTraining loss: 0.06255879405680603 | Validation loss: 0.06996388707695336\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 29\nTraining loss: 0.05977157332501778 | Validation loss: 0.06343879846149478\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n------------------------------\nEpoch: 30\nTraining loss: 0.057576970637255295 | Validation loss: 0.06144785264442707\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 31\nTraining loss: 0.05884648127642673 | Validation loss: 0.061254425552384605\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 32\nTraining loss: 0.06008706400244255 | Validation loss: 0.06730351057545893\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 33\nTraining loss: 0.06149038758979538 | Validation loss: 0.06669318663149044\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 34\nTraining loss: 0.06343353877768038 | Validation loss: 0.08957863676136937\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 35\nTraining loss: 0.06404111976551963 | Validation loss: 0.07944964103657624\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 36\nTraining loss: 0.06280158764470047 | Validation loss: 0.08468614403029968\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 37\nTraining loss: 0.06029054083986076 | Validation loss: 0.06408045762057962\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 38\nTraining loss: 0.05761603463265136 | Validation loss: 0.05974189076444198\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 39\nTraining loss: 0.05507340776462724 | Validation loss: 0.05976667499233936\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n------------------------------\nEpoch: 40\nTraining loss: 0.053342985795942814 | Validation loss: 0.05916541262433447\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 41\nTraining loss: 0.054149042385974976 | Validation loss: 0.059428418761697306\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 42\nTraining loss: 0.05521612206664611 | Validation loss: 0.06161621251496775\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 43\nTraining loss: 0.05741074803013971 | Validation loss: 0.07564911323374715\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 44\nTraining loss: 0.059064230778965894 | Validation loss: 0.08295242616842533\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 45\nTraining loss: 0.06096260105149718 | Validation loss: 0.0736706999355349\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 46\nTraining loss: 0.058049106735765466 | Validation loss: 0.08584050705720639\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 47\nTraining loss: 0.05583139895424834 | Validation loss: 0.06207926982435687\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 48\nTraining loss: 0.053796297921909125 | Validation loss: 0.06523714240255027\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 49\nTraining loss: 0.05198358270654997 | Validation loss: 0.05872965193000333\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n------------------------------\nEpoch: 50\nTraining loss: 0.0499456028314674 | Validation loss: 0.05686888162945879\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 51\nTraining loss: 0.05126144335876534 | Validation loss: 0.056591979195845535\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 52\nTraining loss: 0.05224344710724091 | Validation loss: 0.05856672815721611\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 53\nTraining loss: 0.054411778094496314 | Validation loss: 0.07042139074925718\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 54\nTraining loss: 0.055745888523405465 | Validation loss: 0.06894256778318307\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 55\nTraining loss: 0.05707524980117721 | Validation loss: 0.06330927410002413\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 56\nTraining loss: 0.05516272258952143 | Validation loss: 0.06743599166130197\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 57\nTraining loss: 0.05250089465985148 | Validation loss: 0.06211624667048454\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 58\nTraining loss: 0.051365907509319894 | Validation loss: 0.06060262486852448\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 59\nTraining loss: 0.04909465120973315 | Validation loss: 0.05675130376014216\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n------------------------------\nEpoch: 60\nTraining loss: 0.04706054231826597 | Validation loss: 0.05575629599906247\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 61\nTraining loss: 0.04808880996296373 | Validation loss: 0.05616617549596162\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 62\nTraining loss: 0.04886479155109154 | Validation loss: 0.05892433553677181\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 63\nTraining loss: 0.05041125721085494 | Validation loss: 0.06897250832668667\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 64\nTraining loss: 0.05188799403813176 | Validation loss: 0.062462263580026295\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 65\nTraining loss: 0.054635593799624856 | Validation loss: 0.08355555824678519\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 66\nTraining loss: 0.0523650164781945 | Validation loss: 0.06177988340114725\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 67\nTraining loss: 0.051539598399494575 | Validation loss: 0.05858155166537597\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 68\nTraining loss: 0.04865856434211252 | Validation loss: 0.058884155133674884\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 69\nTraining loss: 0.04666972260071537 | Validation loss: 0.05566656884962115\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563 ]\n------------------------------\nEpoch: 70\nTraining loss: 0.044823936340729086 | Validation loss: 0.054462554125950254\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 71\nTraining loss: 0.0449102942824481 | Validation loss: 0.054748651667915545\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 72\nTraining loss: 0.046292417118047165 | Validation loss: 0.059005388540440594\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 73\nTraining loss: 0.048082990483857516 | Validation loss: 0.05723131852674073\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 74\nTraining loss: 0.04956973357287448 | Validation loss: 0.05968912909257001\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 75\nTraining loss: 0.051790501929702255 | Validation loss: 0.05894576591150514\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 76\nTraining loss: 0.049998988016090526 | Validation loss: 0.059786687123364414\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 77\nTraining loss: 0.048110113376531545 | Validation loss: 0.061614472932856656\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 78\nTraining loss: 0.04663481412631437 | Validation loss: 0.06338580736312373\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 79\nTraining loss: 0.04410836506473619 | Validation loss: 0.05611117794338999\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255]\n------------------------------\nEpoch: 80\nTraining loss: 0.04312163801849123 | Validation loss: 0.05400283981500001\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 81\nTraining loss: 0.043554561714986416 | Validation loss: 0.056934723437860095\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 82\nTraining loss: 0.044636147390083064 | Validation loss: 0.0660295981014597\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 83\nTraining loss: 0.0459298248894102 | Validation loss: 0.06809257998548705\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 84\nTraining loss: 0.04766980750764918 | Validation loss: 0.06455176233731467\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 85\nTraining loss: 0.04948333185899446 | Validation loss: 0.08151760183531663\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 86\nTraining loss: 0.04826224101780672 | Validation loss: 0.07454486461035137\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 87\nTraining loss: 0.04570534480662327 | Validation loss: 0.058246461867258466\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 88\nTraining loss: 0.04386360907384495 | Validation loss: 0.058009152525457845\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 89\nTraining loss: 0.04169096091512855 | Validation loss: 0.054397762720954826\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284]\n------------------------------\nEpoch: 90\nTraining loss: 0.041218897184782374 | Validation loss: 0.05259814074841039\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 91\nTraining loss: 0.040955901549263735 | Validation loss: 0.05439365038584019\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 92\nTraining loss: 0.042126013860693125 | Validation loss: 0.054666623344709134\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 93\nTraining loss: 0.043382279897534 | Validation loss: 0.054504195998968746\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 94\nTraining loss: 0.0458415110822855 | Validation loss: 0.07741936720136938\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 95\nTraining loss: 0.04743661339886076 | Validation loss: 0.05820449994042002\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 96\nTraining loss: 0.04553815280270623 | Validation loss: 0.08208692163742821\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 97\nTraining loss: 0.044244329628395286 | Validation loss: 0.05583971597511193\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 98\nTraining loss: 0.04123691922625688 | Validation loss: 0.05591851151708899\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 99\nTraining loss: 0.04009729196266161 | Validation loss: 0.05433603845022876\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814]\n------------------------------\nEpoch: 100\nTraining loss: 0.03822168787780005 | Validation loss: 0.053116938934243955\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 101\nTraining loss: 0.038870895644429866 | Validation loss: 0.054241204441621386\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 102\nTraining loss: 0.04008115804937529 | Validation loss: 0.055827350130882754\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 103\nTraining loss: 0.041735652746178034 | Validation loss: 0.057072933634807324\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 104\nTraining loss: 0.043364248979162044 | Validation loss: 0.05602561297087834\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 105\nTraining loss: 0.045468564585261925 | Validation loss: 0.056964905804087376\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 106\nTraining loss: 0.044400842112349716 | Validation loss: 0.05765183186479684\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 107\nTraining loss: 0.04242724046023108 | Validation loss: 0.05811212301768105\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 108\nTraining loss: 0.03993679110811451 | Validation loss: 0.05642743761940249\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 109\nTraining loss: 0.03817879260099662 | Validation loss: 0.05428962120465163\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n------------------------------\nEpoch: 110\nTraining loss: 0.03728278963569933 | Validation loss: 0.05238519949388915\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 111\nTraining loss: 0.0370556244765973 | Validation loss: 0.05351860776286701\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 112\nTraining loss: 0.03803750030755058 | Validation loss: 0.05330655434779052\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 113\nTraining loss: 0.03953701080974397 | Validation loss: 0.05610055540656221\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 114\nTraining loss: 0.042156051558361746 | Validation loss: 0.056494539603590965\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 115\nTraining loss: 0.04371086021107951 | Validation loss: 0.059335493065159894\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 116\nTraining loss: 0.04238700263172857 | Validation loss: 0.0607527428916816\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 117\nTraining loss: 0.04007319592702107 | Validation loss: 0.059337090572406506\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 118\nTraining loss: 0.03805773765376703 | Validation loss: 0.059619187216820385\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 119\nTraining loss: 0.036783665865953045 | Validation loss: 0.05485150924530523\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n------------------------------\nEpoch: 120\nTraining loss: 0.03522829567705552 | Validation loss: 0.05326138205569366\nValidation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852\n 0.05326138]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 20000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.1937587873558321 | Validation loss: 0.16695564253288403\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 1\nTraining loss: 0.10741124820162559 | Validation loss: 0.09033931210114245\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 2\nTraining loss: 0.0909428682097434 | Validation loss: 0.08505091312945935\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 3\nTraining loss: 0.08447369038911731 | Validation loss: 0.08490002619331344\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 4\nTraining loss: 0.08043632089459803 | Validation loss: 0.0835269209193556\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 5\nTraining loss: 0.07768446769146524 | Validation loss: 0.07836407364199036\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 6\nTraining loss: 0.07283868803413074 | Validation loss: 0.07246115702416814\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 7\nTraining loss: 0.06800214434134419 | Validation loss: 0.07295098345269237\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 8\nTraining loss: 0.06450905432627047 | Validation loss: 0.06473837368059576\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 9\nTraining loss: 0.06113856292056026 | Validation loss: 0.06293978251255396\nValidation loss (ends of cycles): [0.16695564]\n------------------------------\nEpoch: 10\nTraining loss: 0.05838155703843228 | Validation loss: 0.058973234833071105\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 11\nTraining loss: 0.0590651414707979 | Validation loss: 0.06184358401387407\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 12\nTraining loss: 0.06017314939147975 | Validation loss: 0.06425718737668112\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 13\nTraining loss: 0.061172162533482385 | Validation loss: 0.06329335121993433\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 14\nTraining loss: 0.06214155544043763 | Validation loss: 0.06571824806170505\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 15\nTraining loss: 0.062408381161928414 | Validation loss: 0.06477978142599265\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 16\nTraining loss: 0.05940165142556267 | Validation loss: 0.06300883746722288\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 17\nTraining loss: 0.05636550784772317 | Validation loss: 0.06063326899158327\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 18\nTraining loss: 0.053919580950598275 | Validation loss: 0.05690223664829606\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 19\nTraining loss: 0.05147737494249198 | Validation loss: 0.054520211787077416\nValidation loss (ends of cycles): [0.16695564 0.05897323]\n------------------------------\nEpoch: 20\nTraining loss: 0.049701670721114505 | Validation loss: 0.051985655927605796\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 21\nTraining loss: 0.050548364601191685 | Validation loss: 0.05337116988212393\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 22\nTraining loss: 0.05188654155949869 | Validation loss: 0.055069943040347936\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 23\nTraining loss: 0.05248021238522064 | Validation loss: 0.05857947808608674\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 24\nTraining loss: 0.05433177806554579 | Validation loss: 0.055786194933349624\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 25\nTraining loss: 0.05556651474852886 | Validation loss: 0.05825148323518142\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 26\nTraining loss: 0.053006139650015084 | Validation loss: 0.05651293652491611\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 27\nTraining loss: 0.0507659353499523 | Validation loss: 0.054801122726578465\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 28\nTraining loss: 0.048554422462421525 | Validation loss: 0.05305930501536319\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 29\nTraining loss: 0.04574381269117784 | Validation loss: 0.0514153082315859\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n------------------------------\nEpoch: 30\nTraining loss: 0.04418007252991376 | Validation loss: 0.048620141879246945\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 31\nTraining loss: 0.045360717239111836 | Validation loss: 0.05021227791643979\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 32\nTraining loss: 0.046448042166200736 | Validation loss: 0.05336992553713029\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 33\nTraining loss: 0.04778991164217099 | Validation loss: 0.05832194040218989\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 34\nTraining loss: 0.04944647156451581 | Validation loss: 0.0614949915100608\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 35\nTraining loss: 0.050366569427990115 | Validation loss: 0.06288488016447477\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 36\nTraining loss: 0.048872271721679315 | Validation loss: 0.05419730365668472\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 37\nTraining loss: 0.04679563639711229 | Validation loss: 0.05351380425456323\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 38\nTraining loss: 0.04441320958236853 | Validation loss: 0.05135479059658552\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 39\nTraining loss: 0.042865205434916995 | Validation loss: 0.04892440996410554\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n------------------------------\nEpoch: 40\nTraining loss: 0.04114158953405932 | Validation loss: 0.04640055317104908\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 41\nTraining loss: 0.04168075290407537 | Validation loss: 0.04898981227163683\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 42\nTraining loss: 0.04272683222339116 | Validation loss: 0.04995410621427653\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 43\nTraining loss: 0.04432621974813985 | Validation loss: 0.0549667902421533\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 44\nTraining loss: 0.04593426139044338 | Validation loss: 0.05504765328869485\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 45\nTraining loss: 0.04727091159693588 | Validation loss: 0.054771556371920986\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 46\nTraining loss: 0.045680517350971936 | Validation loss: 0.05265137781960923\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 47\nTraining loss: 0.04370591644550392 | Validation loss: 0.05152795839597259\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 48\nTraining loss: 0.04130219880866229 | Validation loss: 0.05182074438453766\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 49\nTraining loss: 0.03948871645878053 | Validation loss: 0.04831022197347984\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n------------------------------\nEpoch: 50\nTraining loss: 0.03837553308303128 | Validation loss: 0.044728756146995646\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 51\nTraining loss: 0.03918741399484744 | Validation loss: 0.04656248438384449\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 52\nTraining loss: 0.040425572272470955 | Validation loss: 0.048068265806426085\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 53\nTraining loss: 0.041100266152585045 | Validation loss: 0.048309058506499254\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 54\nTraining loss: 0.043417033105647776 | Validation loss: 0.05213266104590474\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 55\nTraining loss: 0.044981646573784555 | Validation loss: 0.05113487236463187\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 56\nTraining loss: 0.04303855704661657 | Validation loss: 0.0511682754741949\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 57\nTraining loss: 0.04084054631395982 | Validation loss: 0.04861888168543054\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 58\nTraining loss: 0.038960600914745874 | Validation loss: 0.04695257093561323\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 59\nTraining loss: 0.03764710921970521 | Validation loss: 0.04477690320396632\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n------------------------------\nEpoch: 60\nTraining loss: 0.03553954229912403 | Validation loss: 0.043018419851075136\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 61\nTraining loss: 0.03677014895959131 | Validation loss: 0.045161824052532516\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 62\nTraining loss: 0.03763243454073131 | Validation loss: 0.047717598740730366\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 63\nTraining loss: 0.039285847094255325 | Validation loss: 0.04886620066929282\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 64\nTraining loss: 0.041000166599302605 | Validation loss: 0.05117675080372576\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 65\nTraining loss: 0.042431575734778154 | Validation loss: 0.05887748026534131\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 66\nTraining loss: 0.04095806826559985 | Validation loss: 0.04888384812103029\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 67\nTraining loss: 0.03861006007105465 | Validation loss: 0.04978298546190847\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 68\nTraining loss: 0.03706407239862683 | Validation loss: 0.04655743958918672\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 69\nTraining loss: 0.03537350038481652 | Validation loss: 0.044241009476153476\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842]\n------------------------------\nEpoch: 70\nTraining loss: 0.033808498057358596 | Validation loss: 0.04274106940679383\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 71\nTraining loss: 0.03488694527048684 | Validation loss: 0.04366510390843216\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 72\nTraining loss: 0.03567285149099206 | Validation loss: 0.04591469525506622\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 73\nTraining loss: 0.03710445848567305 | Validation loss: 0.048434995899074955\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 74\nTraining loss: 0.03909004668482897 | Validation loss: 0.04879440321472653\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 75\nTraining loss: 0.0402635611626788 | Validation loss: 0.049998423476752485\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 76\nTraining loss: 0.03877175844933919 | Validation loss: 0.047474542403953116\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 77\nTraining loss: 0.03686592971108839 | Validation loss: 0.049321869518934636\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 78\nTraining loss: 0.03513916212830259 | Validation loss: 0.04538330256023951\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 79\nTraining loss: 0.03325431909195388 | Validation loss: 0.04405715048574565\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107]\n------------------------------\nEpoch: 80\nTraining loss: 0.032414121551464885 | Validation loss: 0.041407169022581035\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 81\nTraining loss: 0.03273387079694744 | Validation loss: 0.04332864193017023\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 82\nTraining loss: 0.03379245271219246 | Validation loss: 0.04393824611447359\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 83\nTraining loss: 0.036216145730347794 | Validation loss: 0.04601557598563663\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 84\nTraining loss: 0.03738668957390846 | Validation loss: 0.0478383689269162\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 85\nTraining loss: 0.03850440040260142 | Validation loss: 0.046575586066434256\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 86\nTraining loss: 0.037245836072184395 | Validation loss: 0.04904622127089584\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 87\nTraining loss: 0.035356279124581134 | Validation loss: 0.04652073737560657\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 88\nTraining loss: 0.034129417027462514 | Validation loss: 0.04397078325743215\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 89\nTraining loss: 0.032686933013564616 | Validation loss: 0.04174699555886419\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717]\n------------------------------\nEpoch: 90\nTraining loss: 0.03151373515232545 | Validation loss: 0.040432571188399664\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 91\nTraining loss: 0.031794840381783845 | Validation loss: 0.04145001789979767\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 92\nTraining loss: 0.03241383722070348 | Validation loss: 0.04404820838387598\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 93\nTraining loss: 0.034046458082247884 | Validation loss: 0.045810290955399215\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 94\nTraining loss: 0.03595437882449735 | Validation loss: 0.059114713091076465\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 95\nTraining loss: 0.037035941107738654 | Validation loss: 0.047888670345408876\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 96\nTraining loss: 0.035954072046473884 | Validation loss: 0.04650187672099523\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 97\nTraining loss: 0.03383453256272412 | Validation loss: 0.04721131607105857\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 98\nTraining loss: 0.032827389764524186 | Validation loss: 0.042917570001200625\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 99\nTraining loss: 0.03113685309872237 | Validation loss: 0.04149719017247359\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257]\n------------------------------\nEpoch: 100\nTraining loss: 0.030135778122429076 | Validation loss: 0.03988342725655489\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 101\nTraining loss: 0.03010753776944248 | Validation loss: 0.04145811442612556\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 102\nTraining loss: 0.03123408529165346 | Validation loss: 0.04319112502822751\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 103\nTraining loss: 0.032564907242898525 | Validation loss: 0.044689477470360305\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 104\nTraining loss: 0.033901535042541384 | Validation loss: 0.04784849746838996\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 105\nTraining loss: 0.03641212473511402 | Validation loss: 0.051434298160306195\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 106\nTraining loss: 0.03419514418041418 | Validation loss: 0.045509515918399156\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 107\nTraining loss: 0.0329815001140626 | Validation loss: 0.046876753043187294\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 108\nTraining loss: 0.031264307952800094 | Validation loss: 0.043929761667784895\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 109\nTraining loss: 0.030383295491631684 | Validation loss: 0.04097266080217403\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n------------------------------\nEpoch: 110\nTraining loss: 0.029282903673468964 | Validation loss: 0.0398062279840049\nValidation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343 0.03980623]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 30000\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.1400897194288279 | Validation loss: 0.12329615446574547\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 1\nTraining loss: 0.09554748258210327 | Validation loss: 0.08580511917962748\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 2\nTraining loss: 0.08333759619609306 | Validation loss: 0.0800603346351315\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 3\nTraining loss: 0.07792517879585686 | Validation loss: 0.07561177874312681\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 4\nTraining loss: 0.07436479456526668 | Validation loss: 0.07901463929344626\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 5\nTraining loss: 0.07109708578668927 | Validation loss: 0.06837146382998018\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 6\nTraining loss: 0.06542817449785376 | Validation loss: 0.0787474691429559\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 7\nTraining loss: 0.061334840936194124 | Validation loss: 0.05965573042631149\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 8\nTraining loss: 0.057118404958103046 | Validation loss: 0.0656482653144528\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 9\nTraining loss: 0.05406631466344391 | Validation loss: 0.05449213265057872\nValidation loss (ends of cycles): [0.12329615]\n------------------------------\nEpoch: 10\nTraining loss: 0.05106979912931198 | Validation loss: 0.052335235345013\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 11\nTraining loss: 0.052069277808952485 | Validation loss: 0.05276231921332724\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 12\nTraining loss: 0.05316668891494996 | Validation loss: 0.05438405183308265\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 13\nTraining loss: 0.054588352700107194 | Validation loss: 0.05557882632402813\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 14\nTraining loss: 0.05497209098386137 | Validation loss: 0.05784036999239641\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 15\nTraining loss: 0.05595910447208505 | Validation loss: 0.06857345152427169\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 16\nTraining loss: 0.05293017792956609 | Validation loss: 0.07677049303756041\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 17\nTraining loss: 0.05036535177526898 | Validation loss: 0.054857921183985824\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 18\nTraining loss: 0.0480112096307015 | Validation loss: 0.04789778322857969\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 19\nTraining loss: 0.045682432433884396 | Validation loss: 0.04683302770204404\nValidation loss (ends of cycles): [0.12329615 0.05233524]\n------------------------------\nEpoch: 20\nTraining loss: 0.04365749700289023 | Validation loss: 0.04525614475064418\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 21\nTraining loss: 0.04457468941905781 | Validation loss: 0.046083587692940936\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 22\nTraining loss: 0.04552650913155001 | Validation loss: 0.045803798384526195\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 23\nTraining loss: 0.04733393179546846 | Validation loss: 0.04765025776098756\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 24\nTraining loss: 0.048526976326186404 | Validation loss: 0.05163861622705179\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 25\nTraining loss: 0.04971253661144721 | Validation loss: 0.06940157834221335\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 26\nTraining loss: 0.047659809549192064 | Validation loss: 0.049176037705996455\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 27\nTraining loss: 0.04544191577006131 | Validation loss: 0.047405094828675776\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 28\nTraining loss: 0.04377149401943346 | Validation loss: 0.04654841545750113\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 29\nTraining loss: 0.041435345920341975 | Validation loss: 0.04301520901567796\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n------------------------------\nEpoch: 30\nTraining loss: 0.039447522790808424 | Validation loss: 0.041631047142779126\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 31\nTraining loss: 0.04063450964412799 | Validation loss: 0.04336159229278565\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 32\nTraining loss: 0.04200547775253653 | Validation loss: 0.04426659119918066\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 33\nTraining loss: 0.04342988368711973 | Validation loss: 0.044159990264212384\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 34\nTraining loss: 0.04422340278611764 | Validation loss: 0.04507376104593277\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 35\nTraining loss: 0.04580574428760692 | Validation loss: 0.051384791403132325\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 36\nTraining loss: 0.04419933799531703 | Validation loss: 0.04598492960281232\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 37\nTraining loss: 0.04210811722165856 | Validation loss: 0.043686308317324694\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 38\nTraining loss: 0.04024575349727744 | Validation loss: 0.043796195002163155\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 39\nTraining loss: 0.038407098160027284 | Validation loss: 0.04090072779971011\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n------------------------------\nEpoch: 40\nTraining loss: 0.036603522439193174 | Validation loss: 0.039455434132148234\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 41\nTraining loss: 0.03742256851278638 | Validation loss: 0.04058602978201473\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 42\nTraining loss: 0.03843546623829752 | Validation loss: 0.041917330859338534\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 43\nTraining loss: 0.039746915991418066 | Validation loss: 0.04393004884614664\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 44\nTraining loss: 0.04127714485542751 | Validation loss: 0.044884422421455385\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 45\nTraining loss: 0.0428990826157755 | Validation loss: 0.04536734256235992\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 46\nTraining loss: 0.04116344682284092 | Validation loss: 0.04351957151118447\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 47\nTraining loss: 0.039308764099290495 | Validation loss: 0.041029254938749704\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 48\nTraining loss: 0.03747508091067797 | Validation loss: 0.04191207048647544\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 49\nTraining loss: 0.035598073072584446 | Validation loss: 0.03979100354892366\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n------------------------------\nEpoch: 50\nTraining loss: 0.03438328353835172 | Validation loss: 0.037895834172034966\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 51\nTraining loss: 0.0351846847768971 | Validation loss: 0.03996862692429739\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 52\nTraining loss: 0.03598356681973919 | Validation loss: 0.0409109014141209\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 53\nTraining loss: 0.03732215452351068 | Validation loss: 0.042976783128345714\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 54\nTraining loss: 0.039110059012952995 | Validation loss: 0.044927890949389516\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 55\nTraining loss: 0.04038672065852504 | Validation loss: 0.04456539171583512\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 56\nTraining loss: 0.03914046656757005 | Validation loss: 0.04287377705468851\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 57\nTraining loss: 0.037129987647609886 | Validation loss: 0.040416243420365976\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 58\nTraining loss: 0.03537750957705277 | Validation loss: 0.03983505024191211\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 59\nTraining loss: 0.03378118044378138 | Validation loss: 0.03855334082070519\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n------------------------------\nEpoch: 60\nTraining loss: 0.03227280559949577 | Validation loss: 0.03684680106885293\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 61\nTraining loss: 0.03293099604963668 | Validation loss: 0.03870913982391357\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 62\nTraining loss: 0.03440849116121076 | Validation loss: 0.03925530374707545\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 63\nTraining loss: 0.03551614041636257 | Validation loss: 0.04271749407052994\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 64\nTraining loss: 0.03671016464451034 | Validation loss: 0.044847844102803396\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 65\nTraining loss: 0.038699115091003475 | Validation loss: 0.04217615879195578\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 66\nTraining loss: 0.036989314980363765 | Validation loss: 0.04112030182252912\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 67\nTraining loss: 0.035253367830361974 | Validation loss: 0.03938967145102865\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 68\nTraining loss: 0.033740292163565756 | Validation loss: 0.0396494365461609\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 69\nTraining loss: 0.03221185418017405 | Validation loss: 0.03731777299852932\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468 ]\n------------------------------\nEpoch: 70\nTraining loss: 0.030991236463581262 | Validation loss: 0.03593932203948498\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 71\nTraining loss: 0.03149883268572586 | Validation loss: 0.037519738275338624\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 72\nTraining loss: 0.03258418612868378 | Validation loss: 0.03948524081531693\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 73\nTraining loss: 0.033819790642806574 | Validation loss: 0.040817394786897825\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 74\nTraining loss: 0.0349594693140764 | Validation loss: 0.038657077379962976\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 75\nTraining loss: 0.037001777448887496 | Validation loss: 0.043351417955230266\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 76\nTraining loss: 0.03563734809132783 | Validation loss: 0.04313999725615277\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 77\nTraining loss: 0.03373342292799957 | Validation loss: 0.04140989732216386\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 78\nTraining loss: 0.031958367353256203 | Validation loss: 0.03882313241415164\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 79\nTraining loss: 0.03071572730121644 | Validation loss: 0.038228221697842374\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932]\n------------------------------\nEpoch: 80\nTraining loss: 0.02992848748292186 | Validation loss: 0.03572125563069301\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 81\nTraining loss: 0.030003602803978874 | Validation loss: 0.03800442082916989\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 82\nTraining loss: 0.030959714718751218 | Validation loss: 0.0395452414146241\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 83\nTraining loss: 0.03212108367833456 | Validation loss: 0.043741495951133615\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 84\nTraining loss: 0.03381063934832223 | Validation loss: 0.04081341168459724\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 85\nTraining loss: 0.035855333057330234 | Validation loss: 0.041724231225602766\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 86\nTraining loss: 0.03387873944654865 | Validation loss: 0.03887862777885269\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 87\nTraining loss: 0.03237174124898095 | Validation loss: 0.039358641645487615\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 88\nTraining loss: 0.030933335009276083 | Validation loss: 0.03714800371843226\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 89\nTraining loss: 0.029362504421978405 | Validation loss: 0.03726502972490647\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126]\n------------------------------\nEpoch: 90\nTraining loss: 0.02861379960943994 | Validation loss: 0.03534455056137899\nValidation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n 0.0368468  0.03593932 0.03572126 0.03534455]\nEarly stopping!\n--------------------------------------------------------------------------------\nSeed: 10 | Size: 40132\n--------------------------------------------------------------------------------\n------------------------------\nEpoch: 0\nTraining loss: 0.22611369123667713 | Validation loss: 0.17660081768985345\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 1\nTraining loss: 0.10115880248188151 | Validation loss: 0.08647104853813627\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 2\nTraining loss: 0.0806360789464684 | Validation loss: 0.07522331774894116\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 3\nTraining loss: 0.07291009324221454 | Validation loss: 0.06868328935409014\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 4\nTraining loss: 0.06777285844982257 | Validation loss: 0.060847439992744314\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 5\nTraining loss: 0.06530867708575597 | Validation loss: 0.05860075982600714\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 6\nTraining loss: 0.0605629275820592 | Validation loss: 0.060367112501268895\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 7\nTraining loss: 0.056550232517555005 | Validation loss: 0.0516667994546943\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 8\nTraining loss: 0.05290380347424781 | Validation loss: 0.05032209189921881\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 9\nTraining loss: 0.04968396053414821 | Validation loss: 0.046576345174581604\nValidation loss (ends of cycles): [0.17660082]\n------------------------------\nEpoch: 10\nTraining loss: 0.04691924053559622 | Validation loss: 0.04340929457241983\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 11\nTraining loss: 0.04784105510054904 | Validation loss: 0.04530351345254257\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 12\nTraining loss: 0.04946117783808626 | Validation loss: 0.04510788741496812\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 13\nTraining loss: 0.05063574022694132 | Validation loss: 0.0505708995052671\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 14\nTraining loss: 0.05166313559271571 | Validation loss: 0.05817923330561777\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 15\nTraining loss: 0.05248750614801671 | Validation loss: 0.0493360729311156\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 16\nTraining loss: 0.049662602678542646 | Validation loss: 0.05467164823043663\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 17\nTraining loss: 0.04728653871703629 | Validation loss: 0.052291345279828635\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 18\nTraining loss: 0.04459606154132488 | Validation loss: 0.05458670186983273\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 19\nTraining loss: 0.04198971974542438 | Validation loss: 0.03901625990010468\nValidation loss (ends of cycles): [0.17660082 0.04340929]\n------------------------------\nEpoch: 20\nTraining loss: 0.03987340637625701 | Validation loss: 0.03802946162105134\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 21\nTraining loss: 0.04096880447445804 | Validation loss: 0.038806912893083245\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 22\nTraining loss: 0.04220846461638163 | Validation loss: 0.0440736518167289\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 23\nTraining loss: 0.04363556982084053 | Validation loss: 0.04022814183438246\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 24\nTraining loss: 0.04542448289974732 | Validation loss: 0.046775908052789424\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 25\nTraining loss: 0.04642166053568284 | Validation loss: 0.04878175716880148\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 26\nTraining loss: 0.044440456977715405 | Validation loss: 0.04138228518113626\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 27\nTraining loss: 0.041865324622614115 | Validation loss: 0.04312394201689589\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 28\nTraining loss: 0.039823972675986 | Validation loss: 0.03708740910597607\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 29\nTraining loss: 0.03766148964009123 | Validation loss: 0.036518600266591636\nValidation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n------------------------------\nEpoch: 30\n\n\n\ndef load_all(src_dir):\n    dumps = []\n    for file in glob.glob(str(src_dir/'*.pickle')):\n        with open(file, 'rb') as f: \n            dumps.append(pickle.load(f))\n    return dumps\n\n\nsrc_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/learning_curve')\ndumps = load_all(src_dir)\n\n\ndf = pd.concat([pd.DataFrame(perf) for perf in dumps])\ngrps = df.groupby(['n_samples']).agg({'test_score':['mean','std'], 'n_epochs':['mean','std']})\ngrps.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      test_score\n      n_epochs\n    \n    \n      \n      mean\n      std\n      mean\n      std\n    \n    \n      n_samples\n      \n      \n      \n      \n    \n  \n  \n    \n      500\n      0.413549\n      0.184284\n      111.000000\n      47.749346\n    \n    \n      1000\n      0.453063\n      0.073105\n      79.333333\n      17.224014\n    \n    \n      2000\n      0.534975\n      0.051113\n      121.000000\n      33.466401\n    \n    \n      5000\n      0.607885\n      0.039500\n      87.666667\n      23.380904\n    \n    \n      10000\n      0.677188\n      0.020144\n      101.000000\n      28.284271\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngrps.columns = grps.columns.map('_'.join)\ngrps.reset_index().plot(x='n_samples', y='test_score_mean', logx=True);\n\n\n\n\n\n\nSave results\n\ndest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps')\n\nwith open(dest_dir/'cnn_test_perf_vs_n_samples.pickle', 'wb') as f: \n    pickle.dump(grps, f)"
  },
  {
    "objectID": "paper/select_transform.html#piping-data-selection-and-simple-transformation",
    "href": "paper/select_transform.html#piping-data-selection-and-simple-transformation",
    "title": "2. Select & transform",
    "section": "2.1 Piping data selection and simple transformation",
    "text": "2.1 Piping data selection and simple transformation\n\nsrc_dir = 'data'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Wavenumbers:\\n {X_names}')\nprint(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\nprint(f'Taxonomic order lookup:\\n {tax_lookup}')\n\nX shape: (50494, 1764)\ny shape: (50494,)\nWavenumbers:\n [3999 3997 3995 ...  603  601  599]\ndepth_order (first 3 rows):\n [[43.  2.]\n [ 0.  0.]\n [ 0.  1.]]\nTaxonomic order lookup:\n {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}\n\n\n\ndata = X, y, X_id, depth_order\n\ntransforms = [select_y, select_tax_order, select_X, log_transform_y]\nX, y, X_id, depth_order = compose(*transforms)(data)\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'depth_order shape: {depth_order.shape}')\n\nX shape: (40132, 1764)\ny shape: (40132,)\ndepth_order shape: (40132, 2)"
  },
  {
    "objectID": "paper/cnn_valid_curve_by_tax.html#load-and-transform",
    "href": "paper/cnn_valid_curve_by_tax.html#load-and-transform",
    "title": "4.4. Validation curve by Soil Taxonomy Orders (CNN)",
    "section": "Load and transform",
    "text": "Load and transform\n\nsrc_dir = 'data'\nfnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n          'depth-order.npy', 'target.npy', \n          'tax-order-lu.pkl', 'spectra-id.npy']\n\nX, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)"
  },
  {
    "objectID": "paper/cnn_valid_curve_by_tax.html#experiment",
    "href": "paper/cnn_valid_curve_by_tax.html#experiment",
    "title": "4.4. Validation curve by Soil Taxonomy Orders (CNN)",
    "section": "Experiment",
    "text": "Experiment\n\nUtilities\n\ndef format_tax_losses(losses, idx=None):\n    if idx is None:\n        losses_tax = []\n        for i, loss in enumerate(losses):\n            df_seed = pd.DataFrame(loss['valid_tax']).T\n            df_seed.index.name = 'tax'\n            df_seed.reset_index()\n            losses_tax.append(df_seed)\n        df = pd.concat(losses_tax)\n        return df.reset_index().groupby('tax').mean().reset_index()\n    else:\n        df_seed = pd.DataFrame(losses[idx]['valid_tax']).T\n        df_seed.index.name = 'tax'\n        df_seed.reset_index()\n        return df_seed\n\n\n\nSetup\n\ndest_dir_loss = Path('dumps/cnn/train_eval/all/losses')\nlosses = load_dumps(dest_dir_loss)\n\n\ndf_loss_all_mean = pd.DataFrame([loss['valid'] for loss in losses]).mean()\n\n\ndf_loss_all_mean.values\n\narray([0.14582814, 0.09014846, 0.07893631, 0.07276324, 0.07073818,\n       0.06707921, 0.0625751 , 0.05599464, 0.05248269, 0.04967756,\n       0.046866  , 0.04880906, 0.05141463, 0.05284351, 0.05540708,\n       0.05563474, 0.05164713, 0.04905587, 0.04571185, 0.04315375,\n       0.04110955, 0.04223788, 0.04455341, 0.04628459, 0.05034362,\n       0.05034709, 0.04645335, 0.04503993, 0.0423074 , 0.04000635,\n       0.03812311, 0.03942788, 0.04132601, 0.04227399, 0.04667591,\n       0.04809851, 0.04418473, 0.04192892, 0.04063427, 0.03810782,\n       0.03614225, 0.03727109, 0.03956102, 0.04122068, 0.04300844,\n       0.04640118, 0.04282962, 0.04091016, 0.03853729, 0.03682557,\n       0.03487694, 0.03627837, 0.03842524, 0.03981313, 0.04410538,\n       0.04434885, 0.04282881, 0.03975203, 0.03722999, 0.03535308,\n       0.03379202, 0.03589583, 0.03702463, 0.03876628, 0.04165303,\n       0.04548378, 0.04082348, 0.03925791, 0.03737891, 0.0351957 ,\n       0.03304794, 0.03445445, 0.03572285, 0.03869938, 0.04030689,\n       0.04104632, 0.04048262, 0.03780278, 0.03554481, 0.03410327,\n       0.03247798, 0.03368324, 0.03557672, 0.03649407, 0.03866227,\n       0.04052344, 0.03961371, 0.03757647, 0.0350988 , 0.03336657,\n       0.03200022, 0.03318095, 0.03569654, 0.0362442 , 0.03761844,\n       0.04199123, 0.03784717, 0.03650881, 0.0344024 , 0.03328214,\n       0.03169661, 0.03307035, 0.0344853 , 0.03680656, 0.03922563,\n       0.04119334, 0.03634852, 0.03607549, 0.03402154, 0.03302471,\n       0.03142086, 0.0328199 , 0.03437598, 0.03562572, 0.03644703,\n       0.03928935, 0.03724415, 0.0347058 , 0.0335458 , 0.03231729,\n       0.03109171, 0.03247988, 0.0336959 , 0.03519582, 0.035883  ,\n       0.03786512, 0.03678176, 0.0361663 , 0.03366792, 0.03258806,\n       0.03082712, 0.03266753, 0.03334443, 0.0340696 , 0.03588395,\n       0.03690009, 0.03612227, 0.0347346 , 0.03323   , 0.032378  ,\n       0.03065846, 0.03213505, 0.0328987 , 0.03498865, 0.03602875,\n       0.03856968, 0.03543443, 0.0344587 , 0.03336348, 0.03221112,\n       0.0305272 , 0.03203912, 0.03337082, 0.03419619, 0.03735174,\n       0.03675175, 0.03624353, 0.03369276, 0.03310249, 0.03192003,\n       0.03038229, 0.03181916, 0.03348516, 0.03550255, 0.03607852,\n       0.03806242, 0.03534237, 0.03439606, 0.03301949, 0.03169708,\n       0.03017788, 0.03145204, 0.03313161, 0.03380575, 0.03539771,\n       0.03568755, 0.03461054, 0.03325683, 0.03267145, 0.03180435,\n       0.03008223, 0.0319777 , 0.03314908, 0.03342559, 0.03488767,\n       0.03633015, 0.03424385, 0.03341905, 0.03243577, 0.0317504 ,\n       0.02996512, 0.0314795 , 0.0332628 , 0.03418779, 0.03427937,\n       0.0358278 , 0.03570055, 0.03365938, 0.03267484, 0.03144413,\n       0.02991178])\n\n\n\n# Exclude \"Oxisols\" as too few samples\ntax_of_interest = {k: v for k, v in tax_lookup.items() if k != 'oxisols'}\ntax_of_interest\n\n{'alfisols': 0,\n 'mollisols': 1,\n 'inceptisols': 2,\n 'entisols': 3,\n 'spodosols': 4,\n 'undefined': 5,\n 'ultisols': 6,\n 'andisols': 7,\n 'histosols': 8,\n 'vertisols': 10,\n 'aridisols': 11,\n 'gelisols': 12}\n\n\n\ndf = format_tax_losses(losses, idx=None).filter(items=tax_of_interest.values(), axis=0); df\n\n\n\n\n\n  \n    \n      \n      tax\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      ...\n      191\n      192\n      193\n      194\n      195\n      196\n      197\n      198\n      199\n      200\n    \n  \n  \n    \n      0\n      0\n      0.136776\n      0.056102\n      0.051624\n      0.049182\n      0.046964\n      0.046138\n      0.043792\n      0.039819\n      0.038829\n      ...\n      0.026873\n      0.027761\n      0.028256\n      0.028500\n      0.029622\n      0.029936\n      0.027817\n      0.027758\n      0.026603\n      0.025761\n    \n    \n      1\n      1\n      0.108970\n      0.074844\n      0.066805\n      0.060365\n      0.060410\n      0.057207\n      0.053487\n      0.046880\n      0.044224\n      ...\n      0.025806\n      0.027472\n      0.028190\n      0.028275\n      0.029406\n      0.029591\n      0.027757\n      0.026725\n      0.025784\n      0.024443\n    \n    \n      2\n      2\n      0.179343\n      0.080996\n      0.071688\n      0.070212\n      0.064672\n      0.065054\n      0.060657\n      0.055836\n      0.054061\n      ...\n      0.036132\n      0.037807\n      0.038594\n      0.039017\n      0.039383\n      0.039225\n      0.038377\n      0.037397\n      0.036353\n      0.034852\n    \n    \n      3\n      3\n      0.166356\n      0.087728\n      0.078601\n      0.075305\n      0.066709\n      0.067684\n      0.064100\n      0.057511\n      0.054260\n      ...\n      0.029942\n      0.031722\n      0.032132\n      0.032915\n      0.035358\n      0.035649\n      0.031892\n      0.030620\n      0.030083\n      0.028646\n    \n    \n      4\n      4\n      0.197614\n      0.083122\n      0.072006\n      0.068903\n      0.068574\n      0.066645\n      0.065523\n      0.062196\n      0.057514\n      ...\n      0.039557\n      0.039785\n      0.040462\n      0.042845\n      0.044306\n      0.044326\n      0.041461\n      0.040120\n      0.039678\n      0.039088\n    \n    \n      5\n      5\n      0.153747\n      0.107503\n      0.092829\n      0.084751\n      0.082516\n      0.076731\n      0.070696\n      0.062839\n      0.057833\n      ...\n      0.032669\n      0.034629\n      0.035730\n      0.035848\n      0.037953\n      0.037656\n      0.035190\n      0.034110\n      0.032548\n      0.030803\n    \n    \n      6\n      6\n      0.152534\n      0.064571\n      0.060710\n      0.058852\n      0.059420\n      0.059187\n      0.055980\n      0.054683\n      0.053309\n      ...\n      0.038558\n      0.039243\n      0.041479\n      0.041662\n      0.043007\n      0.042023\n      0.040298\n      0.040001\n      0.038738\n      0.037550\n    \n    \n      7\n      7\n      0.169730\n      0.083335\n      0.069036\n      0.065518\n      0.060544\n      0.063257\n      0.060464\n      0.051612\n      0.047301\n      ...\n      0.029281\n      0.031648\n      0.032247\n      0.031448\n      0.033532\n      0.033072\n      0.031675\n      0.029762\n      0.028676\n      0.026958\n    \n    \n      8\n      8\n      0.293496\n      0.203235\n      0.151515\n      0.138089\n      0.133604\n      0.122406\n      0.117703\n      0.106278\n      0.100228\n      ...\n      0.064171\n      0.067258\n      0.070667\n      0.069261\n      0.071917\n      0.070003\n      0.069514\n      0.068517\n      0.065218\n      0.062353\n    \n    \n      10\n      10\n      0.106765\n      0.070213\n      0.062454\n      0.057815\n      0.055850\n      0.050079\n      0.046619\n      0.039625\n      0.037970\n      ...\n      0.022706\n      0.024911\n      0.024899\n      0.025516\n      0.026076\n      0.025788\n      0.025336\n      0.023495\n      0.023181\n      0.021947\n    \n    \n      11\n      11\n      0.132431\n      0.100199\n      0.091722\n      0.084992\n      0.084716\n      0.081571\n      0.074755\n      0.068620\n      0.063533\n      ...\n      0.037069\n      0.039889\n      0.041584\n      0.039900\n      0.042083\n      0.039988\n      0.039025\n      0.038969\n      0.037147\n      0.034838\n    \n    \n      12\n      12\n      0.269967\n      0.159637\n      0.136138\n      0.130422\n      0.111166\n      0.105764\n      0.104931\n      0.086638\n      0.082811\n      ...\n      0.050695\n      0.054884\n      0.057524\n      0.055711\n      0.055529\n      0.056401\n      0.056093\n      0.053794\n      0.052021\n      0.049314\n    \n  \n\n12 rows × 202 columns\n\n\n\n\nindexes = [i for i in range(len(losses[0]['valid_tax'])) if not i%10]\n\n\ndf_selected_tax = df[indexes[2:]].T\ndf_selected_all = df_loss_all_mean[indexes[2:]]\n\ndf_selected_tax\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      10\n      11\n      12\n    \n  \n  \n    \n      20\n      0.032607\n      0.034638\n      0.045875\n      0.041617\n      0.046397\n      0.043431\n      0.047601\n      0.036641\n      0.081370\n      0.029222\n      0.048862\n      0.061913\n    \n    \n      30\n      0.030932\n      0.032115\n      0.043283\n      0.037829\n      0.043756\n      0.039932\n      0.045231\n      0.034765\n      0.076238\n      0.027701\n      0.044190\n      0.058242\n    \n    \n      40\n      0.029718\n      0.030283\n      0.041323\n      0.035609\n      0.042389\n      0.037653\n      0.043479\n      0.032929\n      0.073283\n      0.027140\n      0.041524\n      0.055829\n    \n    \n      50\n      0.029158\n      0.029090\n      0.040370\n      0.034228\n      0.040828\n      0.036120\n      0.042232\n      0.032216\n      0.071332\n      0.026365\n      0.040523\n      0.054847\n    \n    \n      60\n      0.028400\n      0.028178\n      0.039149\n      0.032863\n      0.040171\n      0.034912\n      0.040847\n      0.031589\n      0.070367\n      0.025167\n      0.039058\n      0.053372\n    \n    \n      70\n      0.027902\n      0.027529\n      0.038346\n      0.031901\n      0.039783\n      0.034051\n      0.040239\n      0.031052\n      0.068467\n      0.024631\n      0.038168\n      0.052347\n    \n    \n      80\n      0.027659\n      0.026991\n      0.037488\n      0.030898\n      0.039623\n      0.033449\n      0.039798\n      0.030357\n      0.067050\n      0.024588\n      0.037636\n      0.051267\n    \n    \n      90\n      0.027229\n      0.026614\n      0.036943\n      0.030656\n      0.039570\n      0.032891\n      0.039234\n      0.030320\n      0.066097\n      0.023612\n      0.036969\n      0.050530\n    \n    \n      100\n      0.027036\n      0.026320\n      0.036623\n      0.029997\n      0.039183\n      0.032602\n      0.038751\n      0.029793\n      0.065453\n      0.023484\n      0.035918\n      0.051719\n    \n    \n      110\n      0.026978\n      0.026052\n      0.036248\n      0.029669\n      0.039254\n      0.032338\n      0.038368\n      0.029803\n      0.064705\n      0.023107\n      0.036339\n      0.050241\n    \n    \n      120\n      0.026698\n      0.025662\n      0.035772\n      0.029381\n      0.039068\n      0.031964\n      0.038495\n      0.029521\n      0.063948\n      0.023047\n      0.036142\n      0.050535\n    \n    \n      130\n      0.026518\n      0.025433\n      0.035409\n      0.028974\n      0.038853\n      0.031759\n      0.038028\n      0.028710\n      0.063033\n      0.022909\n      0.035758\n      0.050205\n    \n    \n      140\n      0.026253\n      0.025275\n      0.035423\n      0.029010\n      0.038941\n      0.031570\n      0.038232\n      0.028489\n      0.062779\n      0.022545\n      0.035582\n      0.049381\n    \n    \n      150\n      0.026318\n      0.025069\n      0.035212\n      0.028338\n      0.038775\n      0.031421\n      0.038360\n      0.028602\n      0.062334\n      0.022152\n      0.035817\n      0.049453\n    \n    \n      160\n      0.026169\n      0.025002\n      0.035056\n      0.028403\n      0.039017\n      0.031195\n      0.037916\n      0.028707\n      0.062255\n      0.022264\n      0.035208\n      0.050368\n    \n    \n      170\n      0.026144\n      0.024773\n      0.034726\n      0.028882\n      0.038378\n      0.030972\n      0.038065\n      0.027879\n      0.061568\n      0.021844\n      0.035046\n      0.050528\n    \n    \n      180\n      0.026045\n      0.024655\n      0.034677\n      0.028401\n      0.038746\n      0.030909\n      0.037958\n      0.028300\n      0.061259\n      0.021814\n      0.034687\n      0.049658\n    \n    \n      190\n      0.025905\n      0.024487\n      0.034526\n      0.028732\n      0.039135\n      0.030784\n      0.037352\n      0.027796\n      0.060807\n      0.021915\n      0.034836\n      0.049858\n    \n    \n      200\n      0.025761\n      0.024443\n      0.034852\n      0.028646\n      0.039088\n      0.030803\n      0.037550\n      0.026958\n      0.062353\n      0.021947\n      0.034838\n      0.049314"
  },
  {
    "objectID": "paper/cnn_valid_curve_by_tax.html#plot",
    "href": "paper/cnn_valid_curve_by_tax.html#plot",
    "title": "4.4. Validation curve by Soil Taxonomy Orders (CNN)",
    "section": "Plot",
    "text": "Plot\n\ndef plot_val_losses_tax(df_tax, df_all, tax_lookup,\n                        figsize=(16*centimeter, 10*centimeter), dpi=600):\n    # Layout \n    fig = plt.figure(figsize=figsize, dpi=600)\n    gs = GridSpec(nrows=1, ncols=1)\n    ax = fig.add_subplot(gs[0, 0])\n\n    cc = (cycler(color=[f'C{i}' for i in range(7)]) *\n          cycler(linestyle=['-', '--']))\n\n    epochs = df_tax.index.to_numpy()\n    # Plots\n    for tax_label, tax_idx in tax_lookup.items():\n        #ax.plot(epochs, np.mean(deltas[:,:,tax_idx], axis=0), \n        ax.plot(epochs, df_tax.loc[:, tax_idx], \n                label=tax_label.capitalize(),\n                **list(cc)[tax_idx])\n    \n    ax.plot(epochs, df_all, \n                label='All',\n                c='black')\n    # Ornaments\n    ax.legend(loc='best', frameon=False, ncol=3) \n    #ax.set_ylabel('$MSE_{test} - MSE_{train}$ →', loc='top')\n    ax.set_ylabel('Validation loss (MSE) →', loc='top')\n    ax.set_xlabel('Number of epochs →', loc='right')\n    ax.grid(True, \"minor\", color=\"0.85\", linewidth=0.2, zorder=-2)\n    ax.grid(True, \"major\", color=\"0.65\", linewidth=0.4, zorder=-1) \n    plt.tight_layout()\n\n\n#FIG_PATH = Path('nameofyourfolder')\nFIG_PATH = Path('images')\nset_style(DEFAULT_STYLE)\nplot_val_losses_tax(df_selected_tax, df_selected_all, tax_of_interest)\n\n# To save/export it\nplt.savefig(FIG_PATH/'validation_loss_tax.png', dpi=600, transparent=True, format='png')"
  }
]