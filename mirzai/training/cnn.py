# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/api/training/cnn.ipynb.

# %% auto 0
__all__ = ['Model', 'weights_init', 'Learner', 'Learners']

# %% ../../nbs/api/training/cnn.ipynb 4
# Python utils
from collections import OrderedDict
import pickle
from tqdm.auto import tqdm
from functools import partial, reduce
import operator
from pathlib import Path
import math
import glob
import re


import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

from ..data.loading import load_kssl
from ..data.selection import (select_y, select_tax_order, select_X)
from ..data.transform import log_transform_y
from ..data.torch import DataLoaders, SNV_transform
from .metrics import eval_reg
from .core import is_plateau

# Deep Learning stack
import torch
from torch import nn
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from torch.optim import Adam
from torch.optim.lr_scheduler import CyclicLR
from torch.nn import MSELoss

from fastcore.test import *
from fastcore.basics import store_attr
from fastcore.transform import compose

import warnings
warnings.filterwarnings('ignore')

# %% ../../nbs/api/training/cnn.ipynb 5
# https://datascience.stackexchange.com/questions/40906/determining-size-of-fc-layer-after-conv-layer-in-pytorch
class Model(nn.Module):
    def __init__(self, input_dim, in_channel=1, out_channel=16, is_classifier=False,
                 dropout=0.4):
        super(Model, self).__init__()
        # Build the neural network
        self.feature_extractor = nn.Sequential(
            self.make_convpool_block(in_channel, out_channel),
            self.make_convpool_block(out_channel, out_channel*2),
            self.make_convpool_block(out_channel*2, out_channel*4),
            self.make_convpool_block(out_channel*4, out_channel*8),
            self.make_convpool_block(out_channel*8, out_channel*16))

        num_features_before_fcnn = reduce(operator.mul,
                                          self.feature_extractor(torch.rand(1, in_channel, input_dim)).shape)

        output_layers = [nn.Dropout(dropout),
                         nn.Linear(in_features=num_features_before_fcnn, out_features=20),
                         nn.BatchNorm1d(20),
                         nn.LeakyReLU(0.1),
                         nn.Linear(in_features=20, out_features=1)];

        if is_classifier:
            output_layers.append(nn.Sigmoid())

        self.output_block = nn.Sequential(*output_layers)

    def make_convpool_block(self, input_channels, output_channels, kernel_size=3, stride=1):
        return nn.Sequential(
            nn.Conv1d(input_channels, output_channels, kernel_size, bias=False),
            nn.BatchNorm1d(output_channels),
            nn.LeakyReLU(0.1),
            nn.AvgPool1d(3))

    def forward(self, x):
        batch_size = x.size(0)
        out = self.feature_extractor(x)
        out = out.view(batch_size, -1)  # flatten the vector
        out = self.output_block(out)
        return out

# %% ../../nbs/api/training/cnn.ipynb 6
def weights_init(m):
    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):
        torch.nn.init.kaiming_uniform_(m.weight)
    if isinstance(m, nn.BatchNorm1d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
        torch.nn.init.constant_(m.bias, 0)

# %% ../../nbs/api/training/cnn.ipynb 7
class Learner():
    def __init__(self, 
                 model, 
                 criterion=nn.MSELoss(), 
                 opt=torch.optim.Adam, 
                 n_epochs=50,
                 scheduler=None, 
                 early_stopper= None, 
                 tax_lookup=range(13), 
                 verbose=True
                ):
        store_attr() # see https://fastpages.fast.ai/fastcore
        use_cuda = torch.cuda.is_available()
        self.device = torch.device('cuda:0' if use_cuda else 'cpu')
        self._init_losses()

    def fit(self, 
            training_generator, 
            validation_generator,
            losses_append=None
           ):
        if losses_append is None: self._init_losses()
            
        for epoch in range(self.n_epochs):
            print(30*'-')
            print(f'Epoch: {epoch}')
            loss_train = []
            loss_valid = []
            
            loss_valid_tax = [[] for i in range(len(self.tax_lookup))]
            
            # Training loop
            self.model.train()
            for batches in training_generator:
                local_batch, local_labels, _ = self._to_device(batches)
                if len(local_batch) > 1:
                    self.opt.zero_grad() 
                    y_hat, loss = self.eval(local_batch, local_labels)
                    loss.backward(retain_graph=True) 
                    self.opt.step()
                    loss_train.append(loss.item())


            # Validation loop
            self.model.eval()
            with torch.no_grad():
                for batches in validation_generator:
                    local_batch, local_labels, local_tax = self._to_device(batches)
                    if len(local_batch) > 1:
                        y_hat, loss = self.eval(local_batch, local_labels)
                        loss_valid.append(loss.item())
                    
                        # Eval by tax order
                        for tax in self.tax_lookup:
                            mask = (local_tax == tax).squeeze()
                            if torch.sum(mask).item() == 0:
                                loss_valid_tax[tax].append(float('nan'))
                            else:
                                _, loss = self.eval(local_batch[mask, :, :], local_labels[mask, :])
                                loss_valid_tax[tax].append(loss.item())
                            
            # mean losses after an epoch
            mean_loss_train = np.mean(np.array(loss_train))
            mean_loss_valid = np.mean(np.array(loss_valid))
            mean_loss_valid_tax = np.nanmean(np.array(loss_valid_tax), axis=1)
            
            if self.scheduler is not None:
                self.scheduler.step()

            if self.verbose:
                print(f'Training loss: {mean_loss_train} | Validation loss: {mean_loss_valid}')

            self.losses['train'].append(mean_loss_train)
            self.losses['valid'].append(mean_loss_valid)
            self.losses['valid_tax'].append(mean_loss_valid_tax)

            # At the end of a Cyclic learning rate cycle check if levelling
            losses_end_cycle = np.array(
                [el 
                 for i, el in enumerate(self.losses['valid']) 
                 if not i % self.scheduler.total_size])
            print(f'Validation loss (ends of cycles): {losses_end_cycle}')
            if self.early_stopper(losses_end_cycle):
                print('Early stopping!')
                
                break
            
        return self.model, self.losses

    def lr_finder(self,
                  training_generator,
                  start=1e-6,
                  end=1e-1,
                  n_epochs=8
                 ):
        opt = torch.optim.Adam(self.model.parameters(), lr=start)
        lr_lambda = lambda x: math.exp(x * math.log(end / start) / (n_epochs * len(training_generator)))
        scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)
        criterion = nn.MSELoss()
        
        lrs = []
        losses = []
        i = 0
        smoothing = 0.01
        for epoch in range(n_epochs):
            print(f'Epoch: {epoch}')
            self.model.train()
            for batches in training_generator:
                local_batch, local_labels, local_tax = self._to_device(batches)
                if len(local_batch) > 1:
                    opt.zero_grad()
                    y_hat = self.model(local_batch)
                    loss = criterion(y_hat, local_labels)
                    loss.backward()
                    opt.step()

                    # update lr
                    scheduler.step()
                    lr_step = opt.state_dict()["param_groups"][0]["lr"]
                    lrs.append(lr_step)

                    # smooth loss
                    if i == 0:
                        losses.append(loss.item())
                    else:
                        loss = smoothing  * loss + (1 - smoothing) * losses[-1]
                        losses.append(loss.item())
                    i += 1
        return lrs, losses
        
    def eval(self, 
             X, 
             y
            ):
        y_hat = self.model(X)
        loss = self.criterion(y_hat, y)
        return y_hat, loss

    def predict(self, 
                data_generator,
                sample=None
               ):
        self.model.eval() if sample is None else self._MCDropoutOn()
        ys_hat, ys = [], []
        if sample is not None: raise Exception(f'MCDropout needs refactoring')
        with torch.no_grad():
            for batches in data_generator:
                local_batch, local_labels, _ = self._to_device(batches)
                if sample is None:
                    ys_hat.append(self.model(local_batch))
                else:
                    # TO BE FIXED: MCDROPOUT
                    y_hat = np.stack([self.model(local_batch).cpu() for s in tqdm(range(sample))]).squeeze()
                ys.append(local_labels)
        return [self._npify(torch.cat(y)) for y in (ys_hat, ys)]

    def _to_device(self, 
                   batches
                  ):
        if self.device.type == 'cpu': return batches
        return (batch.to(self.device) for batch in batches)

    def _init_losses(self):
        self.losses = {'train': [], 'valid': [], 'valid_tax': []}

    def _MCDropoutOn(self):
         # Check https://discuss.pytorch.org/t/turn-off-batch-norm-but-leave-dropout-on/14815
        # to deactivate BatchNorm during inference (maybe not critical when predicting all set)
        # and
        # for m in model.modules():
        #   if isinstance(m, nn.BatchNorm1d):
        #     m.eval()
        self.model.train()
        for m in self.model.modules():
            if isinstance(m, nn.BatchNorm1d):
                m.eval()
    def _npify(self, 
               tensor
              ):
        if self.device.type == 'cpu': return tensor.numpy()
        return tensor.cpu().numpy()

# %% ../../nbs/api/training/cnn.ipynb 8
class Learners():
    def __init__(self, 
                 model,
                 tax_lookup,
                 seeds=range(20), 
                 device='cpu',
                 verbose=True, 
                 split_ratio=0.1):
        store_attr() # see https://fastpages.fast.ai/fastcore
         
    def train(self,
              data,
              order=None,
              dest_dir_loss='',
              dest_dir_model='',
              optimizer=torch.optim.Adam,
              scheduler=torch.optim.lr_scheduler.CyclicLR,
              criterion=nn.MSELoss(),
              n_epochs=201,
              early_stop=1e-4,
              sc_kwargs={}):
        
        X, y, tax_order = data                
        for seed in self.seeds:
            print(80*'-')
            print(f'Seed: {seed}')
            print(80*'-')

            generators = self._get_generators((X, y, tax_order), seed, order=order)
            training_generator, validation_generator, test_generator  = generators

            # Modeling
            model = self.model(X.shape[1], out_channel=16).to(device)
            opt = optimizer(model.parameters())
            model = model.apply(weights_init)
            early_stopper = partial(is_plateau, delta=early_stop, verbose=False)
            learner = Learner(model, criterion, opt, n_epochs=n_epochs,
                              scheduler=scheduler(opt, **sc_kwargs), 
                              early_stopper=early_stopper,
                              tax_lookup=self.tax_lookup.values(),
                              verbose=self.verbose)
            
            model, losses = learner.fit(training_generator, validation_generator)

            with open(dest_dir_loss/f'cnn-loss-seed-{seed}.pickle', 'wb') as f: 
                pickle.dump(losses, f)

            torch.save(model.state_dict(), dest_dir_model/f'model-seed-{seed}.pt')
    
    def evaluate(self,
                 data,
                 order=None,
                 batch_size=32,
                 src_dir_model=''):
        X, y, tax_order = data
        perfs = []
        y_hats = []
        y_trues = []
        ns = []
        for fname in glob.glob(str(src_dir_model/'*.pt')):
            model = Model(X.shape[1], out_channel=16).to(device)
            model.load_state_dict(torch.load(fname))
            learner = Learner(model, tax_lookup=self.tax_lookup.values())
            seed = int(re.search(r'-(\d+)\.', fname).group(1))
            generators = self._get_generators((X, y, tax_order), seed, order=order)
            training_generator, validation_generator, test_generator  = generators
            nb_data_gen = np.sum(1 for _ in test_generator)
            if nb_data_gen > 1:
                y_hat, y_true = learner.predict(test_generator)
                perfs.append(eval_reg(y_true, y_hat))
                y_hats.append(y_hat.ravel())
                y_trues.append(y_true.ravel())
                ns.append(nb_data_gen*batch_size)
        return pd.DataFrame(perfs), pd.DataFrame(y_hats).T, pd.DataFrame(y_trues).T, pd.DataFrame(ns)
            
    
    def _get_generators(self, data, seed, batch_size=32, order=None):
        X, y, tax_order = data
        # Train/test split
        data = train_test_split(X, 
                                y, 
                                tax_order, 
                                test_size=self.split_ratio,
                                random_state=seed)
        X_train, X_test, y_train, y_test, tax_order_train, tax_order_test = data
        data_test = X_test, y_test, tax_order_test

        # Further train/valid split
        data = train_test_split(X_train, 
                                y_train,
                                tax_order_train,
                                test_size=self.split_ratio, 
                                random_state=seed)
        X_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = data
        data_train = X_train, y_train, tax_order_train
        data_valid = X_valid, y_valid, tax_order_valid

        if order is not None:
            data_train, data_valid, data_test = [self._filter(data, order=order) 
                                                 for data in [data_train, data_valid, data_test]]
           
        dls = DataLoaders(data_train, 
                          data_valid,
                          data_test,
                          transform=SNV_transform(),
                          batch_size=batch_size)

        training_generator, validation_generator, test_generator = dls.loaders()
        for batches in validation_generator:
            local_batch, local_labels, local_tax = batches
        return training_generator, validation_generator, test_generator
    
    def _filter(self, data, order=None):
        X, y, tax_order = data
        mask = tax_order == order
        return X[mask, :], y[mask], tax_order[mask]
