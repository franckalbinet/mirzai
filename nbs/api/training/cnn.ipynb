{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & validation (CNN)\n",
    "\n",
    "> Various utilities function to train and evaluate the Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',  force_remount=False)\n",
    "    !pip install mirzai\n",
    "else:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python utils\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial, reduce\n",
    "import operator\n",
    "from pathlib import Path\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mirzai.data.loading import load_kssl\n",
    "from mirzai.data.selection import (select_y, select_tax_order, select_X)\n",
    "from mirzai.data.transform import log_transform_y\n",
    "from mirzai.data.torch import DataLoaders, SNV_transform\n",
    "from mirzai.training.metrics import eval_reg\n",
    "from mirzai.training.core import is_plateau\n",
    "\n",
    "# Deep Learning stack\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from fastcore.test import *\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.transform import compose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# https://datascience.stackexchange.com/questions/40906/determining-size-of-fc-layer-after-conv-layer-in-pytorch\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, in_channel=1, out_channel=16, is_classifier=False,\n",
    "                 dropout=0.4):\n",
    "        super(Model, self).__init__()\n",
    "        # Build the neural network\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.make_convpool_block(in_channel, out_channel),\n",
    "            self.make_convpool_block(out_channel, out_channel*2),\n",
    "            self.make_convpool_block(out_channel*2, out_channel*4),\n",
    "            self.make_convpool_block(out_channel*4, out_channel*8),\n",
    "            self.make_convpool_block(out_channel*8, out_channel*16))\n",
    "\n",
    "        num_features_before_fcnn = reduce(operator.mul,\n",
    "                                          self.feature_extractor(torch.rand(1, in_channel, input_dim)).shape)\n",
    "\n",
    "        output_layers = [nn.Dropout(dropout),\n",
    "                         nn.Linear(in_features=num_features_before_fcnn, out_features=20),\n",
    "                         nn.BatchNorm1d(20),\n",
    "                         nn.LeakyReLU(0.1),\n",
    "                         nn.Linear(in_features=20, out_features=1)];\n",
    "\n",
    "        if is_classifier:\n",
    "            output_layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.output_block = nn.Sequential(*output_layers)\n",
    "\n",
    "    def make_convpool_block(self, input_channels, output_channels, kernel_size=3, stride=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(input_channels, output_channels, kernel_size, bias=False),\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.AvgPool1d(3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out = self.feature_extractor(x)\n",
    "        out = out.view(batch_size, -1)  # flatten the vector\n",
    "        out = self.output_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "    if isinstance(m, nn.BatchNorm1d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 criterion=nn.MSELoss(), \n",
    "                 opt=torch.optim.Adam, \n",
    "                 n_epochs=50,\n",
    "                 scheduler=None, \n",
    "                 early_stopper= None, \n",
    "                 tax_lookup=range(13), \n",
    "                 verbose=True\n",
    "                ):\n",
    "        store_attr() # see https://fastpages.fast.ai/fastcore\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "        self._init_losses()\n",
    "\n",
    "    def fit(self, \n",
    "            training_generator, \n",
    "            validation_generator,\n",
    "            losses_append=None\n",
    "           ):\n",
    "        if losses_append is None: self._init_losses()\n",
    "            \n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(30*'-')\n",
    "            print(f'Epoch: {epoch}')\n",
    "            loss_train = []\n",
    "            loss_valid = []\n",
    "            \n",
    "            loss_valid_tax = [[] for i in range(len(self.tax_lookup))]\n",
    "            \n",
    "            # Training loop\n",
    "            self.model.train()\n",
    "            for batches in training_generator:\n",
    "                local_batch, local_labels, _ = self._to_device(batches)\n",
    "                if len(local_batch) > 1:\n",
    "                    self.opt.zero_grad() \n",
    "                    y_hat, loss = self.eval(local_batch, local_labels)\n",
    "                    loss.backward(retain_graph=True) \n",
    "                    self.opt.step()\n",
    "                    loss_train.append(loss.item())\n",
    "\n",
    "\n",
    "            # Validation loop\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batches in validation_generator:\n",
    "                    local_batch, local_labels, local_tax = self._to_device(batches)\n",
    "                    if len(local_batch) > 1:\n",
    "                        y_hat, loss = self.eval(local_batch, local_labels)\n",
    "                        loss_valid.append(loss.item())\n",
    "                    \n",
    "                        # Eval by tax order\n",
    "                        for tax in self.tax_lookup:\n",
    "                            mask = (local_tax == tax).squeeze()\n",
    "                            if torch.sum(mask).item() == 0:\n",
    "                                loss_valid_tax[tax].append(float('nan'))\n",
    "                            else:\n",
    "                                _, loss = self.eval(local_batch[mask, :, :], local_labels[mask, :])\n",
    "                                loss_valid_tax[tax].append(loss.item())\n",
    "                            \n",
    "            # mean losses after an epoch\n",
    "            mean_loss_train = np.mean(np.array(loss_train))\n",
    "            mean_loss_valid = np.mean(np.array(loss_valid))\n",
    "            mean_loss_valid_tax = np.nanmean(np.array(loss_valid_tax), axis=1)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'Training loss: {mean_loss_train} | Validation loss: {mean_loss_valid}')\n",
    "\n",
    "            self.losses['train'].append(mean_loss_train)\n",
    "            self.losses['valid'].append(mean_loss_valid)\n",
    "            self.losses['valid_tax'].append(mean_loss_valid_tax)\n",
    "\n",
    "            # At the end of a Cyclic learning rate cycle check if levelling\n",
    "            losses_end_cycle = np.array(\n",
    "                [el \n",
    "                 for i, el in enumerate(self.losses['valid']) \n",
    "                 if not i % self.scheduler.total_size])\n",
    "            print(f'Validation loss (ends of cycles): {losses_end_cycle}')\n",
    "            if self.early_stopper(losses_end_cycle):\n",
    "                print('Early stopping!')\n",
    "                \n",
    "                break\n",
    "            \n",
    "        return self.model, self.losses\n",
    "\n",
    "    def lr_finder(self,\n",
    "                  training_generator,\n",
    "                  start=1e-6,\n",
    "                  end=1e-1,\n",
    "                  n_epochs=8\n",
    "                 ):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=start)\n",
    "        lr_lambda = lambda x: math.exp(x * math.log(end / start) / (n_epochs * len(training_generator)))\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        lrs = []\n",
    "        losses = []\n",
    "        i = 0\n",
    "        smoothing = 0.01\n",
    "        for epoch in range(n_epochs):\n",
    "            print(f'Epoch: {epoch}')\n",
    "            self.model.train()\n",
    "            for batches in training_generator:\n",
    "                local_batch, local_labels, local_tax = self._to_device(batches)\n",
    "                if len(local_batch) > 1:\n",
    "                    opt.zero_grad()\n",
    "                    y_hat = self.model(local_batch)\n",
    "                    loss = criterion(y_hat, local_labels)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                    # update lr\n",
    "                    scheduler.step()\n",
    "                    lr_step = opt.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "                    lrs.append(lr_step)\n",
    "\n",
    "                    # smooth loss\n",
    "                    if i == 0:\n",
    "                        losses.append(loss.item())\n",
    "                    else:\n",
    "                        loss = smoothing  * loss + (1 - smoothing) * losses[-1]\n",
    "                        losses.append(loss.item())\n",
    "                    i += 1\n",
    "        return lrs, losses\n",
    "        \n",
    "    def eval(self, \n",
    "             X, \n",
    "             y\n",
    "            ):\n",
    "        y_hat = self.model(X)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def predict(self, \n",
    "                data_generator,\n",
    "                sample=None\n",
    "               ):\n",
    "        self.model.eval() if sample is None else self._MCDropoutOn()\n",
    "        ys_hat, ys = [], []\n",
    "        if sample is not None: raise Exception(f'MCDropout needs refactoring')\n",
    "        with torch.no_grad():\n",
    "            for batches in data_generator:\n",
    "                local_batch, local_labels, _ = self._to_device(batches)\n",
    "                if sample is None:\n",
    "                    ys_hat.append(self.model(local_batch))\n",
    "                else:\n",
    "                    # TO BE FIXED: MCDROPOUT\n",
    "                    y_hat = np.stack([self.model(local_batch).cpu() for s in tqdm(range(sample))]).squeeze()\n",
    "                ys.append(local_labels)\n",
    "        return [self._npify(torch.cat(y)) for y in (ys_hat, ys)]\n",
    "\n",
    "    def _to_device(self, \n",
    "                   batches\n",
    "                  ):\n",
    "        if self.device.type == 'cpu': return batches\n",
    "        return (batch.to(self.device) for batch in batches)\n",
    "\n",
    "    def _init_losses(self):\n",
    "        self.losses = {'train': [], 'valid': [], 'valid_tax': []}\n",
    "\n",
    "    def _MCDropoutOn(self):\n",
    "         # Check https://discuss.pytorch.org/t/turn-off-batch-norm-but-leave-dropout-on/14815\n",
    "        # to deactivate BatchNorm during inference (maybe not critical when predicting all set)\n",
    "        # and\n",
    "        # for m in model.modules():\n",
    "        #   if isinstance(m, nn.BatchNorm1d):\n",
    "        #     m.eval()\n",
    "        self.model.train()\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.eval()\n",
    "    def _npify(self, \n",
    "               tensor\n",
    "              ):\n",
    "        if self.device.type == 'cpu': return tensor.numpy()\n",
    "        return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learners():\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tax_lookup,\n",
    "                 seeds=range(20), \n",
    "                 device='cpu',\n",
    "                 verbose=True, \n",
    "                 split_ratio=0.1):\n",
    "        store_attr() # see https://fastpages.fast.ai/fastcore\n",
    "         \n",
    "    def train(self,\n",
    "              data,\n",
    "              order=None,\n",
    "              dest_dir_loss='',\n",
    "              dest_dir_model='',\n",
    "              optimizer=torch.optim.Adam,\n",
    "              scheduler=torch.optim.lr_scheduler.CyclicLR,\n",
    "              criterion=nn.MSELoss(),\n",
    "              n_epochs=201,\n",
    "              early_stop=1e-4,\n",
    "              sc_kwargs={}):\n",
    "        \n",
    "        X, y, tax_order = data                \n",
    "        for seed in self.seeds:\n",
    "            print(80*'-')\n",
    "            print(f'Seed: {seed}')\n",
    "            print(80*'-')\n",
    "\n",
    "            generators = self._get_generators((X, y, tax_order), seed, order=order)\n",
    "            training_generator, validation_generator, test_generator  = generators\n",
    "\n",
    "            # Modeling\n",
    "            model = self.model(X.shape[1], out_channel=16).to(device)\n",
    "            opt = optimizer(model.parameters())\n",
    "            model = model.apply(weights_init)\n",
    "            early_stopper = partial(is_plateau, delta=early_stop, verbose=False)\n",
    "            learner = Learner(model, criterion, opt, n_epochs=n_epochs,\n",
    "                              scheduler=scheduler(opt, **sc_kwargs), \n",
    "                              early_stopper=early_stopper,\n",
    "                              tax_lookup=self.tax_lookup.values(),\n",
    "                              verbose=self.verbose)\n",
    "            \n",
    "            model, losses = learner.fit(training_generator, validation_generator)\n",
    "\n",
    "            with open(dest_dir_loss/f'cnn-loss-seed-{seed}.pickle', 'wb') as f: \n",
    "                pickle.dump(losses, f)\n",
    "\n",
    "            torch.save(model.state_dict(), dest_dir_model/f'model-seed-{seed}.pt')\n",
    "    \n",
    "    def evaluate(self,\n",
    "                 data,\n",
    "                 order=None,\n",
    "                 batch_size=32,\n",
    "                 src_dir_model=''):\n",
    "        X, y, tax_order = data\n",
    "        perfs = []\n",
    "        y_hats = []\n",
    "        y_trues = []\n",
    "        ns = []\n",
    "        for fname in glob.glob(str(src_dir_model/'*.pt')):\n",
    "            model = Model(X.shape[1], out_channel=16).to(device)\n",
    "            model.load_state_dict(torch.load(fname))\n",
    "            learner = Learner(model, tax_lookup=self.tax_lookup.values())\n",
    "            seed = int(re.search(r'-(\\d+)\\.', fname).group(1))\n",
    "            generators = self._get_generators((X, y, tax_order), seed, order=order)\n",
    "            training_generator, validation_generator, test_generator  = generators\n",
    "            nb_data_gen = np.sum(1 for _ in test_generator)\n",
    "            if nb_data_gen > 1:\n",
    "                y_hat, y_true = learner.predict(test_generator)\n",
    "                perfs.append(eval_reg(y_true, y_hat))\n",
    "                y_hats.append(y_hat.ravel())\n",
    "                y_trues.append(y_true.ravel())\n",
    "                ns.append(nb_data_gen*batch_size)\n",
    "        return pd.DataFrame(perfs), pd.DataFrame(y_hats).T, pd.DataFrame(y_trues).T, pd.DataFrame(ns)\n",
    "            \n",
    "    \n",
    "    def _get_generators(self, data, seed, batch_size=32, order=None):\n",
    "        X, y, tax_order = data\n",
    "        # Train/test split\n",
    "        data = train_test_split(X, \n",
    "                                y, \n",
    "                                tax_order, \n",
    "                                test_size=self.split_ratio,\n",
    "                                random_state=seed)\n",
    "        X_train, X_test, y_train, y_test, tax_order_train, tax_order_test = data\n",
    "        data_test = X_test, y_test, tax_order_test\n",
    "\n",
    "        # Further train/valid split\n",
    "        data = train_test_split(X_train, \n",
    "                                y_train,\n",
    "                                tax_order_train,\n",
    "                                test_size=self.split_ratio, \n",
    "                                random_state=seed)\n",
    "        X_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = data\n",
    "        data_train = X_train, y_train, tax_order_train\n",
    "        data_valid = X_valid, y_valid, tax_order_valid\n",
    "\n",
    "        if order is not None:\n",
    "            data_train, data_valid, data_test = [self._filter(data, order=order) \n",
    "                                                 for data in [data_train, data_valid, data_test]]\n",
    "           \n",
    "        dls = DataLoaders(data_train, \n",
    "                          data_valid,\n",
    "                          data_test,\n",
    "                          transform=SNV_transform(),\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "        training_generator, validation_generator, test_generator = dls.loaders()\n",
    "        for batches in validation_generator:\n",
    "            local_batch, local_labels, local_tax = batches\n",
    "        return training_generator, validation_generator, test_generator\n",
    "    \n",
    "    def _filter(self, data, order=None):\n",
    "        X, y, tax_order = data\n",
    "        mask = tax_order == order\n",
    "        return X[mask, :], y[mask], tax_order[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the Model, Learner and Learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 1764)\n"
     ]
    }
   ],
   "source": [
    "src_dir = 'test'\n",
    "fnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n",
    "          'depth-order-smp.npy', 'target-smp.npy', \n",
    "          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n",
    "\n",
    "# Or real data\n",
    "#src_dir = '../_data'\n",
    "#fnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n",
    "#          'depth-order.npy', 'target.npy', \n",
    "#          'tax-order-lu.pkl', 'spectra-id.npy']\n",
    "\n",
    "X, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n",
    "transforms = [select_y, select_tax_order, select_X, log_transform_y]\n",
    "\n",
    "data = X, y, X_id, depth_order\n",
    "X, y, X_id, depth_order = compose(*transforms)(data)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime is: cpu\n"
     ]
    }
   ],
   "source": [
    "# Is a GPU available?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "print(f'Runtime is: {device}')\n",
    "\n",
    "params_scheduler = {\n",
    "    'base_lr': 3e-5,\n",
    "    'max_lr': 1e-3,\n",
    "    'step_size_up': 5,\n",
    "    'mode': 'triangular',\n",
    "    'cycle_momentum': False\n",
    "}\n",
    "\n",
    "n_epochs = 21\n",
    "seeds = range(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Seed: 0\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2095176726579666 | Validation loss: 0.28754910826683044\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.20893988013267517 | Validation loss: 0.2862740755081177\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.20712845027446747 | Validation loss: 0.28408676385879517\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.20162831246852875 | Validation loss: 0.2809036374092102\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.1960785835981369 | Validation loss: 0.2767537534236908\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.18827631324529648 | Validation loss: 0.27165573835372925\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.18386302888393402 | Validation loss: 0.26757434010505676\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.17783822119235992 | Validation loss: 0.26450157165527344\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.17546673864126205 | Validation loss: 0.26242998242378235\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.17405737936496735 | Validation loss: 0.26132825016975403\n",
      "Validation loss (ends of cycles): [0.28754911]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.1731555312871933 | Validation loss: 0.2611456513404846\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.171500563621521 | Validation loss: 0.26005008816719055\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.17028532177209854 | Validation loss: 0.2580767273902893\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.16719596087932587 | Validation loss: 0.2550870180130005\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.16363266110420227 | Validation loss: 0.25126326084136963\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.15928955376148224 | Validation loss: 0.24669373035430908\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.1538691222667694 | Validation loss: 0.24314354360103607\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.1488434001803398 | Validation loss: 0.24059247970581055\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.14692240208387375 | Validation loss: 0.2389926314353943\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.14454391598701477 | Validation loss: 0.238305926322937\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.14511099457740784 | Validation loss: 0.23853451013565063\n",
      "Validation loss (ends of cycles): [0.28754911 0.26114565 0.23853451]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 1\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.25518064200878143 | Validation loss: 0.20745772123336792\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.2529909759759903 | Validation loss: 0.20660457015037537\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.25137291848659515 | Validation loss: 0.20521025359630585\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.2424357831478119 | Validation loss: 0.20297007262706757\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.23724160343408585 | Validation loss: 0.2000746876001358\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.22914429008960724 | Validation loss: 0.19651295244693756\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.22360220551490784 | Validation loss: 0.1936747431755066\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.21922896057367325 | Validation loss: 0.19150234758853912\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.2143341824412346 | Validation loss: 0.19003605842590332\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.2121415138244629 | Validation loss: 0.18925482034683228\n",
      "Validation loss (ends of cycles): [0.20745772]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.2118835374712944 | Validation loss: 0.18918269872665405\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.21091558784246445 | Validation loss: 0.18842312693595886\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.2112690731883049 | Validation loss: 0.1870431900024414\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.20744766294956207 | Validation loss: 0.1849963366985321\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.20407014340162277 | Validation loss: 0.18240754306316376\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.19959653913974762 | Validation loss: 0.1791967898607254\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.19397148489952087 | Validation loss: 0.17683245241641998\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.1890103444457054 | Validation loss: 0.17504656314849854\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.18793968856334686 | Validation loss: 0.1737639605998993\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.18513494729995728 | Validation loss: 0.17320512235164642\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.18279500305652618 | Validation loss: 0.17318619787693024\n",
      "Validation loss (ends of cycles): [0.20745772 0.1891827  0.1731862 ]\n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('test/dumps-test/cnn/train_eval/all/losses')\n",
    "dest_dir_model = Path('test/dumps-test/cnn/train_eval/all/models')\n",
    "\n",
    "\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, split_ratio=0.1, device=device)\n",
    "learners.train((X, y, depth_order[:, -1]), \n",
    "               dest_dir_loss=dest_dir_loss,\n",
    "               dest_dir_model=dest_dir_model,\n",
    "               n_epochs=n_epochs,\n",
    "               sc_kwargs=params_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('test/dumps-test/cnn/train_eval/all/models')\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_global_all, y_hats_all, y_trues_all, ns_all = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                                      src_dir_model=src_dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: []\n",
       " Index: [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learners.evaluate((X, y, depth_order[:, -1]), src_dir_model=src_dir_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.1\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test, tax_order_train, tax_order_test = train_test_split(X, \n",
    "                                                                                     y, \n",
    "                                                                                     depth_order[:,1], \n",
    "                                                                                     test_size=split_ratio,\n",
    "                                                                                     random_state=42)\n",
    "\n",
    "# Further train/valid split\n",
    "X_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = train_test_split(X_train, \n",
    "                                                                                      y_train,\n",
    "                                                                                      tax_order_train, \n",
    "                                                                                      test_size=split_ratio, \n",
    "                                                                                      random_state=42)\n",
    "\n",
    "\n",
    "dls = DataLoaders((X_train, y_train, tax_order_train), \n",
    "                  (X_valid, y_valid, tax_order_valid),\n",
    "                  (X_test, y_test, tax_order_test), \n",
    "                  transform=SNV_transform())\n",
    "\n",
    "training_generator, validation_generator, test_generator = dls.loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "step_size_up = 5\n",
    "criterion = MSELoss() # Mean Squared Error loss\n",
    "base_lr, max_lr = 3e-5, 1e-3 # Based on learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n"
     ]
    }
   ],
   "source": [
    "## LR finder\n",
    "model = Model(X.shape[1], out_channel=16).to(device)\n",
    "\n",
    "opt = Adam(model.parameters(), lr=1e-4)\n",
    "model = model.apply(weights_init)\n",
    "\n",
    "scheduler = CyclicLR(opt, base_lr=base_lr, max_lr=max_lr,\n",
    "                     step_size_up=step_size_up, mode='triangular',\n",
    "                     cycle_momentum=False)\n",
    "\n",
    "learner = Learner(model, criterion, opt, n_epochs=n_epochs, \n",
    "                  scheduler=scheduler, early_stopper=None,\n",
    "                  tax_lookup=tax_lookup.values(), verbose=True)\n",
    "\n",
    "lrs, losses = learner.lr_finder(training_generator, end=0.1, n_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAELCAYAAADQsFGkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtSUlEQVR4nO3deXhV5bn+8e+diQiESQIooAwyyCAQAgoizorVijiCyqgiWj3H01Nb/WlbW1r1aFurFWWmWgdEBCes6LE9UssgYZ4VESGiEkYJECDJ8/sjC92EIDuQZGVnP5/r4sre73rXWs9axn1nDXu9MjOcc87Fn4SwC3DOORcODwDnnItTHgDOORenPACccy5OeQA451yc8gBwzrk4lRR2AaVRv359a9asWdhlOOdcTFmwYMEWM0sv3h5TAdCsWTOysrLCLsM552KKpC9KavdTQM45F6c8AJxzLk55ADjnXJyKqWsAzjl3vA4cOEB2djZ5eXlhl1LmUlNTadKkCcnJyVH19wBwzsWV7Oxs0tLSaNasGZLCLqfMmBlbt24lOzub5s2bRzVPVKeAJPWRtEbSWkn3lTC9r6SlkhZLypLUK2hvE7Qd/PetpHuCafUkvS/p0+Bn3eg31Tnnjk1eXh4nnnhilfrwB5DEiSeeWKojm6MGgKREYBRwGdAOGCCpXbFuHwCdzKwzMAwYD2Bma8ysc9DeFdgDTA/muQ/4wMxaBfMfFixlZfvu/SzL3llei3fOxZiq9uF/UGm3K5ojgO7AWjNbZ2b7gclA38gOZpZr3w8sUAMoaZCBC4HPzOzg/ah9geeC188BV5Wq8lJ46K0VXDdmNh+s+qa8VuGcc1GrWbNm2CUA0QVAY2BjxPvsoO0QkvpJWg3MoOgooLj+wMsR7xua2VcAwc8G0RZdWg9e3o7WDdO47fksXpxX4vchnHMu7kQTACUdUxz2F76ZTTezthT9JT/ykAVIKcCVwKulLVDS8OC6QlZOTk5pZwcgPa0ak4efxbmt03lg+nL+MHMNPhKacy5sZsa9995Lhw4d6NixI6+88goAX331Fb1796Zz58506NCBf/3rXxQUFDBkyJDv+j7xxBPHvf5o7gLKBppGvG8CbDpSZzObJamlpPpmtiVovgxYaGaR52C+kXSSmX0l6SRg8xGWNxYYC5CZmXnMn9rVU5IYNyiTX76xnKf/uZavdubx6DUdSU70r0I4F69+89YKVm76tkyX2e7kWvz6x+2j6jtt2jQWL17MkiVL2LJlC926daN379689NJLXHrppTzwwAMUFBSwZ88eFi9ezJdffsny5csB2LFjx3HXGs2n33yglaTmwV/y/YE3IztIOk3B1QdJGUAKsDWiywAOPf1DsIzBwevBwBulL790khITeLhfR356cWteW5jNsL/OZ1fegfJerXPOleijjz5iwIABJCYm0rBhQ84991zmz59Pt27dmDRpEg899BDLli0jLS2NFi1asG7dOu6++27effddatWqddzrP+oRgJnlS7oLmAkkAhPNbIWkEcH00cA1wCBJB4C9wA0HLwpLqg5cDNxebNGPAlMk3QJsAK477q2JgiT+48JWNKqdyv3TlnHDmLlMGtqNhrVSK2L1zrlKJNq/1MvLkU5F9+7dm1mzZjFjxgwGDhzIvffey6BBg1iyZAkzZ85k1KhRTJkyhYkTJx7X+hVL58IzMzOtLJ8G+uEnOdzxwgLqVk/huWHdOK1BWpkt2zlXOa1atYrTTz891Bpq1qxJbm4u06ZNY8yYMbzzzjts27aNzMxM5s2bx759+2jcuDFJSUn8+c9/Zv369Tz44IOkpKRQq1YtFi9ezJAhQ1i8ePFhyy5p+yQtMLPM4n3j+pvA57ZOZ8rtPRgyaT5XPzOb8YO70b15vbDLcs7FiX79+jFnzhw6deqEJB577DEaNWrEc889x+OPP05ycjI1a9bk+eef58svv2To0KEUFhYC8Mgjjxz3+uP6COCgjdv2MHjSx2Rv38ufb+jMjzqeVObrcM5VDpXhCKA8leYIwG+BAZrWq85rI3rSsXFtfvLSQiZ89HnYJTnnXLnzAAjUrZHCi7eeySXtGjLy7ZWMfHslhYWxc3TknHOl5QEQITU5kWdu6sqQns2Y8NHn3D15EXkHCsIuyznnykVcXwQuSWKC+PWP23FynVQefmc1Obv2MW5gJrWrR/d8bedc5WdmVfKBcKW9putHACWQxPDeLXmyf2cWbdjONaNn8+WOvWGX5ZwrA6mpqWzdurXKPQ7m4HgAqanRf6fJjwB+QN/OjUlPq8btf1tAv1H/ZtLQbrQ/uXbYZTnnjkOTJk3Izs7mWJ8tVpkdHBEsWn4baBTWfL2LIZM+ZldePs/enME5rdIrvAbnnDtWfhvocWjTKI1pd/akSd0TGDppPtMWZoddknPOHTc/BRSlk2qfwJQRPbj9+QX8dMoS1m/dw9ktTyQpMYHkRJEc/ExKSCDpu/fB66AtKUFV8sKTcy42+SmgUtqXX8DPpy7ljcVHfCL2DyoeEkkJRT+rJScwtGczBvZoVrYFO+finj8LqIxUS0rkies7M+zs5uTuy2d/QSH5BUZ+QSEHCoOfBYUcCNryC40DBcaBgsJifQ62GQcKC1m/ZTe/fGMFX3+bx88uaeNHCs65cucBcAwSEkSnpnXKdJn5BYU8+PpyRv3zM7bm7ud3V3UgyQercc6VIw+ASiIpMYFHru5Ielo1/vKPtWzbvZ+nBnQhNTkx7NKcc1WU/4lZiUjivy9pw0M/bsf7q75h0MSP2bnXRyxzzpUPD4BKaMjZzXmyfxcWbdjODWPmsPnbvLBLcs5VQR4AldSVnU5m4pBubNi2h2tGz+bzLbvDLsk5V8VEFQCS+khaI2mtpPtKmN5X0lJJiyVlSeoVMa2OpKmSVktaJalH0N5Z0tyIebqX3WZVDee0Sufl285i974Crn12Nsuyd4ZdknOuCjlqAEhKBEYBlwHtgAGS2hXr9gHQycw6A8OA8RHTngTeNbO2QCdgVdD+GPCbYJ5fBe9dMZ2a1uHVET1ITU6k/9g5/HvtlrBLcs5VEdEcAXQH1prZOjPbD0wG+kZ2MLNc+/4bZTUAA5BUC+gNTAj67TezHQdnA2oFr2sDx/bNqjjQMr0mr93RkyZ1qzN00nxmLP0q7JKcc1VANAHQGNgY8T47aDuEpH6SVgMzKDoKAGgB5ACTJC2SNF5SjWDaPcDjkjYCfwDuP7ZNiA+Naqcy5fYenNGkNne9vJC/zVkfdknOuRgXTQCU9JXUw54fYWbTg9M8VwEjg+YkIAN41sy6ALuBg9cQ7gD+y8yaAv9FcJRw2Mql4cE1gqyq+PjW0qhdPZm/3XImF7RpwC/fWMET739S5Z5p7pyrONEEQDbQNOJ9E37gdI2ZzQJaSqofzJttZvOCyVMpCgSAwcC04PWrFJ1qKml5Y80s08wy09P9McwnpCQyZmBXru3ahCc/+JQHX19OgY9d7Jw7BtEEwHyglaTmklKA/sCbkR0knabg4TWSMoAUYKuZfQ1slNQm6HohsDJ4vQk4N3h9AfDpcW1JHElKTODxa8/g9nNb8OK8Ddz98kL25fvYxc650jnqoyDMLF/SXcBMIBGYaGYrJI0Ipo8GrgEGSToA7AVuiLgofDfwYhAe64ChQfttwJOSkoA8YHgZbleVJ4n7Lzud9JrV+N2MVWzfPZ+xg7qSlupjFzvnouOPg64Cpi/K5t5Xl9KmURp/Hdqd9LRqYZfknKtEfESwKqxflyaMG5zJupzdXDt6Nhu27gm7JOdcDPAAqCLOb9OAF287k517D3DN6Nms2OTfGnbO/TA/BVTFrN28i4ETPiY3L5+f92lD7eoppESMQpaSmEBSYuTrYPjKQ4ay1PdDXSYkkJDgg9M4F8uOdArIA6AK2rRjL4Mnfsynm3PLZHkJguTEBBrVTuXJ/l3oXMaD4TjnypcHQJw5UFBI9va93w0/mV8YOVSlBa8PDllZeOiwlQdfF0b0KzDeWf4VObv28VT/LlzSvlHYm+ici5KPCRxnkhMTaF6/xtE7lsKt57Tg1uezuP2FBfz6inYMObt5mS7fOVex/CKwi1p6WjUm33YWF53ekIfeWsnIt1dS6N9Cdi5meQC4UjkhJZHRN3dlSM9mTPjoc+58cSF5B/xbyM7FIg8AV2qJCeKhK9vzyyvaMXPl1wwYN5etufvCLss5V0oeAO6Y3dKrOc/elMHKTd9y9bM+bKVzscYDwB2XPh1O4uXhZ7ErL5+rn/k3Weu3hV2Scy5KHgDuuGWcUpdpd/SkTvUUbhw/z0cscy5GeAC4MtGsfg1eu6MnHRvX5icvLWTsrM98sBrnKjkPAFdm6tVI4cVbz+Tyjifx8Dur+dUbK8gvKAy7LOfcEfgXwVyZSk1O5C8DutCk7gmMmbWOTTv28pcbu1A9xX/VnKts/AjAlbmEBHH/j05nZN/2/HPNZvqPncvmXXlhl+WcK8YDwJWbgT2aMXZgJp9+k0u/UbNZu3lX2CU55yJ4ALhydVG7hrxy+1nsyy/k6mdmM3fd1rBLcs4FogoASX0krZG0VtJ9JUzvK2mppMWSsiT1iphWR9JUSaslrZLUI2La3cFyV0h6rGw2yVU2ZzSpw/Q7e9KgVioDJ8zj9UVfhl2Sc44oAkBSIjAKuAxoBwyQ1K5Ytw+ATmbWGRgGjI+Y9iTwrpm1BToBq4Llng/0Bc4ws/bAH45vU1xl1rRedV4b0ZOMU+pyzyuLGfXPtX6bqHMhi+bWjO7AWjNbByBpMkUf3CsPdjCzyJFHagAW9K0F9AaGBP32A/uDfncAj5rZvmDa5uPZEFf51a6ezPO3dOcXU5fy+Mw1bNi6h76dTy7TdbQ/uTa1qyeX6TKdq6qiCYDGwMaI99nAmcU7SeoHPAI0AC4PmlsAOcAkSZ2ABcB/mtluoDVwjqTfA3nAz8xs/rFuiIsN1ZISeeKGzjSpW52n/7mWV7I2Hn2mUmhUK5WJQ7rR7uRaZbpc56qiaAKgpAFhDzt2N7PpwHRJvYGRwEXB8jOAu81snqQngfuAXwbT6gJnAd2AKZJaWLHzApKGA8MBTjnllGi3y1VikvjZpW34caeT2bFn/9FniNKuvHwefH05142ezaibMjivTYMyW7ZzVVE0AZANNI143wTYdKTOZjZLUktJ9YN5s81sXjB5KkUBcHC504IP/I8lFQL1KTpiiFzeWGAsFA0JGUW9Lka0aZRW5svs0Lg2w/46n1uey2Jk3w7ceKb/0eDckURzF9B8oJWk5pJSgP7Am5EdJJ0mScHrDCAF2GpmXwMbJbUJul7I99cOXgcuCOZpHcyz5fg2x8W7RrVTmTKiB+e0qs//m76MR/6+ykctc+4IjnoEYGb5ku4CZgKJwEQzWyFpRDB9NHANMEjSAWAvcEPEqZy7gReD8FgHDA3aJwITJS2n6MLw4OKnf5w7FjWrJTF+UCa/fnMFYz5cR/a2vfzx+k6kJieGXZpzlYpi6TM3MzPTsrKywi7DxQgzY9y/1vHwO6vJOKUO4wZlcmLNamGX5VyFk7TAzDKLt/s3gV2VJYnhvVvyzE0ZrAhGLVuXk3v0GZ2LEx4Arsr7UceIUcuenc3Hn/uoZc6BB4CLExmn1GX6nT2pVz2Fm8fP443F/jgK5zwAXNw49cQaTLuzJ52b1uE/Jy/m6X986o+jcHHNA8DFlTrVU/jbrd3p2/lk/vDeJ/zitaUc8FHLXJzyYZpc3KmWlMifb+jMqfWq89Q/1rJpRx7P3JxBrVR/hpCLL34E4OKSJH56SRseu/YM5q7byrXPziZ7+56wy3KuQnkAuLh2fWZTnhvWna925NHvmdksy94ZdknOVRgPABf3zj6tPq/d2ZOUxASuHzOH/135TdglOVchPACcA1o3TGP6T3rSqmFNhv8ti7/++/OwS3Ku3HkAOBdokJbK5OFncUHbhjz01kp++9ZKCvxBcq4K87uAnItQPSWJMQO78rsZK5n478+Zv34bDWtVI0EiQSIxQSQkiERR1JYgEoOfCaJo+sF+4rvpiQlCErVSk+jf/RRqVvP/9Vz4/LfQuWISE8Svf9yeFuk1eTVrI5t25FFoRkGhUWCGGUWvC41Cs2Aa3/UpNKMw6Ft4sD2YD+DNJZuYNKSbP5jOhc6fBupcBTEz/nfVZu56aSGN65zAc8O607Re9bDLcnHAnwbqXMgkcXG7hrxw65lsyd3HtaNns/rrb8Muy8UxDwDnKli3ZvWYMqIHANePnsP89f50UhcODwDnQtC2US2mjuhJ/ZrVuHn8PP/ugQuFB4BzIWlarzqvjuhBm0Zp3P7CAqZkbQy7JBdnogoASX0krZG0VtJ9JUzvK2mppMWSsiT1iphWR9JUSaslrZLUo9i8P5Nkkuof/+Y4F1tOrFmNl247ix4tTuTnU5cy+sPP/BHVrsIcNQAkJQKjgMuAdsAASe2KdfsA6GRmnYFhwPiIaU8C75pZW6ATsCpi2U2Bi4ENx7ENzsW0mtWSmDikG1eccRKP/n01v5+xikL/ApqrANF8D6A7sNbM1gFImgz0BVYe7GBmkQOt1gAs6FsL6A0MCfrtB/ZH9H0C+DnwxjFvgXNVQEpSAk/178KJNVIY/9HnbNu9n/+59gySE/0srSs/0fx2NQYiT05mB22HkNRP0mpgBkVHAQAtgBxgkqRFksZLqhH0vxL40syWHM8GOFdVJCSIh65sz39f3Jppi75k+PNZ7NmfH3ZZrgqLJgBUQtthx6dmNj04zXMVMDJoTgIygGfNrAuwG7hPUnXgAeBXR125NDy4rpCVk5MTRbnOxS5J3H1hKx7u15EPP8nhpvHz2LFn/9FndO4YRBMA2UDTiPdNgE1H6mxms4CWwUXdbCDbzOYFk6dSFAgtgebAEknrg2UulNSohOWNNbNMM8tMT0+PolznYt+NZ57CMzdlsGLTt1w3eg5f7dwbdkmuCoomAOYDrSQ1l5QC9AfejOwg6TRJCl5nACnAVjP7GtgoqU3Q9UJgpZktM7MGZtbMzJpRFBQZQX/nHNCnw0k8N7Q7X+/M45pnZrN2866wS3JVzFEDwMzygbuAmRTdwTPFzFZIGiFpRNDtGmC5pMUU3TF0g31/L9vdwIuSlgKdgYfLdhOcq7p6tDyRybefxf4C49rRc1i0YXvYJbkqxB8G51wM+GLrbgZN/JjN3+7j2ZszOK9Ng7BLcjHEHwbnXAw79cQaTB3Rk+b1a3Drc1m8vujLsEtyVYAHgHMxIj2tGpNvP4vMZnW555XFTPjIh610x8cDwLkYUis1mb8O7U6f9o0Y+fZK/ufd1f7oCHfM/BqAczGooND45RvLeWneBqolJVCzWhI1U5OokZL0/etqwetqidSslkyNaomHTEurFtmn6HVKkv9NWBUd6RqADwnpXAxKTBC/v6oDXU+pyyff7CJ3Xz65+/LZvS+fXXn5bN6VR25OPrn7Csjdd4C8A4VRLTclKYGzWpzIqBu7kJaaXM5b4cLmAeBcjJLENV2bRNU3v6CQ3fsLDgmJ3UFo5O7LJzd4v3X3fl6Y+wWDJ37Mc8O6ewhUcR4AzsWBpMQEap+QQO0Tjv6BflaLetz10iIGBSFQy0OgyvITfs65Q/TpcBJP35jBsuydDJrwMd/mHQi7JFdOPACcc4fp06ERo27KYPmXOxk44WN27vUQqIo8AJxzJbq0fSOeuSmDlZt2MmjCPA+BKsgDwDl3RJe0b8QzN3Vl5VffeghUQR4AzrkfdHG7hjwbhMDACfPYucdDoKrwAHDOHdVF7Roy+uaurP5qFzd7CFQZHgDOuahceHpDRg/MYM3Xu7hpwlwfqawK8ABwzkXtgrYNGTOwK598nevDVVYBHgDOuVI5v20Dxgzqyqebc7lx3Dy27/YQiFUeAM65Uju/TQPGDuzK2pyiIwEPgdjkAeCcOybntWnAuEGZrM3J5cbx89jmIRBzogoASX0krZG0VtJ9JUzvK2mppMWSsiT1iphWR9JUSaslrZLUI2h/PGhbKmm6pDpltlXOuQpxbut0xg/KZF1OLjeOm+shEGOOGgCSEika6P0yoB0wQFK7Yt0+ADqZWWdgGDA+YtqTwLtm1hboRNHA8gDvAx3M7AzgE+D+49gO51xIerdOZ/zgTD7fspsbx81la+6+sEtyUYrmCKA7sNbM1pnZfmAy0Deyg5nl2vcjy9QADEBSLaA3MCHot9/MdgSv3zOz/GCeuUB0z7V1zlU657RKZ8Lgbny+ZTc3jZ/nIRAjogmAxsDGiPfZQdshJPWTtBqYQdFRAEALIAeYJGmRpPGSapSwjmHA30tVuXOuUunVqj4Th3Rj/dbd3DhuHls8BCq9aAJAJbQdNo6kmU0PTvNcBYwMmpOADOBZM+sC7AYOuYYg6QEgH3ixxJVLw4PrClk5OTlRlOucC8vZp9Vn4uBufLGt6HSQh0DlFk0AZANNI943ATYdqbOZzQJaSqofzJttZvOCyVMpCgQAJA0GrgBusiMMTmxmY80s08wy09PToyjXORemnkEIbNi2hwFj55Kzy0OgsoomAOYDrSQ1l5QC9AfejOwg6TRJCl5nACnAVjP7GtgoqU3Q9UJgZdCvD/AL4Eoz21MmW+OcqxR6nlafSUO6k719L9eOns2f3lvDh5/k+OAylcxRh4Q0s3xJdwEzgURgopmtkDQimD4auAYYJOkAsBe4IeIv+ruBF4PwWAcMDdqfBqoB7wfZMdfMRpTdpjnnwtSj5Yn8dWg3Rs5YydP/XEuhgQRtGqbR9dS6dD21Lpmn1qNpvRMIPgNcBdMRzrxUSpmZmZaVlRV2Gc65Usrdl8/iDTtY8MV2sr7YxuINO9i1r+gmwPS0anQ9pS6ZzeqScWpdOpxcm5Qk/45qWZK0wMwyi7f7oPDOuXJXs1oSvVrVp1er+gAUFBqffLOLBV9s/+7fuyu+BiAlKYFOTWrT9dR63x0p1KuREmb5VZYfATjnKoXN3+Z9FwZZX2xnxaadHCgo+nxqUb9G0SmjZnXJbFaPluk1Q642thzpCMADwDlXKeUdKGBp9s4gFLax4IvtbA8GovnZJa2564JWIVcYO/wUkHMupqQmJ9K9eT26N68HtMTMWLdlN396/xP++P4ndGxSh3Nb+63hx8OvtDjnYoIkWqbX5A/XdqJNwzT+c/Iisrf7HeTHwwPAORdTTkhJ5Nmbu1JQYPzkxYXsyy8Iu6SY5QHgnIs5zevX4PHrOrEkeycj314ZdjkxywPAOReT+nRoxO29W/DC3A1MW5gddjkxyQPAORez7r20Dd2b1+P/TV/G6q+/DbucmOMB4JyLWUmJCTx9YxfSUpO544WF/qyhUvIAcM7FtAZpqYy6MYMN2/Zw76tLiKXvNoXNA8A5F/O6N6/H/Ze1ZeaKbxj3r3VhlxMzPACcc1XCLb2ac1mHRvzPu2uYu25r2OXEBA8A51yVIInHrj2DU+tV566XFrH527ywS6r0PACcc1VGWmoyz97cld378vnJSws5UFAYdkmVmgeAc65KadMojUeu7sj89dt57N3VYZdTqXkAOOeqnKu6NGZQj1MZ96/PeWfZV2GXU2l5ADjnqqQHLj+dzk3r8POpS/ksJzfsciqlqAJAUh9JayStlXRfCdP7SloqabGkLEm9IqbVkTRV0mpJqyT1CNrrSXpf0qfBz7plt1nOuXhXLSmRZ27KIDlR3PHCAvbszw+7pErnqAEgKREYBVwGtAMGSGpXrNsHQCcz6wwMA8ZHTHsSeNfM2gKdgFVB+33AB2bWKpj/sGBxzrnjcXKdE3hqQBc+3ZzL/dOW+ZfEionmCKA7sNbM1pnZfmAy0Deyg5nl2vd7tgZgAJJqAb2BCUG//Wa2I+jXF3gueP0ccNWxb4ZzzpXsnFbp/PSi1ryxeBMvzP0i7HIqlWgCoDGwMeJ9dtB2CEn9JK0GZlB0FADQAsgBJklaJGm8pBrBtIZm9hVA8LPBMW6Dc879oJ+cfxrnt0nnt2+vZNGG7WGXU2lEEwAqoe2w4ygzmx6c5rkKGBk0JwEZwLNm1gXYTSlP9UgaHlxXyMrJySnNrM45B0BCgnjihs40rJXKnS8uZGvuvrBLqhSiCYBsoGnE+ybApiN1NrNZQEtJ9YN5s81sXjB5KkWBAPCNpJMAgp+bj7C8sWaWaWaZ6ek+/qdz7tjUqZ7Cszd1Zevu/dzzymIKCv16QDQBMB9oJam5pBSgP/BmZAdJp0lS8DoDSAG2mtnXwEZJbYKuFwIHh+95ExgcvB4MvHFcW+Kcc0fRsUltfntle/716Rae/ODTsMsJXdLROphZvqS7gJlAIjDRzFZIGhFMHw1cAwySdADYC9wQcVH4buDFIDzWAUOD9keBKZJuATYA15XhdjnnXIlu6NaUBV9s56kPPqVL0zqc3zZ+Lz8qlm6LyszMtKysrLDLcM7FuLwDBfR7Zjabduzl7bt70bRe9bBLKleSFphZZvF2/yawcy7upCYn8uxNGRSaceeLC8k7UBB2SaHwAHDOxaVm9Wvwx+s6sezLnfzitaXkx+GTQz0AnHNx65L2jfh5nza8sXgTd7+8iH358XUk4AHgnItrd553Gr+6oh1/X/41tz6XFVfPDPIAcM7FvWG9mvPYtWfw77VbGDThY77NOxB2SRXCA8A554DrM5vylwEZLMnewYCxc+Pi28IeAM45F7j8jJMYNyiTz3JyuX7MHL7auTfsksqVB4BzzkU4r00Dnh92Jt98u49rn53D+i27wy6p3HgAOOdcMd2b1+Pl285iz/58rhszhzVf7wq7pHLhAeCccyXo2KQ2U27vQYLghrFzWLxxR9gllTkPAOecO4JWDdOYOqIntVKTuWncXOZ8tjXsksqUB4Bzzv2ApvWq8+qIHpxc5wQGT/qYD1Z9E3ZJZcYDwDnnjqJhrVReub0HbRulcfvfFvDmkiMOiRJTPACccy4K9Wqk8OKtZ5Jxal3+c/IiXpq3IeySjpsHgHPORSktNZnnh3XnvNbp/L/pyxg767OwSzouHgDOOVcKqcmJjBmYyeVnnMTD76zmj++tIZbGVYl01BHBnHPOHSolKYGn+nchrVoSf/nHWnbl5fOrK9qRkKCwSysVDwDnnDsGiQnikas7UqNaEhM++pzcffk8enVHkhJj58RKVJVK6iNpjaS1ku4rYXpfSUslLZaUJalXxLT1kpYdnBbR3lnS3Ih5upfNJjnnXMWQxIOXn849F7Vi6oLsmBtT4KhHAJISgVHAxUA2MF/Sm2a2MqLbB8CbZmaSzgCmAG0jpp9vZluKLfox4Ddm9ndJPwren3fsm+KccxVPEvdc1Jq01GRGvr2S3OeyGDOwK9VTKv8JlmiOALoDa81snZntByYDfSM7mFmufX8VpAYQzRURA2oFr2sDVePGWudcXLqlV3Meu6ZoTIGhk+ZTUFj5LwxHEwCNgY0R77ODtkNI6idpNTADGBYxyYD3JC2QNDyi/R7gcUkbgT8A95eyduecq1Su79aUR68+g3mfb2Py/Mr/PYFoAqCky9qHRZuZTTeztsBVwMiISWebWQZwGfATSb2D9juA/zKzpsB/ARNKXLk0PLhGkJWTkxNFuc45F57rMpvQvXk9/vjeJ+zcW7lHFosmALKBphHvm/ADp2vMbBbQUlL94P2m4OdmYDpFp5QABgPTgtevRrQXX95YM8s0s8z09PQoynXOufBI4ldXtGP7nv089cGnYZfzg6IJgPlAK0nNJaUA/YE3IztIOk2SgtcZQAqwVVINSWlBew3gEmB5MNsm4Nzg9QVA5d5TzjkXpQ6Na9O/W1Oem72ez3Jywy7niI4aAGaWD9wFzARWAVPMbIWkEZJGBN2uAZZLWkzRHUM3BBeFGwIfSVoCfAzMMLN3g3luA/4YTHsYiLw+4JxzMe2/L2nDCcmJ/O7tlUfvHBLF0leYMzMzLSsr6+gdnXOuEhg3ax2/f2cVk4Z24/w2DUKrQ9ICM8ss3h47X1lzzrkYM7hnM5rXr8Hv3l7JgYLCsMs5jAeAc86Vk5SkBB68/HQ+y9nN3+Z8EXY5h/EAcM65cnRB2wac06o+f/7fT9i2e3/Y5RzCA8A558rRwdtCd+8v4E/vrwm7nEN4ADjnXDlr1TCNgWedykvzNrDqq2/DLuc7HgDOOVcB7rmoFbVOSOa3b62sNAPIeAA451wFqFM9hZ9e3Jo567Yyc8U3YZcDeAA451yFubH7KbRuWJOH31lVKcYN8ABwzrkKkpSYwK+uaM+GbXuY+NH6sMvxAHDOuYrUq1V9Ljq9IU//41M2f5sXai0eAM45V8EevPx09hcU8vjMcG8L9QBwzrkK1qx+DYad3ZxXF2SzNHtHaHV4ADjnXAjuuuA06tdM4Tch3hbqAeCccyFIS03m3kvbsOCL7by5JJwh0T0AnHMuJNd2bUqHxrV49O+r2bu/4m8L9QBwzrmQJCaIX13Rnq925jH6w88qfP0eAM45F6LuzetxxRknMWbWZ3y5Y2+FrtsDwDnnQnb/j07HDB79++oKXW9UASCpj6Q1ktZKuq+E6X0lLZW0WFKWpF4R09ZLWnZwWrH57g6Wu0LSY8e/Oc45F3sa1zmB289tyVtLNjF//bYKW+9RA0BSIkUDvV8GtAMGSGpXrNsHQCcz6wwMA8YXm36+mXWOHJNS0vlAX+AMM2sP/OGYt8I552LciHNb0KhWKr99ayWFhRVzW2g0RwDdgbVmts7M9gOTKfrg/o6Z5dr3N7LWAKKp/g7gUTPbFyxjc/RlO+dc1VI9JYn7f9SWZV/uZOrC7ApZZzQB0BjYGPE+O2g7hKR+klYDMyg6CjjIgPckLZA0PKK9NXCOpHmSPpTUrfTlO+dc1XFlp5PJOKUOj727hl15B8p9fdEEgEpoO+wvfDObbmZtgauAkRGTzjazDIpOIf1EUu+gPQmoC5wF3AtMkXTYuiQND64rZOXk5ERRrnPOxSZJ/PrH7dmSu49R/yz/20KjCYBsoGnE+ybAEb+2ZmazgJaS6gfvNwU/NwPTKTqldHC506zIx0AhUL+E5Y01s0wzy0xPT4+iXOeci12dmtbhmowmTPzoc77Yurtc1xVNAMwHWklqLikF6A+8GdlB0mkH/3qXlAGkAFsl1ZCUFrTXAC4BlgezvQ5cEExrHcyz5bi3yDnnYtwv+rQhOVH8fsaqcl3PUQPAzPKBu4CZwCpgipmtkDRC0oig2zXAckmLKbpj6IbgonBD4CNJS4CPgRlm9m4wz0SghaTlFF1YHhxxIdk55+JWg1qp3Hn+aby38hv+vbb8/i5WLH3mZmZmWlZW1tE7OudcjMs7UMDFT3xI9eQkZvxHL5ISj/17u5IWRN6Gf5B/E9g55yqh1OREHvjR6az5Zhcvz9949BmOgQeAc85VUpe2b8RZLerxp/fWsHNP2d8W6gHgnHOVlFT0tNC8A4Us3LC9zJefVOZLdM45V2banVyLufdfSO3qyWW+bD8CcM65Sq48PvzBA8A55+KWB4BzzsUpDwDnnItTHgDOORenPACccy5OeQA451yc8gBwzrk4FVMPg5OUA3xxDLPWBnaWYf8jTS+pPZq2yPeRr+tTdo/Irsz74If2h+8D3wdQdfdBad4fzz441cwOH1DFzKr8P2BsWfY/0vSS2qNpi3xf7HVWPOyDo+wP3we+D6rsPijN+7LcBwf/xcspoLfKuP+RppfUHk3bWz8wraxU5n3wQ/ujLPk+8H1wLMstz31Q2vdlKqZOAcUbSVlWwjO844nvA98H4PsAymcfxMsRQKwaG3YBlYDvA98H4PsAymEf+BGAc87FKT8CcM65OOUB4JxzccoDwDnn4pQHQIySdJ6kf0kaLem8sOsJg6QakhZIuiLsWsIg6fTgv/9USXeEXU8YJF0laZykNyRdEnY9YZDUQtIESVNLO68HQAgkTZS0WdLyYu19JK2RtFbSfUdZjAG5QCqQXV61locy2n6AXwBTyqfK8lUW+8DMVpnZCOB6IOZukSyjffC6md0GDAFuKMdyy0UZ7YN1ZnbLMa3f7wKqeJJ6U/Th/byZdQjaEoFPgIsp+kCfDwwAEoFHii1iGLDFzAolNQT+ZGY3VVT9x6uMtv8Mir4an0rRvni7YqovG2WxD8xss6QrgfuAp83spYqqvyyU1T4I5vsj8KKZLayg8stEGe+DqWZ2bWnW74PCh8DMZklqVqy5O7DWzNYBSJoM9DWzR4AfOsWxHahWLoWWk7LYfknnAzWAdsBeSe+YWWH5Vl52yup3wMzeBN6UNAOIqQAoo98DAY8Cf4+1D38o88+CUvMAqDwaAxsj3mcDZx6ps6SrgUuBOsDT5VpZxSjV9pvZAwCShhAcDZVrdRWjtL8D5wFXU/QHwDvlWVgFKtU+AO4GLgJqSzrNzEaXZ3EVpLS/BycCvwe6SLo/CIqoeABUHiqh7Yjn58xsGjCt/MqpcKXa/u86mP217EsJTWl/B/4P+L/yKiYkpd0HTwFPlV85oSjtPtgKjDiWFflF4MojG2ga8b4JsCmkWsIQ79sPvg/A9wFU4D7wAKg85gOtJDWXlAL0B94MuaaKFO/bD74PwPcBVOA+8AAIgaSXgTlAG0nZkm4xs3zgLmAmsAqYYmYrwqyzvMT79oPvA/B9AOHvA78N1Dnn4pQfATjnXJzyAHDOuTjlAeCcc3HKA8A55+KUB4BzzsUpDwDnnItTHgDOORenPABclSAptwLWMULSoPJezxHWPUTSyWGs21Vd/kUwVyVIyjWzmmWwnEQzKyiLmspy3ZL+D/iZmWVVbFWuKvMjAFflSLpX0nxJSyX9JqL9dRUNIblC0vCI9lxJv5U0D+gRvP+9pCWS5gaD7iDpIUk/C17/n6T/kfSxpE8knRO0V5c0JVj3K5LmSTriaF0lrPtXQe3LJY1VkWspGvHrRUmLJZ0gqaukD4PtmSnppPLZm64q8wBwVYqKxoVtRdGgGp2BrsGoS1A0elJXij5M/yN4jjoUDSyz3MzONLOPgvdzzawTMAu47QirSzKz7sA9wK+DtjuB7WZ2BjAS6HqUkouv+2kz6xaMDnUCcIWZTQWygJvMrDOQD/wFuDbYnokUPQ/euVLx8QBcVXNJ8G9R8L4mRYEwi6IP/X5Be9OgfStQALwWsYz9wMEhJhdQNDRfSaZF9GkWvO4FPAlgZsslLT1KvcXXfb6knwPVgXrACuCtYvO0AToA7xcNiEUi8NVR1uPcYTwAXFUj4BEzG3NIY9HoWRcBPcxsT3BOPTWYnFfs3PsB+/7iWAFH/v9kXwl9ShrM44d8t25JqcAzQKaZbZT0UESNh2wOsMLMepRyXc4dwk8BuapmJjBMUk0ASY0lNQBqU3RqZo+ktsBZ5bT+j4Drg3W3AzqWYt6DH/ZbgvojB/jeBaQFr9cA6ZJ6BOtJltT+uKp2ccmPAFyVYmbvSTodmBOcHskFbgbeBUYEp2TWAHPLqYRngOeC9SwClgI7o5nRzHZIGgcsA9ZTNDDIQX8FRkvaC/SgKByeklSbov+P/0zR6SLnoua3gTpXhiQlAslmliepJfAB0NrM9odcmnOH8SMA58pWdeCfkpIpOld/h3/4u8rKjwCcqwDBff7VijUPNLNlYdTjHHgAOOdc3PK7gJxzLk55ADjnXJzyAHDOuTjlAeCcc3HKA8A55+LU/wflLmxerqhi2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'learning_rate': lrs, 'loss': losses}).plot(x='learning_rate', y='loss', logx=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
