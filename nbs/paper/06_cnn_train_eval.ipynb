{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/franckalbinet/mirzai/blob/main/nbs/17_paper.cnn.train_eval.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. Train & evaluate (CNN)\n",
    "\n",
    "> Train & evaluate on multiple train/test splits with different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',  force_remount=False)\n",
    "    !pip install mirzai\n",
    "else:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python utilities\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Data science stack\n",
    "import pandas as pd\n",
    "\n",
    "from mirzai.data.loading import load_kssl\n",
    "from mirzai.data.selection import (select_y, select_tax_order, select_X)\n",
    "from mirzai.data.transform import log_transform_y\n",
    "from mirzai.data.torch import DataLoaders, SNV_transform\n",
    "from mirzai.training.cnn import (Model, weights_init)\n",
    "from mirzai.training.cnn import Learner, Learners\n",
    "from mirzai.training.core import load_dumps\n",
    "\n",
    "# Deep Learning stack\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "from fastcore.transform import compose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose\n",
    "#src_dir = 'test'\n",
    "#fnames = ['spectra-features-smp.npy', 'spectra-wavenumbers-smp.npy', \n",
    "#          'depth-order-smp.npy', 'target-smp.npy', \n",
    "#          'tax-order-lu-smp.pkl', 'spectra-id-smp.npy']\n",
    "\n",
    "\n",
    "# or with all data\n",
    "src_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\n",
    "fnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n",
    "          'depth-order.npy', 'target.npy', \n",
    "          'tax-order-lu.pkl', 'spectra-id.npy']\n",
    "\n",
    "X, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n",
    "data = X, y, X_id, depth_order\n",
    "transforms = [select_y, select_tax_order, select_X, log_transform_y]\n",
    "X, y, X_id, depth_order = compose(*transforms)(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Is a GPU available?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "print(f'Runtime is: {device}')\n",
    "\n",
    "params_scheduler = {\n",
    "    'base_lr': 3e-5,\n",
    "    'max_lr': 1e-3,\n",
    "    'step_size_up': 5,\n",
    "    'mode': 'triangular',\n",
    "    'cycle_momentum': False\n",
    "}\n",
    "\n",
    "n_epochs = 201\n",
    "seeds = range(20)\n",
    "seeds = range(15, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all Soil Taxonomic Orders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Seed: 15\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.21006273476802928 | Validation loss: 0.16685934806555774\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.10366790779631614 | Validation loss: 0.12193218717532875\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08515885396031883 | Validation loss: 0.07348759699843627\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07681338573754655 | Validation loss: 0.0775093322174739\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07012089212630385 | Validation loss: 0.06804058407392122\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06663906943111673 | Validation loss: 0.06383055649631847\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06138754077439057 | Validation loss: 0.05653325315004429\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.057238537138034624 | Validation loss: 0.05207979377458053\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.053663239335846595 | Validation loss: 0.04987848757774429\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05063598560660667 | Validation loss: 0.04656141813415869\n",
      "Validation loss (ends of cycles): [0.16685935]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.048641224010429515 | Validation loss: 0.045020436414772956\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04944960257005474 | Validation loss: 0.04620409672832595\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05068350452599739 | Validation loss: 0.0466461240322189\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05165666111700941 | Validation loss: 0.048602522936015004\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05240861789184058 | Validation loss: 0.04709551842734877\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.053400266255567395 | Validation loss: 0.05373546291571275\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.050950179472345654 | Validation loss: 0.046860707412778806\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04810693795888091 | Validation loss: 0.049740820958287316\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.045981014085068245 | Validation loss: 0.04217774676472212\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04369535342254859 | Validation loss: 0.04023268423249236\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04157303913503768 | Validation loss: 0.03929079252303438\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04292749061604859 | Validation loss: 0.03935050310781308\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04409183335387859 | Validation loss: 0.04125337862302508\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04542167893222233 | Validation loss: 0.04289548397393881\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.04644744235868236 | Validation loss: 0.04298555405101681\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04782074668232029 | Validation loss: 0.04539380502779927\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04569561580951348 | Validation loss: 0.04396933422679395\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04414120492503399 | Validation loss: 0.04126848547463923\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.041882850408488076 | Validation loss: 0.03920497682637873\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03984382633563483 | Validation loss: 0.03720222598156043\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03824256880274849 | Validation loss: 0.03624860502252009\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.039079163438002544 | Validation loss: 0.036830263261773945\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04015510225774881 | Validation loss: 0.037671556770471876\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04150084875373712 | Validation loss: 0.040750166566102905\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.0429485932956853 | Validation loss: 0.04076049813127096\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.044326993446122474 | Validation loss: 0.04463050882043564\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.042592402374349886 | Validation loss: 0.04191957023489264\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04073864440012019 | Validation loss: 0.039296620451243575\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.0388192206421339 | Validation loss: 0.040280445091492305\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.03695870364381836 | Validation loss: 0.03545188218915621\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.03567687908414839 | Validation loss: 0.03444945969643582\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.03621109440926404 | Validation loss: 0.03518754746600063\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.03740448744116923 | Validation loss: 0.035378930587486354\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.038526631041987267 | Validation loss: 0.037030073467938775\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04034058869666031 | Validation loss: 0.03999876431230687\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.041215690155330255 | Validation loss: 0.041170561809785074\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.040143450241770566 | Validation loss: 0.03838985987765863\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.03800150607052574 | Validation loss: 0.04004549053786075\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.03671635882598971 | Validation loss: 0.03537545478449459\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.0347628424257612 | Validation loss: 0.034168427686446005\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03355114189009586 | Validation loss: 0.03320952699379583\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.034235434844085255 | Validation loss: 0.033966143739170736\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03527097588709343 | Validation loss: 0.03463689532889202\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.036384562783925906 | Validation loss: 0.043450314378514224\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.037796340584864946 | Validation loss: 0.037529862032527415\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.040052222718638696 | Validation loss: 0.037125598194193\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.037771799428366476 | Validation loss: 0.03799022023722661\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.03619785493654089 | Validation loss: 0.037243562148867454\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03466686871229106 | Validation loss: 0.036433838901266585\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03298729962181652 | Validation loss: 0.03261748232375995\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.0318513594633775 | Validation loss: 0.03203852672492508\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03245821607750144 | Validation loss: 0.03227031828456484\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.033735571010221586 | Validation loss: 0.036995998938131124\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.03462631959838455 | Validation loss: 0.03523477534475052\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.03623737948744991 | Validation loss: 0.03887613358355201\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.037692032123761855 | Validation loss: 0.0393562819314214\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03629782138663659 | Validation loss: 0.03982825133850617\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.034753578830525045 | Validation loss: 0.035794618621930084\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03335795502130705 | Validation loss: 0.03471514085653873\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.031571629029097346 | Validation loss: 0.03225022736541201\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.03022792704007405 | Validation loss: 0.03138879530824128\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03081579728044687 | Validation loss: 0.0321810659581581\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.031975348947540394 | Validation loss: 0.03330272281196265\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03338813357143186 | Validation loss: 0.03498803695614359\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03510964766335417 | Validation loss: 0.03675926781663325\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.03621827504931267 | Validation loss: 0.04031967806868848\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03502747318872405 | Validation loss: 0.03644606344138099\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.033388999948923394 | Validation loss: 0.03493262910638499\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.031588033860087336 | Validation loss: 0.03346493154500438\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.030269897492477802 | Validation loss: 0.031828868301766636\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888 ]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.029260817702536978 | Validation loss: 0.030966343737281528\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.02962339904496637 | Validation loss: 0.03175661460686047\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.030601339510964654 | Validation loss: 0.03278937654784032\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03195925210107587 | Validation loss: 0.034539599556772584\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03342047210885432 | Validation loss: 0.03554835073254277\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03485352619034128 | Validation loss: 0.03597379817866382\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03383251438632343 | Validation loss: 0.03613145378278156\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.032122112861027806 | Validation loss: 0.03380987030841344\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.030471733455018208 | Validation loss: 0.03271121671835406\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.029372684216662125 | Validation loss: 0.031361686634476735\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.028345677224317873 | Validation loss: 0.03078324074931113\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.02855107771859717 | Validation loss: 0.03187606106223786\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.029404923101652443 | Validation loss: 0.03323598527473159\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.03089467578340234 | Validation loss: 0.03677618085711667\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.032298622542889566 | Validation loss: 0.03654344159846021\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.033855747049271065 | Validation loss: 0.03389050771144375\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.032686932765013416 | Validation loss: 0.03563051058424521\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03117338846085637 | Validation loss: 0.036191234661810165\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.029448336268973162 | Validation loss: 0.03330194152298227\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.028292122928565002 | Validation loss: 0.03124773993560698\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.027255275193235184 | Validation loss: 0.030370980245679354\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.027862145766197874 | Validation loss: 0.031165265132038468\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.028608309474451043 | Validation loss: 0.03325484803490407\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.02942115707344955 | Validation loss: 0.03360139277755423\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.031027071798535606 | Validation loss: 0.035984445478668255\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03268752752080941 | Validation loss: 0.04319226613218805\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.03163052153018281 | Validation loss: 0.033679972509894754\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.030197662804605747 | Validation loss: 0.03366826350215526\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.028440817642414313 | Validation loss: 0.03247853203684883\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.02726938468074118 | Validation loss: 0.031231511301830807\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.026660842424089923 | Validation loss: 0.030472978850645302\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.026930124313739225 | Validation loss: 0.030931837073799255\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.027782215082313953 | Validation loss: 0.03232833159576475\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.02897368960804856 | Validation loss: 0.03295310706196897\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.030728193830449398 | Validation loss: 0.034547940753729994\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.03180026859100028 | Validation loss: 0.03586612556096727\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.030403358757305217 | Validation loss: 0.036287696990885035\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.029338854772962747 | Validation loss: 0.033448813400701084\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.027786362097657277 | Validation loss: 0.03349754812640954\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.02651610360645843 | Validation loss: 0.03075202837982009\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.025863564623644444 | Validation loss: 0.030081888875075145\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.025841036838054013 | Validation loss: 0.030718148438737984\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.026887093637666597 | Validation loss: 0.03189418640449247\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.0282036214269419 | Validation loss: 0.033052585845961505\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.02943545867190293 | Validation loss: 0.03484091803068872\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.031106999646285622 | Validation loss: 0.03587077220127118\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.029798714455041127 | Validation loss: 0.03478194975945274\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.028319378263375713 | Validation loss: 0.03545742583320995\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.02707512585783568 | Validation loss: 0.031950104305833844\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.025894583061733468 | Validation loss: 0.03058983964960923\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.02535024630070818 | Validation loss: 0.030067410971145188\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.025085657799715454 | Validation loss: 0.03055939608278264\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.026115530444580447 | Validation loss: 0.03141266045745759\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.027371244338216392 | Validation loss: 0.03283754703217903\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.028676321570061176 | Validation loss: 0.0335505915624378\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.030171366977710716 | Validation loss: 0.036135951370264575\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.028916694708791834 | Validation loss: 0.03220553534616411\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.02758560871511082 | Validation loss: 0.034407987646692624\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.026476486200580417 | Validation loss: 0.032068598825913085\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.025373654041299962 | Validation loss: 0.03088633716930594\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.02446029111412977 | Validation loss: 0.029658473261623783\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.024613056748951454 | Validation loss: 0.030765600189302876\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.02563061149273889 | Validation loss: 0.031055306257531706\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.026793520601546963 | Validation loss: 0.03368187746016589\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.028121095472455435 | Validation loss: 0.034668188031136464\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.02954825781392095 | Validation loss: 0.036907955427217275\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.02867951834992337 | Validation loss: 0.03384090841343972\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.027033594139030306 | Validation loss: 0.03186083252055455\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.02586826024298358 | Validation loss: 0.03187882715621881\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.02469652157055038 | Validation loss: 0.030002151368663903\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.024060468244417682 | Validation loss: 0.02966953616226669\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.02407615681684862 | Validation loss: 0.030171342457817718\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.025213620124942087 | Validation loss: 0.03115276776386046\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.02618761574271993 | Validation loss: 0.03224841546498041\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.02742909563532994 | Validation loss: 0.033071257239949386\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.028960566331566438 | Validation loss: 0.03520545495294892\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.027878794489578734 | Validation loss: 0.03331690206629249\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.026770222055777088 | Validation loss: 0.03253524844427552\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.025255439297926768 | Validation loss: 0.031415996751624396\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.023955525235478274 | Validation loss: 0.030671831624236253\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.023950381831936977 | Validation loss: 0.02982574759001753\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.023566104945594694 | Validation loss: 0.030525477438242035\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.02436839387015124 | Validation loss: 0.03077274331806508\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.025670240620356492 | Validation loss: 0.03336958942390912\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.026950350978162403 | Validation loss: 0.03207134384563012\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.02825807590371247 | Validation loss: 0.03577499762507139\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.027466312799727412 | Validation loss: 0.034949956346402127\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.025565710374315483 | Validation loss: 0.03352450120336978\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.024536826447987065 | Validation loss: 0.030833024449184933\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.023544609255164745 | Validation loss: 0.030002048099001425\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.02321960965692117 | Validation loss: 0.02953936348347801\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.02307436841917701 | Validation loss: 0.030252535151630903\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.02397868466677188 | Validation loss: 0.031158533524227355\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.025020400550781096 | Validation loss: 0.032842995162097224\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.026263095045693248 | Validation loss: 0.033730272541597356\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.02800633968862404 | Validation loss: 0.03604198408733427\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.02651089855550429 | Validation loss: 0.03513341484410045\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.02560398147412114 | Validation loss: 0.0321963859095642\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.02422986511053063 | Validation loss: 0.03157056310106959\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.023086803899271282 | Validation loss: 0.030515983684268673\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.022762703376558176 | Validation loss: 0.02935167956701686\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.022699793172343892 | Validation loss: 0.030329733221602123\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.023518949007344116 | Validation loss: 0.030502196112894907\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.024388733594935007 | Validation loss: 0.03174259513616562\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.025955424753135056 | Validation loss: 0.03257236281096672\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.027296889776833297 | Validation loss: 0.03400973833899582\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.026026336777786627 | Validation loss: 0.03210558142282267\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.025013451092311834 | Validation loss: 0.03260836849170449\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.02365047061141199 | Validation loss: 0.03127120152544395\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.022748118945705432 | Validation loss: 0.030008847141160373\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.022331331235972623 | Validation loss: 0.029065387826248082\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.022117540770114994 | Validation loss: 0.029946955860452314\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.02293831821417392 | Validation loss: 0.030338736745504153\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.024056396900408526 | Validation loss: 0.03252197858935173\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.02526241676986408 | Validation loss: 0.032178846088988064\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.026862020887559148 | Validation loss: 0.0375595488214651\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.025642202743585772 | Validation loss: 0.03363440044850639\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.024161111101907804 | Validation loss: 0.032072389045054404\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.02327819517813623 | Validation loss: 0.030471804131448798\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.022183483882667863 | Validation loss: 0.02956509131905252\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.022008079754951313 | Validation loss: 0.029146107779074032\n",
      "Validation loss (ends of cycles): [0.16685935 0.04502044 0.03929079 0.03624861 0.03444946 0.03320953\n",
      " 0.03203853 0.0313888  0.03096634 0.03078324 0.03037098 0.03047298\n",
      " 0.03008189 0.03006741 0.02965847 0.02966954 0.02982575 0.02953936\n",
      " 0.02935168 0.02906539 0.02914611]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 16\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2737787661369978 | Validation loss: 0.20023219089592453\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.10422682451216254 | Validation loss: 0.09987394712799419\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08268818595468647 | Validation loss: 0.07145754689663912\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07512161015041584 | Validation loss: 0.06633957702897292\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07036189057893581 | Validation loss: 0.06764761305752054\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06704740458366087 | Validation loss: 0.06609862109860487\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06204092073965612 | Validation loss: 0.05567961194412371\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05805403113504505 | Validation loss: 0.05421012262525284\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.054467795774208634 | Validation loss: 0.05056867451794379\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05140738620965441 | Validation loss: 0.04763612818731144\n",
      "Validation loss (ends of cycles): [0.20023219]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.049082192368658746 | Validation loss: 0.04559225025298321\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04975575218380495 | Validation loss: 0.046683357748310124\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05101741386364823 | Validation loss: 0.047293581431154655\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05192214983339444 | Validation loss: 0.0476417343206374\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.053264485360453216 | Validation loss: 0.05344951939068537\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05388570320975827 | Validation loss: 0.05068364296009583\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.051310861563882024 | Validation loss: 0.04789036223912133\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04877522815290222 | Validation loss: 0.051986201707504495\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04632410562347885 | Validation loss: 0.043588970103754406\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04381964350573894 | Validation loss: 0.042146987932841336\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04200194686500898 | Validation loss: 0.03990887502719343\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04261076600862828 | Validation loss: 0.041349465662069554\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.044018229102994515 | Validation loss: 0.044328473145719124\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04533218726970431 | Validation loss: 0.04635683298770305\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.04664624337194942 | Validation loss: 0.05748243245864864\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04778335881456146 | Validation loss: 0.04696861471552237\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.045971616663544726 | Validation loss: 0.04174737498757586\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04344583072655607 | Validation loss: 0.04394390559301967\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.04164245962000298 | Validation loss: 0.041117376507779135\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.0392682946013932 | Validation loss: 0.03982357207718676\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.037741293180110595 | Validation loss: 0.036664327384030394\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.038508784161572614 | Validation loss: 0.03875069599598646\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.03946622626353642 | Validation loss: 0.03949111793190241\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04122776931929072 | Validation loss: 0.043168582302411045\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.04259670557505561 | Validation loss: 0.042725293262474304\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04403580553411675 | Validation loss: 0.04463229184986743\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04248637171447512 | Validation loss: 0.040492692583166394\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.040044852228116566 | Validation loss: 0.038837947511831214\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.0383974302965253 | Validation loss: 0.03937832881754215\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.03638932610473295 | Validation loss: 0.03644921438587186\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.03480823439413931 | Validation loss: 0.03473438378588288\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.03557032718246172 | Validation loss: 0.036001175196956746\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.036576151781840115 | Validation loss: 0.037461821863477206\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.038131684358195056 | Validation loss: 0.03910830907062092\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.03957182635905177 | Validation loss: 0.04135864841199554\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.04120923336925293 | Validation loss: 0.04687123353370523\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.03937091541890876 | Validation loss: 0.04366569883659878\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.03759949713465061 | Validation loss: 0.03793287259913915\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.035793748596228483 | Validation loss: 0.036690053769049394\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.034185784380318315 | Validation loss: 0.0347253294348453\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03280941256061636 | Validation loss: 0.033420166544682155\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03306701497768792 | Validation loss: 0.03450813103236456\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03430615315089134 | Validation loss: 0.03600950290209952\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.03589855856720475 | Validation loss: 0.037268991961218086\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.037301842755762725 | Validation loss: 0.04395043955440015\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.038821042116592076 | Validation loss: 0.043963905704100574\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.03712832113071601 | Validation loss: 0.040459541430845194\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.035522612095920475 | Validation loss: 0.038693558564821706\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03354250567352883 | Validation loss: 0.03509865964935944\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03192239735774168 | Validation loss: 0.03395114307481367\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.030853263973295103 | Validation loss: 0.03248945271711698\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03124623723130116 | Validation loss: 0.03348119809926875\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.032311980573159794 | Validation loss: 0.035267103158465\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.033898110482218406 | Validation loss: 0.03626691218696337\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.03541915445023869 | Validation loss: 0.03732870159819063\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.03680497418546418 | Validation loss: 0.04455761301570234\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03531477025592714 | Validation loss: 0.0376130314873515\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.033475336373057656 | Validation loss: 0.03557608191715141\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03201586188103153 | Validation loss: 0.03565459449536505\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.030342980738047247 | Validation loss: 0.033357611354605285\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.029202395655933445 | Validation loss: 0.03172996544600588\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.029675779442530213 | Validation loss: 0.03339542720499819\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.030540388210412613 | Validation loss: 0.03487979933059057\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03213733445218669 | Validation loss: 0.03967391237187966\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03372008166198919 | Validation loss: 0.04293370014468653\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.03520092460983176 | Validation loss: 0.04376732475594082\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.033938281122580405 | Validation loss: 0.03703245200810179\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.032296309855355934 | Validation loss: 0.03492208946067675\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.030508256989428554 | Validation loss: 0.035185562314844765\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.02909078923380369 | Validation loss: 0.03294597795249614\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02803419754514104 | Validation loss: 0.031490433136973764\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.028328925535251483 | Validation loss: 0.032534939574852456\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.02926808329002096 | Validation loss: 0.03435256992326637\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03089131600513145 | Validation loss: 0.035728449348063594\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.032449012219825185 | Validation loss: 0.037415493403322404\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03403688801679848 | Validation loss: 0.04564265021642225\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03234556234463697 | Validation loss: 0.041697077766324565\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03099816353665298 | Validation loss: 0.03541089097384067\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.029276077776751296 | Validation loss: 0.03348477607576457\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.027866539251110628 | Validation loss: 0.032401630176907094\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.026741057555521214 | Validation loss: 0.030844259166480165\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.02716905342902636 | Validation loss: 0.03190479325377835\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.02814049190863615 | Validation loss: 0.03378076414552938\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.02952301814257512 | Validation loss: 0.034746238747529225\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.031075982355850008 | Validation loss: 0.03731691609837313\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.03271141126256905 | Validation loss: 0.060955908380251014\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.03134089423614869 | Validation loss: 0.03622718771045978\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.029479217560646514 | Validation loss: 0.033980404232851175\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.028381742420606315 | Validation loss: 0.03344887570865386\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.02674665158214854 | Validation loss: 0.03224219867250824\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.0260751176383493 | Validation loss: 0.030910984786078993\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.02635237188665123 | Validation loss: 0.03226050084242515\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.027261614670251004 | Validation loss: 0.033868492984798104\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.028737410438232358 | Validation loss: 0.03410222200033939\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.03006675193823258 | Validation loss: 0.056256531152577526\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03191802492016912 | Validation loss: 0.048147553182413094\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.030185575500739314 | Validation loss: 0.03620794531683215\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.029039505783901146 | Validation loss: 0.03526321702958208\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.027365574273134134 | Validation loss: 0.03291359628690819\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.026011773122027224 | Validation loss: 0.03202740053555607\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.02524538153294826 | Validation loss: 0.030587680316406542\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.02533113995959293 | Validation loss: 0.032261734877803684\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.02640439592041515 | Validation loss: 0.03358038018696603\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.027467445776555834 | Validation loss: 0.03410669124429732\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.029203762646808518 | Validation loss: 0.03419871860703008\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.030872044258010316 | Validation loss: 0.035971489265165496\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.029417167569869968 | Validation loss: 0.03570391744605999\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.0279873248426185 | Validation loss: 0.03376021398544575\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.02645933153804421 | Validation loss: 0.032196724564826064\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.025299409457563062 | Validation loss: 0.03130617110569656\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.024414063379989834 | Validation loss: 0.030250687490060792\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.024591018528747188 | Validation loss: 0.03162372223538371\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.025364326536802092 | Validation loss: 0.03282762849621013\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.026572306790777784 | Validation loss: 0.03422165076058786\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.028306469948702 | Validation loss: 0.035084132006210564\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.030099466125898824 | Validation loss: 0.04039957774061281\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.028633888242546264 | Validation loss: 0.037205871458338424\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.027315277815246442 | Validation loss: 0.03901227530652443\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.025800537531113794 | Validation loss: 0.03346189024108174\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.024668992970021456 | Validation loss: 0.031561887114251085\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.02404350608699097 | Validation loss: 0.030343229462087683\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.02407458088888721 | Validation loss: 0.031308728204298336\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.024918176032104125 | Validation loss: 0.03360111465825971\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.026058742077104513 | Validation loss: 0.03443116572710265\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.027624107491203004 | Validation loss: 0.03449264597312539\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.02928206292494369 | Validation loss: 0.03776389098338849\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.028131398687681813 | Validation loss: 0.03479092995083965\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.026286100760637598 | Validation loss: 0.0348371911167571\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.0250626021893475 | Validation loss: 0.03248374907514163\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.024081416701252654 | Validation loss: 0.03174565414345897\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.02352582935544508 | Validation loss: 0.030051779883823036\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.023359468449677127 | Validation loss: 0.0312756019174657\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.02415206788408416 | Validation loss: 0.03251146805365529\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.02550879377490423 | Validation loss: 0.03490573369724825\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.026806120814821557 | Validation loss: 0.0345633335873089\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.0286723116860087 | Validation loss: 0.03890983096892591\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.027140730675713227 | Validation loss: 0.03415699658371442\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.02588838122154432 | Validation loss: 0.03404608176015647\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.024426499718074195 | Validation loss: 0.03193692797053177\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.023166143655314983 | Validation loss: 0.03149128182733481\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.0229603738123441 | Validation loss: 0.029910476128282272\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.02284284436792927 | Validation loss: 0.030855711153914445\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.02365900123194887 | Validation loss: 0.03222047771632144\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.024800017015003843 | Validation loss: 0.03301934850921409\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.026273147880437043 | Validation loss: 0.03420751300368425\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.027664226583410494 | Validation loss: 0.0349949984110694\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.026663722442444707 | Validation loss: 0.033630230348656136\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.025194934456550525 | Validation loss: 0.03472642444766465\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.023837915450283215 | Validation loss: 0.032156074790498326\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.023005821732950845 | Validation loss: 0.031500728331110645\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.02235974420061863 | Validation loss: 0.0296877749115888\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.022434936120911818 | Validation loss: 0.030912983049927033\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.02321441728148727 | Validation loss: 0.03159520993487234\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.024087414784378952 | Validation loss: 0.03426515600289655\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.02584073524720468 | Validation loss: 0.03549671795351052\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.027481816246130275 | Validation loss: 0.04569036421258893\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.025911257378610334 | Validation loss: 0.03599856464208755\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.0245359625546335 | Validation loss: 0.03235892685278591\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.02326479062573676 | Validation loss: 0.03178735901560403\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.02218883088652382 | Validation loss: 0.03116621243544912\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.021824004793609308 | Validation loss: 0.029547290860024174\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.021793495756885637 | Validation loss: 0.0311219496570066\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.022444999180954155 | Validation loss: 0.03314602608273251\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.023874418106018088 | Validation loss: 0.033780165544125884\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.025131154348927102 | Validation loss: 0.03473723620440053\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.0267372918269399 | Validation loss: 0.03518312112881546\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.02542390567012219 | Validation loss: 0.03591320638965189\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.02414658128038254 | Validation loss: 0.03431402006705778\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.02274914544788886 | Validation loss: 0.03193719099910386\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.021763978673577836 | Validation loss: 0.030923344120712935\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.02152854327009771 | Validation loss: 0.029603901574701334\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.021365951617912014 | Validation loss: 0.03074364638598883\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.021945311958300257 | Validation loss: 0.035094125188095904\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.02318344924848263 | Validation loss: 0.03233091902416364\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.0248239246356543 | Validation loss: 0.03261617678258799\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.026298621777577077 | Validation loss: 0.03375745768271453\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.024853772596514775 | Validation loss: 0.03509717840965079\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.023714199431211225 | Validation loss: 0.03329228342766256\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.022584474363224947 | Validation loss: 0.032880636723299994\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.02153710593188944 | Validation loss: 0.03080118345343961\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039 ]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.02122366759563085 | Validation loss: 0.029605589848242502\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.020808289958692088 | Validation loss: 0.030839143395094217\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.021608949259407526 | Validation loss: 0.03275987589979066\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.022862843535730513 | Validation loss: 0.03243703589634558\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.024137301308168903 | Validation loss: 0.034653909075840386\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.02555413278989406 | Validation loss: 0.03679713180733729\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.024435840169422504 | Validation loss: 0.03782723306686477\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.02327967907932741 | Validation loss: 0.03321316239910316\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.022098688396815884 | Validation loss: 0.03147691657222741\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.02113150341131937 | Validation loss: 0.03040765568745875\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.020685331293704413 | Validation loss: 0.029657005612631286\n",
      "Validation loss (ends of cycles): [0.20023219 0.04559225 0.03990888 0.03666433 0.03473438 0.03342017\n",
      " 0.03248945 0.03172997 0.03149043 0.03084426 0.03091098 0.03058768\n",
      " 0.03025069 0.03034323 0.03005178 0.02991048 0.02968777 0.02954729\n",
      " 0.0296039  0.02960559 0.02965701]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 17\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.19528153301839987 | Validation loss: 0.15108045474090406\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.09630404248481661 | Validation loss: 0.07499229344250881\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.0770322958220119 | Validation loss: 0.0657210370476267\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07153833540721144 | Validation loss: 0.06682331098523815\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06794721161301329 | Validation loss: 0.058392948561669454\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06519884280823758 | Validation loss: 0.05638599817731739\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06049888590204082 | Validation loss: 0.062485484785474506\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.056857929563179144 | Validation loss: 0.04876981789361587\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.053296953524044885 | Validation loss: 0.04630405498685035\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05026454213588667 | Validation loss: 0.04334998397832423\n",
      "Validation loss (ends of cycles): [0.15108045]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04759277868282607 | Validation loss: 0.04201392273921355\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04893287182528907 | Validation loss: 0.042212111397390874\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.0500142074952738 | Validation loss: 0.04350175510729309\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05130318975384076 | Validation loss: 0.04694888262753993\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05227278390583971 | Validation loss: 0.048200364991099434\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05314655587678467 | Validation loss: 0.05896381667889325\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05050081228030714 | Validation loss: 0.04554605564778357\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.048094492702649566 | Validation loss: 0.0431616822735662\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04548985188584921 | Validation loss: 0.04048775626033281\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04332584545280286 | Validation loss: 0.03895747074774936\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04135821033525831 | Validation loss: 0.037356803407975\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04213623469237121 | Validation loss: 0.038663210462679905\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04348462626730334 | Validation loss: 0.04059584674516083\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.044896001835889 | Validation loss: 0.0408192587175728\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.04613425673396275 | Validation loss: 0.040819176052392055\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04764102466081775 | Validation loss: 0.04785382432813665\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04551267926581204 | Validation loss: 0.04141203267911894\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04350365237809542 | Validation loss: 0.038893470408774056\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.041088982838147325 | Validation loss: 0.03846450380783165\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.039170486238367094 | Validation loss: 0.035727420409696294\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03741839054172609 | Validation loss: 0.03534357691496874\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.038331123063428785 | Validation loss: 0.03712475580056157\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.03958883121752686 | Validation loss: 0.03590556875332794\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04092563144536322 | Validation loss: 0.03797215149136244\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.042431336150167316 | Validation loss: 0.03862651510048756\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04385021059612531 | Validation loss: 0.04650221689216859\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.042333089369789176 | Validation loss: 0.04138898033549828\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.040143472958341475 | Validation loss: 0.04083230859080775\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.038212460662810296 | Validation loss: 0.03604175413307627\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.03621550740795137 | Validation loss: 0.03490787063750018\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.0347418464531418 | Validation loss: 0.033902818650270984\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.03551816663844144 | Validation loss: 0.0339503469918154\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.03664073076623484 | Validation loss: 0.036111429589182405\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.03794130406940106 | Validation loss: 0.038581867809448625\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.039544879462511284 | Validation loss: 0.03753844731017551\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.04120968023817399 | Validation loss: 0.040164445156017234\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.03963290285832417 | Validation loss: 0.03767492653455882\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.037655649592139295 | Validation loss: 0.03940686863739934\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.0360215248523544 | Validation loss: 0.03606964662605155\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.03379709184294435 | Validation loss: 0.03300323327426362\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.032713453326645624 | Validation loss: 0.032592986331247124\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03327064899779035 | Validation loss: 0.0332814001074407\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03456818787784382 | Validation loss: 0.03814794470976412\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.035899267839011186 | Validation loss: 0.03423762608286554\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.03718895989829513 | Validation loss: 0.037633575235320404\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.038761256454241146 | Validation loss: 0.0388596230145313\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.03721329200770852 | Validation loss: 0.03893185608023036\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.035715234900392065 | Validation loss: 0.03628284927556472\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03370676121645145 | Validation loss: 0.033211706354554775\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.0323047479999611 | Validation loss: 0.03253601488978725\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.030942986885225034 | Validation loss: 0.03169571242369382\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03151171489281389 | Validation loss: 0.03195847763754098\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.0326535613743559 | Validation loss: 0.03367336982020498\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.03397060671223577 | Validation loss: 0.0392299928265599\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.03557851326406208 | Validation loss: 0.036114410315928734\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.036995837769258445 | Validation loss: 0.046228273697527106\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03573583279219346 | Validation loss: 0.035999696652314304\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.03398162927240221 | Validation loss: 0.033870757285472564\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.032125317916919395 | Validation loss: 0.03275708983594601\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.030659903030600545 | Validation loss: 0.030907584725162095\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.029677720091445006 | Validation loss: 0.030892113479167486\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.030018180326419317 | Validation loss: 0.030626152551411528\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03106435915953883 | Validation loss: 0.033222900274976166\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.032380191656929534 | Validation loss: 0.03328017588803726\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03383174836525591 | Validation loss: 0.03539824518745979\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.03549576116777135 | Validation loss: 0.03629769839807949\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.034087463657877695 | Validation loss: 0.03376856591321726\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03234960998346164 | Validation loss: 0.03851411953172852\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03068069219369236 | Validation loss: 0.0314846167884833\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.029369526592222608 | Validation loss: 0.030833860068062765\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02836777383780591 | Validation loss: 0.030553860598103662\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.028889779986879662 | Validation loss: 0.031743324707896835\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.029690888467406137 | Validation loss: 0.031142891442353746\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.031053277377889852 | Validation loss: 0.03351418073165469\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03267707209850801 | Validation loss: 0.042135526879435094\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03411234047458192 | Validation loss: 0.0370925584132165\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03266451155994175 | Validation loss: 0.03274107290909881\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.031198181013034027 | Validation loss: 0.0340254512013851\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.029595627138706466 | Validation loss: 0.031916605308651924\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.028043366407638225 | Validation loss: 0.030369010976458017\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.027346802133824823 | Validation loss: 0.030075445589897908\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.027608434050013935 | Validation loss: 0.030171571629105415\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.028327987452338294 | Validation loss: 0.03024076547605538\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.029624219871984107 | Validation loss: 0.03404002234471583\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.03148827917679528 | Validation loss: 0.03371854062167417\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.03307377927529618 | Validation loss: 0.04036632132411531\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.031799388683665046 | Validation loss: 0.03298156123311646\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03007377838939575 | Validation loss: 0.033205109755549814\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.028580836875650183 | Validation loss: 0.030641627497971058\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.027427082016008106 | Validation loss: 0.0313515687625097\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.026483976918338672 | Validation loss: 0.030348778084184215\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.026674966022599576 | Validation loss: 0.03149610185023166\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.02751217969953574 | Validation loss: 0.03084385859887157\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.028731230297125876 | Validation loss: 0.031040501080255592\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.030062061833496405 | Validation loss: 0.03604654427123281\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.0318882070439611 | Validation loss: 0.039417957194742906\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.030514564989391334 | Validation loss: 0.03316041232144411\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.028916336958091267 | Validation loss: 0.035398528771062865\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.02764455142969955 | Validation loss: 0.031008046623154553\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.02622884780894525 | Validation loss: 0.031359491927499795\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.02562284080117325 | Validation loss: 0.030416654530022524\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.02577647100859065 | Validation loss: 0.03090014162811294\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.026568011741894555 | Validation loss: 0.030865917913615704\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.028003387864473768 | Validation loss: 0.03234168109937315\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.029407237495999695 | Validation loss: 0.032332020902396306\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.03110336661650469 | Validation loss: 0.03286148103333152\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.02993302761877733 | Validation loss: 0.035776853462499855\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.02837571760045555 | Validation loss: 0.03131979187966975\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.02665195077214038 | Validation loss: 0.031021781488264028\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.02563444459370858 | Validation loss: 0.030956934082560835\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.024944440375529522 | Validation loss: 0.030078876598746376\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.02519254589051844 | Validation loss: 0.03037429168259939\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.02574926438213392 | Validation loss: 0.030591894485122336\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.027375541950070012 | Validation loss: 0.03259252623316988\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.028651660226659454 | Validation loss: 0.03243902798709089\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.030220652338278694 | Validation loss: 0.03472108380720679\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.02893120789346762 | Validation loss: 0.0334128510635511\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.02742837137619664 | Validation loss: 0.03295834002340526\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.02586745542719755 | Validation loss: 0.03359922730421598\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.02518866923601022 | Validation loss: 0.030972652326840742\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.024370399135069585 | Validation loss: 0.029321946928986407\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.024452643489142455 | Validation loss: 0.03170669283223363\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.02514903414646149 | Validation loss: 0.02934865020545183\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.026473972576076355 | Validation loss: 0.03256165103541803\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.027980016625126985 | Validation loss: 0.037934470789886154\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.02950241145545866 | Validation loss: 0.033272428256748\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.02817217055726503 | Validation loss: 0.03544298366569312\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.02676052859971505 | Validation loss: 0.03308902620708784\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.025434287322692456 | Validation loss: 0.0314219071554531\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.024347204041271286 | Validation loss: 0.03068882338208171\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.023864611201391623 | Validation loss: 0.029601984030970956\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.023795595388918175 | Validation loss: 0.032649262017051206\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.0245595584966345 | Validation loss: 0.03269626537288448\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.025921545380095796 | Validation loss: 0.03559081084136151\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.02733462275735535 | Validation loss: 0.033264496363699436\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.029037256825964634 | Validation loss: 0.0364198326833744\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.027521122688808897 | Validation loss: 0.03114709215222207\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.026036638752777334 | Validation loss: 0.03610302006776354\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.024805288894769302 | Validation loss: 0.030778389053204947\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.023793241487610646 | Validation loss: 0.030905219281207673\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.02341242203824442 | Validation loss: 0.029345920012719864\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.023260569254919067 | Validation loss: 0.030717732623812898\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.023884564654637626 | Validation loss: 0.03010071575872402\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.025170819309547426 | Validation loss: 0.029870342207759356\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.026484384465463987 | Validation loss: 0.04114546122408546\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.028278595922830125 | Validation loss: 0.037228376563407675\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.026891657869023543 | Validation loss: 0.035947980639417614\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.025646816195707446 | Validation loss: 0.03072655048485087\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.024305647449003254 | Validation loss: 0.03094838262922996\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.0235144635589104 | Validation loss: 0.03033747435011695\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.02274166181863205 | Validation loss: 0.028855146320981788\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.022850188836418678 | Validation loss: 0.030827936905938966\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.023490403991056914 | Validation loss: 0.03056321562622237\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.024536157205634877 | Validation loss: 0.030726340179026656\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.025968177275233498 | Validation loss: 0.03182310807164264\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.02745464058156587 | Validation loss: 0.03240670689043745\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.02638791023944248 | Validation loss: 0.03207409606867396\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.025040658959004237 | Validation loss: 0.03346892037486608\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.023840111028586373 | Validation loss: 0.03250710043924308\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.022957657927676567 | Validation loss: 0.03261066463866592\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.02253573403536071 | Validation loss: 0.028775880929181534\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.022440487484990317 | Validation loss: 0.030062347749429466\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.022867488988315848 | Validation loss: 0.03976280561986223\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.02382546374629303 | Validation loss: 0.03142198853491418\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.025262574162004208 | Validation loss: 0.03314230741059358\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.02710123380965106 | Validation loss: 0.03711360927044818\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.025813494960220135 | Validation loss: 0.034733611777161076\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.0244471754999758 | Validation loss: 0.030967152215936017\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.023251139880598 | Validation loss: 0.03004695099275724\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.0224692894349344 | Validation loss: 0.03162622818542001\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.021959586049738068 | Validation loss: 0.0287618160874179\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.021884380696766723 | Validation loss: 0.03256968013216964\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.02237591364670281 | Validation loss: 0.03341920127416343\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.023713711671179204 | Validation loss: 0.029729280975210454\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.025079944019763194 | Validation loss: 0.03119673509051842\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.02635635260866559 | Validation loss: 0.03225741138170778\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.025441277809771085 | Validation loss: 0.03134904677394481\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.024010141896208616 | Validation loss: 0.031274090043013604\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.023025057314552264 | Validation loss: 0.03273896375193005\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.022124788933800255 | Validation loss: 0.03352637900517578\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.02186805220626984 | Validation loss: 0.029281317882767294\n",
      "Validation loss (ends of cycles): [0.15108045 0.04201392 0.0373568  0.03534358 0.03390282 0.03259299\n",
      " 0.03169571 0.03089211 0.03055386 0.03007545 0.03034878 0.03041665\n",
      " 0.03007888 0.02932195 0.02960198 0.02934592 0.02885515 0.02877588\n",
      " 0.02876182 0.02928132]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 18\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.14745687956987755 | Validation loss: 0.11181736810017476\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.09623743342162352 | Validation loss: 0.08580441937773628\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08750657575556964 | Validation loss: 0.08224849567740364\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08265806757294991 | Validation loss: 0.07812433627195063\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07909638743011618 | Validation loss: 0.10523036336608693\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07519952726997728 | Validation loss: 0.07433472433646696\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06963092798191145 | Validation loss: 0.0681168861621249\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.0647309234413487 | Validation loss: 0.061909659196977065\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.0603061709351339 | Validation loss: 0.06635272014985043\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.056970927845980006 | Validation loss: 0.053171325606846176\n",
      "Validation loss (ends of cycles): [0.11181737]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.054451444431934067 | Validation loss: 0.050428496311064316\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.05530370387989353 | Validation loss: 0.05998936325179792\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.0562868577292497 | Validation loss: 0.08638219957330585\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.0572645927277049 | Validation loss: 0.06002994229506075\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05791082459604588 | Validation loss: 0.07916613048420543\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.058337017147633154 | Validation loss: 0.06136379625021884\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0556387303206395 | Validation loss: 0.05869993445488204\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05294741382895727 | Validation loss: 0.05184250018369835\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.05023024163912894 | Validation loss: 0.05074060479162541\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.047944747648517215 | Validation loss: 0.04994288424804675\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.045913187161777315 | Validation loss: 0.04389677417621148\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.047116746218493724 | Validation loss: 0.04913257856943966\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.048278160490403614 | Validation loss: 0.04979463033707796\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04970042983906006 | Validation loss: 0.05419276432191904\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05063182357918677 | Validation loss: 0.05380100643146882\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05186005225991757 | Validation loss: 0.05329234860178116\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04985330287572436 | Validation loss: 0.054778525721182865\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.047700814294122804 | Validation loss: 0.04927220066959879\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.04578730350316275 | Validation loss: 0.04668996552318598\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.04332482842838494 | Validation loss: 0.04906922478261774\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.04196088215130873 | Validation loss: 0.04077261508302351\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.042725179914575744 | Validation loss: 0.04192748965810886\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.043890829696723324 | Validation loss: 0.0423654904506639\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04479648191188499 | Validation loss: 0.045411624986909135\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.046417209704765885 | Validation loss: 0.04752046215982564\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04789944863548194 | Validation loss: 0.04866404211626644\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04611892869886686 | Validation loss: 0.049108236747901\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.044292686737186504 | Validation loss: 0.049626477540726154\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.042089813981043896 | Validation loss: 0.05028616577650594\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.040471516651336074 | Validation loss: 0.04539883885104044\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.038953275320772814 | Validation loss: 0.038260011332093086\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.039753880176700535 | Validation loss: 0.03953419919166945\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.04075378145267233 | Validation loss: 0.04775480268341777\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.04191550611716321 | Validation loss: 0.05315274405664047\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04337615864333059 | Validation loss: 0.04678723331441922\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.0448561115373718 | Validation loss: 0.04518167654761171\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.04328001322276069 | Validation loss: 0.04366327006270928\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.04148955256348168 | Validation loss: 0.04564844665274156\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.03969575352251794 | Validation loss: 0.04270295615810736\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.03778248318328016 | Validation loss: 0.045216823161571426\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.036728900827020114 | Validation loss: 0.036400728900216325\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.037182638656318656 | Validation loss: 0.039920183492049705\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03852524513958066 | Validation loss: 0.04021004818182076\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.039626261542045224 | Validation loss: 0.039501024509030105\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.04119911792644072 | Validation loss: 0.04457332704842618\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.042616045866968245 | Validation loss: 0.045998089163836124\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.04113453015039756 | Validation loss: 0.050729175304285196\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.03976550421266868 | Validation loss: 0.04352686356390472\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03769710026296428 | Validation loss: 0.0416220211283823\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03594528460933819 | Validation loss: 0.041781470377360824\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.034993118661507144 | Validation loss: 0.03542220634531922\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03528238928649487 | Validation loss: 0.041782269765317966\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.036277946592565245 | Validation loss: 0.048255273973387955\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.037810007194570436 | Validation loss: 0.04529560640849899\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.039236139712349224 | Validation loss: 0.046266291455357475\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.040906800351318294 | Validation loss: 0.06313739067553419\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03917124048407065 | Validation loss: 0.04793259138818336\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.037500780060205345 | Validation loss: 0.04579635729304457\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.0361412501221112 | Validation loss: 0.04605197491107789\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03438679540280517 | Validation loss: 0.0456059823983011\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.033415974244314035 | Validation loss: 0.03462613776194311\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03395316714611579 | Validation loss: 0.038469126250232216\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03479183258353109 | Validation loss: 0.03594474876876426\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03633535791401143 | Validation loss: 0.04774723141000862\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.0374829898621359 | Validation loss: 0.04101652198726625\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.0391945250370855 | Validation loss: 0.0401327273915563\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03788902773430175 | Validation loss: 0.03995021490330717\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03639131400176859 | Validation loss: 0.03722566730482916\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.0348304618996986 | Validation loss: 0.04006522394453002\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03319978419834323 | Validation loss: 0.04412269613713817\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.03219999324468031 | Validation loss: 0.03391640984445019\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.032688427642244465 | Validation loss: 0.037923850531203555\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.03355343060890638 | Validation loss: 0.04907491460310674\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03503366941531137 | Validation loss: 0.040808099231361285\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.036361888898023234 | Validation loss: 0.04570449341451172\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.037688679410159354 | Validation loss: 0.03851071792959639\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.036509045416225246 | Validation loss: 0.05178985225481797\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03476176010243096 | Validation loss: 0.04965500327535963\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.033466790973274846 | Validation loss: 0.04419837694252487\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.03203875968768078 | Validation loss: 0.037252725571789574\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.0312534848193264 | Validation loss: 0.03305692382288718\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.031425257888072586 | Validation loss: 0.035670075104036164\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.032341907633189844 | Validation loss: 0.05314765029908281\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.03363316069544095 | Validation loss: 0.05042708106338978\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.03508961931472336 | Validation loss: 0.04901893657788766\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.036707183874146204 | Validation loss: 0.06835057373793252\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.0352418556319931 | Validation loss: 0.05205297728885064\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03360738364271966 | Validation loss: 0.03979173940565206\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.032393003445426664 | Validation loss: 0.04202516406642652\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.030928450876729578 | Validation loss: 0.03461811653610352\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.030302010686503445 | Validation loss: 0.032624518037237954\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.030482625337610914 | Validation loss: 0.03657771112908304\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.031544560573877785 | Validation loss: 0.04379735982655424\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.03267700951289767 | Validation loss: 0.05044605840096431\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.033956850249151606 | Validation loss: 0.039384911364290565\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.035652619702228175 | Validation loss: 0.040526895561313205\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.0343283357183031 | Validation loss: 0.037462938774739746\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.0329232818115224 | Validation loss: 0.04072877669097048\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.03152846630963343 | Validation loss: 0.039494824257835875\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.030297402225574113 | Validation loss: 0.035708916085088145\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.02962929017907815 | Validation loss: 0.032125119894611094\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.029658222022855025 | Validation loss: 0.03462422362207312\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.030390083743992637 | Validation loss: 0.037167447627381944\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.03176091816716307 | Validation loss: 0.035366525972443344\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.0330082182680792 | Validation loss: 0.03707461589864925\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.03469608666642943 | Validation loss: 0.05098501968700274\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.033389072871664316 | Validation loss: 0.05167623942272853\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.0319534340987026 | Validation loss: 0.03803503140807152\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.03054207461651444 | Validation loss: 0.037278614163530614\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.029319434328642712 | Validation loss: 0.03323572590551545\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.028277252811762528 | Validation loss: 0.031856293105973606\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.028927232044396966 | Validation loss: 0.03381007744938926\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.029823541352488686 | Validation loss: 0.036562808286563483\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.030995890572410457 | Validation loss: 0.03735244338425387\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.03232222717754015 | Validation loss: 0.0393577231737101\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.033686123715024295 | Validation loss: 0.039191240626098835\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.0328642097200949 | Validation loss: 0.04119740207308689\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.03124009068563991 | Validation loss: 0.045967824147206494\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.029692287028833166 | Validation loss: 0.03530660837855751\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.028460362538380007 | Validation loss: 0.034659543915330306\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.02782241098296748 | Validation loss: 0.03151542620848766\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.028115404061203105 | Validation loss: 0.0364043337779235\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.028829176158563594 | Validation loss: 0.0347792377069065\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.030203928205727298 | Validation loss: 0.03419680051109959\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.03136087996760515 | Validation loss: 0.04011434100700163\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.033034575681429444 | Validation loss: 0.040510393481338974\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.03180332440336594 | Validation loss: 0.03888363813140751\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.030477852163667694 | Validation loss: 0.03862581931186461\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.0292505194876695 | Validation loss: 0.03288874167751158\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.027981118863727165 | Validation loss: 0.03388794129961623\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.027372297242386486 | Validation loss: 0.031212307529243748\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.0276586958958848 | Validation loss: 0.03246931614667441\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.028360963867808595 | Validation loss: 0.034810532453115536\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.029655160085393453 | Validation loss: 0.03974294517420034\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.030910991771168655 | Validation loss: 0.04399756938878414\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.0325270962652083 | Validation loss: 0.061640194658420785\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.03149095214683357 | Validation loss: 0.03760540243073375\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.029875266428039535 | Validation loss: 0.035697247552792584\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.028500897502783305 | Validation loss: 0.03381003710581402\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.027681679243045883 | Validation loss: 0.032980115103444695\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.026877311056509146 | Validation loss: 0.031143162226452765\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.026977618478832576 | Validation loss: 0.03266228110719044\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.027789666411818893 | Validation loss: 0.034961970028492204\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.02894545074526631 | Validation loss: 0.03532901065077929\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.030454562234464945 | Validation loss: 0.040225627527168364\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.03164946059806375 | Validation loss: 0.037975336828854234\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.03068030412000875 | Validation loss: 0.04220050962361614\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.029348198092527922 | Validation loss: 0.03511155945603299\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.028144483990769277 | Validation loss: 0.03515485296668732\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.027031416854432482 | Validation loss: 0.034209251535677276\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.026445907158373787 | Validation loss: 0.030806712303475467\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.026552394952801033 | Validation loss: 0.03321779652836576\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.027329204999108424 | Validation loss: 0.035172222956883166\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.028377699224813657 | Validation loss: 0.03415004107936294\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.02962424848998684 | Validation loss: 0.0380124523576382\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.031245342592386104 | Validation loss: 0.03870395737477636\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.03022429422295733 | Validation loss: 0.03497945312080921\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.02867190224232196 | Validation loss: 0.034943901974938615\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.02769064278008167 | Validation loss: 0.0328868226667421\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.026110646025255674 | Validation loss: 0.032032206835868084\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.025901495435964463 | Validation loss: 0.03086405956244047\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.025832738417959734 | Validation loss: 0.03191902321161686\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.02658675519811855 | Validation loss: 0.032392649120131956\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.02785930638549864 | Validation loss: 0.04055770080272866\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.029304441305676724 | Validation loss: 0.03938528715707032\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.030693826810126817 | Validation loss: 0.042542108787899525\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.02968439479967154 | Validation loss: 0.03487302249182114\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.028509433252010934 | Validation loss: 0.035466368128834046\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.027170329408786133 | Validation loss: 0.03241105497180097\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.026051353732871964 | Validation loss: 0.031891154834127\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.025475371672082546 | Validation loss: 0.030502695054541118\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.02571635264591644 | Validation loss: 0.03273352388852993\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.02636262705855761 | Validation loss: 0.03367737308968749\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.02711716690467392 | Validation loss: 0.03316663689416858\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.028923610236319737 | Validation loss: 0.03617072011451278\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.029695096961955915 | Validation loss: 0.03914791269771821\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.02894029467949629 | Validation loss: 0.033959697874312376\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.02806392555055392 | Validation loss: 0.032832851015295074\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.026699587195840348 | Validation loss: 0.03351602898663388\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.02575078903052105 | Validation loss: 0.032145418597599576\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027 ]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.025277233614740293 | Validation loss: 0.030475220247377866\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.02529354150293351 | Validation loss: 0.031475040253944105\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.02586796419332024 | Validation loss: 0.03231935241811835\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.026974914326904503 | Validation loss: 0.03522281436068294\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.028522872728090294 | Validation loss: 0.03937122993896493\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.029662396922876224 | Validation loss: 0.04080926054940287\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.02874099444799624 | Validation loss: 0.03661016435815697\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.027630861122601144 | Validation loss: 0.03896229999321225\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.026198979643151515 | Validation loss: 0.03245780790900498\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.025587171060399806 | Validation loss: 0.0329771255332548\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.02489157276609399 | Validation loss: 0.030435980206965346\n",
      "Validation loss (ends of cycles): [0.11181737 0.0504285  0.04389677 0.04077262 0.03826001 0.03640073\n",
      " 0.03542221 0.03462614 0.03391641 0.03305692 0.03262452 0.03212512\n",
      " 0.03185629 0.03151543 0.03121231 0.03114316 0.03080671 0.03086406\n",
      " 0.0305027  0.03047522 0.03043598]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 19\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.13013751572364662 | Validation loss: 0.10866411221502102\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.0962619811872123 | Validation loss: 0.08431469047425595\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08808117919403502 | Validation loss: 0.08335764244594406\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08320651522573583 | Validation loss: 0.07264648778446481\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.078821630328967 | Validation loss: 0.086400652452644\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07551912005668081 | Validation loss: 0.07294148150666625\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06994353534147789 | Validation loss: 0.07389080859061363\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06581751633926815 | Validation loss: 0.05924121888799477\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.061769033043359324 | Validation loss: 0.05550177810730132\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05849388224901411 | Validation loss: 0.05526111535398306\n",
      "Validation loss (ends of cycles): [0.10866411]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05599109683640477 | Validation loss: 0.05028300414240993\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.05651184237070792 | Validation loss: 0.05119481994317169\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05796342328608799 | Validation loss: 0.05256845900970223\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05864124310452227 | Validation loss: 0.05311432339053238\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05971920895060216 | Validation loss: 0.053835257805422344\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0600251086206564 | Validation loss: 0.05795030089804029\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05733736497020041 | Validation loss: 0.05345632078175524\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05464613109489712 | Validation loss: 0.05284101431942092\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.05230115667557505 | Validation loss: 0.04781449882857568\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04979933290119012 | Validation loss: 0.04415781805868697\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283  ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.048008861920363674 | Validation loss: 0.04327868562257659\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04882487160425017 | Validation loss: 0.043836381352317016\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.049844463726168305 | Validation loss: 0.044874199962787395\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.05121253936104183 | Validation loss: 0.04625493370458088\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05227855082021982 | Validation loss: 0.06580113441543242\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05329341503725512 | Validation loss: 0.04736212479461611\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.05157633041174657 | Validation loss: 0.04704422052059554\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04926364007405937 | Validation loss: 0.05678830411186261\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.047098851822294646 | Validation loss: 0.043971751379755744\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.044588426964936825 | Validation loss: 0.041111332207786296\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.043297484392432244 | Validation loss: 0.04007098248508652\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.044140496467218154 | Validation loss: 0.041449001322673484\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04528621606360094 | Validation loss: 0.043515847155214414\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.046532035345123623 | Validation loss: 0.04557813516100951\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.04797187640779014 | Validation loss: 0.04983178259129018\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04931510559569194 | Validation loss: 0.047406337331090356\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04766238858651986 | Validation loss: 0.045556718137411946\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.045729071468364184 | Validation loss: 0.043208298611298074\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.04352147483237557 | Validation loss: 0.04185216669487742\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.041625120196726144 | Validation loss: 0.03896412177555329\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.04038160263494713 | Validation loss: 0.03796320309681175\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.04075280056100368 | Validation loss: 0.03851696993924875\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.04215318926217724 | Validation loss: 0.03938400522863443\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.04349201472703455 | Validation loss: 0.04077490941679056\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04480456488250982 | Validation loss: 0.04196629445769091\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.046417191071897804 | Validation loss: 0.04734774543780141\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.044565415281186425 | Validation loss: 0.042343570760129824\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.04292275310736003 | Validation loss: 0.046334737252477\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.04080903969382442 | Validation loss: 0.039976048242069975\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.038945722352442015 | Validation loss: 0.03901533678635559\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632 ]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03775392111123165 | Validation loss: 0.03635692800832006\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03847538466898974 | Validation loss: 0.037682123169039204\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03952817849660894 | Validation loss: 0.038835352138344166\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.040770487166853164 | Validation loss: 0.04136987426112183\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.04249348099743141 | Validation loss: 0.04505128795857978\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.04351341669029725 | Validation loss: 0.054758314850979144\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.04219748325440593 | Validation loss: 0.044231936352047245\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.040449189506177825 | Validation loss: 0.04580309190911002\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03866173452184367 | Validation loss: 0.0378917996994162\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.037179108895178094 | Validation loss: 0.03695562399462261\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.035941219553131405 | Validation loss: 0.034984574340020134\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03639172680041657 | Validation loss: 0.03606705074157335\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.0375638076628551 | Validation loss: 0.03725699788635283\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.039039047017847515 | Validation loss: 0.038210892637746526\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.04020949403707701 | Validation loss: 0.04285801162498187\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.041740124291689024 | Validation loss: 0.041410352474292825\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.040310035649445054 | Validation loss: 0.05140106689877215\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.03865856288173273 | Validation loss: 0.04588074636010997\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.036848236220030806 | Validation loss: 0.039373264755163576\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03542020992562908 | Validation loss: 0.036005012377832844\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.034385565286098915 | Validation loss: 0.03407285567702709\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.034998325296442985 | Validation loss: 0.03499888496852554\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03608989844824679 | Validation loss: 0.037975290641847965\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.037150994481291534 | Validation loss: 0.03802902005876588\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03874208623778046 | Validation loss: 0.041800051696796334\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.04030366619135891 | Validation loss: 0.04402384821292574\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03897080734265807 | Validation loss: 0.038405700957616876\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03700368836206773 | Validation loss: 0.03706241132839855\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03561326385912023 | Validation loss: 0.03773653503935949\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03413904608890971 | Validation loss: 0.034595715953449235\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.033077710618880964 | Validation loss: 0.03346761355621625\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.03339501076990255 | Validation loss: 0.03464310675595714\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.03449362341413553 | Validation loss: 0.03528704326105329\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03574517990213152 | Validation loss: 0.03956749140227263\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03712140995547117 | Validation loss: 0.04075488192647432\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03851752373111236 | Validation loss: 0.04053122321127263\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.037289504063550354 | Validation loss: 0.038621348683285504\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03606580819487278 | Validation loss: 0.03755889262641426\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.03403875287750545 | Validation loss: 0.03664958490207132\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.032725286617779764 | Validation loss: 0.03518285962497502\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.03179220572630985 | Validation loss: 0.03308977194097454\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.03204954804313611 | Validation loss: 0.03412141737453969\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.033113976240187415 | Validation loss: 0.03650290599768668\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.03438642540282944 | Validation loss: 0.03640461193842698\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.035784698719356765 | Validation loss: 0.03660603991783826\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.03738688092631119 | Validation loss: 0.04603489715836744\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.0360823278718694 | Validation loss: 0.03907236244232781\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03440698718571463 | Validation loss: 0.03614757999579991\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.032923828401624805 | Validation loss: 0.03688841782905887\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.03173442275582365 | Validation loss: 0.034847187158544506\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.030615978687268307 | Validation loss: 0.03256259926190946\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.030597679183165742 | Validation loss: 0.03352231219971338\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.03192661127614547 | Validation loss: 0.034362965502438295\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.03307850291681161 | Validation loss: 0.035471899154703175\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.03460319688741675 | Validation loss: 0.04395314779099638\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03603993922812996 | Validation loss: 0.03989762382631281\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.03485340225568965 | Validation loss: 0.03633215443222924\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.03364605744420661 | Validation loss: 0.03456024982167029\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.03178448307852253 | Validation loss: 0.03427424061779691\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.03050562876870665 | Validation loss: 0.03418495161541268\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626 ]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.029790386914158255 | Validation loss: 0.03241037907886558\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.03013055137904933 | Validation loss: 0.03372434017339111\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.030869591665842872 | Validation loss: 0.03451865892527641\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.032268728440312124 | Validation loss: 0.036281384115593625\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.03397129182373797 | Validation loss: 0.037896803192860255\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.03532382988418621 | Validation loss: 0.04804187614174016\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.03400072505730608 | Validation loss: 0.03747494302821898\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.032401689846163956 | Validation loss: 0.03554237289436623\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.03092662235776945 | Validation loss: 0.03437768438814488\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.029526078819634583 | Validation loss: 0.03346088269840827\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.028847131215395245 | Validation loss: 0.03171695443282349\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.029099270120248433 | Validation loss: 0.03273795564057289\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.03011367364243905 | Validation loss: 0.033902542846920215\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.03149592533238291 | Validation loss: 0.04488189828343096\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.03292691715219329 | Validation loss: 0.03982129518305306\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.03405568103294995 | Validation loss: 0.049960068979226384\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.0330583095444205 | Validation loss: 0.03734910000214534\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.03171656856324348 | Validation loss: 0.04288117178773458\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.03015985843582504 | Validation loss: 0.03478364519511176\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.02886176664715882 | Validation loss: 0.033183983189210425\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.02814806290759199 | Validation loss: 0.03104466441359404\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695 0.03104466]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.028255866756149398 | Validation loss: 0.0327843960325143\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695 0.03104466]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.02914011667071893 | Validation loss: 0.033506754794017934\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695 0.03104466]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.030517671123425676 | Validation loss: 0.034617729789981275\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695 0.03104466]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.03195116536941115 | Validation loss: 0.03802656727653425\n",
      "Validation loss (ends of cycles): [0.10866411 0.050283   0.04327869 0.04007098 0.0379632  0.03635693\n",
      " 0.03498457 0.03407286 0.03346761 0.03308977 0.0325626  0.03241038\n",
      " 0.03171695 0.03104466]\n",
      "------------------------------\n",
      "Epoch: 135\n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/losses')\n",
    "dest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "learners.train((X, y, depth_order[:, -1]), \n",
    "               dest_dir_loss=dest_dir_loss,\n",
    "               dest_dir_model=dest_dir_model,\n",
    "               n_epochs=n_epochs,\n",
    "               sc_kwargs=params_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "seeds = range(20)\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_global_all, y_hats_all, y_trues_all, ns_all = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                                      src_dir_model=src_dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test samples: 4032.0\n"
     ]
    }
   ],
   "source": [
    "print(f'# of test samples: {ns_all.mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save spectific seed y_hat, y_true to plot \"Observed vs. predicted\" scatterplots\n",
    "# Replace following Paths with yours\n",
    "dest_dir_predicted = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/')\n",
    "seed = 1\n",
    "with open(dest_dir_predicted/f'predicted-true-cnn-seed-{seed}.pickle', 'wb') as f: \n",
    "    pickle.dump((y_hats_all[seed].to_numpy(), y_trues_all[seed].to_numpy()), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-197f1570-8b10-4f49-b819-e9ceafe894f0\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.184830</td>\n",
       "      <td>2.992808</td>\n",
       "      <td>0.790243</td>\n",
       "      <td>0.884580</td>\n",
       "      <td>0.595624</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>0.237318</td>\n",
       "      <td>30.946025</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.009024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.041494</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.155836</td>\n",
       "      <td>0.259428</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>0.932127</td>\n",
       "      <td>0.012705</td>\n",
       "      <td>0.024443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.114635</td>\n",
       "      <td>2.889447</td>\n",
       "      <td>0.776315</td>\n",
       "      <td>0.874575</td>\n",
       "      <td>0.459073</td>\n",
       "      <td>0.210748</td>\n",
       "      <td>0.221478</td>\n",
       "      <td>29.163000</td>\n",
       "      <td>-0.012504</td>\n",
       "      <td>-0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.159791</td>\n",
       "      <td>2.930840</td>\n",
       "      <td>0.785559</td>\n",
       "      <td>0.881369</td>\n",
       "      <td>0.504516</td>\n",
       "      <td>0.254547</td>\n",
       "      <td>0.225294</td>\n",
       "      <td>30.303971</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>-0.010186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.183314</td>\n",
       "      <td>3.015281</td>\n",
       "      <td>0.790165</td>\n",
       "      <td>0.884643</td>\n",
       "      <td>0.571964</td>\n",
       "      <td>0.327175</td>\n",
       "      <td>0.236342</td>\n",
       "      <td>30.827668</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.007958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.205628</td>\n",
       "      <td>3.028623</td>\n",
       "      <td>0.794389</td>\n",
       "      <td>0.887016</td>\n",
       "      <td>0.617017</td>\n",
       "      <td>0.380868</td>\n",
       "      <td>0.246811</td>\n",
       "      <td>31.663001</td>\n",
       "      <td>0.016258</td>\n",
       "      <td>0.031279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.262492</td>\n",
       "      <td>3.141478</td>\n",
       "      <td>0.804596</td>\n",
       "      <td>0.895479</td>\n",
       "      <td>1.197708</td>\n",
       "      <td>1.434504</td>\n",
       "      <td>0.260846</td>\n",
       "      <td>32.642108</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.062948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-197f1570-8b10-4f49-b819-e9ceafe894f0')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-197f1570-8b10-4f49-b819-e9ceafe894f0 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-197f1570-8b10-4f49-b819-e9ceafe894f0');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "mean    2.184830   2.992808   0.790243   0.884580   0.595624   0.377839   \n",
       "std     0.041494   0.067646   0.007929   0.004986   0.155836   0.259428   \n",
       "min     2.114635   2.889447   0.776315   0.874575   0.459073   0.210748   \n",
       "25%     2.159791   2.930840   0.785559   0.881369   0.504516   0.254547   \n",
       "50%     2.183314   3.015281   0.790165   0.884643   0.571964   0.327175   \n",
       "75%     2.205628   3.028623   0.794389   0.887016   0.617017   0.380868   \n",
       "max     2.262492   3.141478   0.804596   0.895479   1.197708   1.434504   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  20.000000  20.000000  20.000000  20.000000  \n",
       "mean    0.237318  30.946025   0.004749   0.009024  \n",
       "std     0.012061   0.932127   0.012705   0.024443  \n",
       "min     0.221478  29.163000  -0.012504  -0.024802  \n",
       "25%     0.225294  30.303971  -0.005159  -0.010186  \n",
       "50%     0.236342  30.827668   0.004057   0.007958  \n",
       "75%     0.246811  31.663001   0.016258   0.031279  \n",
       "max     0.260846  32.642108   0.032962   0.062948  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs_global_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Soil Tax. Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test metrics on alfisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 422.4\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    1.809327   2.462286   0.691663   0.822509   0.382106   0.174798   \n",
      "std     0.087705   0.182733   0.030946   0.018080   0.174094   0.157874   \n",
      "min     1.602639   2.154843   0.609675   0.773057   0.150495   0.022649   \n",
      "25%     1.752131   2.328294   0.673422   0.810919   0.233184   0.054400   \n",
      "50%     1.814577   2.449380   0.695557   0.823634   0.371884   0.138515   \n",
      "75%     1.875471   2.573424   0.715007   0.835819   0.459433   0.211082   \n",
      "max     1.949812   2.768350   0.736324   0.847946   0.794326   0.630953   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.134967  27.355655   0.003228   0.008986  \n",
      "std     0.020114   1.268820   0.011029   0.028718  \n",
      "min     0.101224  24.950266  -0.013376  -0.032392  \n",
      "25%     0.122148  26.654700  -0.005885  -0.015329  \n",
      "50%     0.130240  27.405189   0.004066   0.010273  \n",
      "75%     0.149955  27.915920   0.012193   0.033151  \n",
      "max     0.168116  29.876232   0.020493   0.053057  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on mollisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 977.6\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.082906   2.750063   0.767633   0.868226   0.441974   0.200178   \n",
      "std     0.105370   0.152068   0.022648   0.014038   0.071353   0.074478   \n",
      "min     1.910614   2.503716   0.725773   0.839147   0.351621   0.123637   \n",
      "25%     2.016213   2.645241   0.753751   0.858901   0.398954   0.159184   \n",
      "50%     2.073633   2.727649   0.767201   0.867132   0.434792   0.189089   \n",
      "75%     2.158195   2.803545   0.785071   0.877551   0.462096   0.213538   \n",
      "max     2.297545   3.113704   0.810361   0.892300   0.693749   0.481287   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.217482  27.362061   0.006410   0.014927  \n",
      "std     0.011783   1.171648   0.014516   0.034156  \n",
      "min     0.198063  25.863001  -0.017837  -0.042497  \n",
      "25%     0.208970  26.442070  -0.002144  -0.005110  \n",
      "50%     0.217809  27.306423   0.006673   0.016067  \n",
      "75%     0.226259  27.765553   0.015423   0.036009  \n",
      "max     0.238283  30.162075   0.034060   0.082286  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on inceptisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 289.6\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    1.909866   2.651945   0.722069   0.838262   0.395929   0.161811   \n",
      "std     0.112304   0.174091   0.033546   0.022861   0.072917   0.060037   \n",
      "min     1.644649   2.362685   0.628917   0.773094   0.297366   0.088427   \n",
      "25%     1.843177   2.472268   0.704578   0.825389   0.341724   0.116911   \n",
      "50%     1.907174   2.692387   0.724081   0.840439   0.403799   0.163472   \n",
      "75%     1.972514   2.807575   0.742076   0.852019   0.446248   0.199140   \n",
      "max     2.168335   2.898377   0.786479   0.878068   0.566843   0.321311   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.188652  35.004717  -0.003812  -0.007661  \n",
      "std     0.019507   3.099745   0.016768   0.033159  \n",
      "min     0.167560  31.472638  -0.023707  -0.052528  \n",
      "25%     0.173922  33.050188  -0.018816  -0.037482  \n",
      "50%     0.185284  34.055609  -0.007071  -0.014775  \n",
      "75%     0.193686  36.080701   0.008284   0.017426  \n",
      "max     0.240127  43.579751   0.026359   0.048525  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on entisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 164.8\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.146993   3.000346   0.772961   0.875231   0.323115   0.107320   \n",
      "std     0.250396   0.376957   0.053555   0.032886   0.055413   0.036646   \n",
      "min     1.703938   2.254587   0.653383   0.784883   0.225837   0.051002   \n",
      "25%     2.015869   2.812905   0.752028   0.869245   0.281971   0.079520   \n",
      "50%     2.098403   3.022831   0.771459   0.877648   0.317529   0.100827   \n",
      "75%     2.324253   3.244836   0.813595   0.896911   0.360752   0.130172   \n",
      "max     2.658273   3.657772   0.857475   0.920406   0.431522   0.186212   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.164777  30.832117   0.004347   0.008009  \n",
      "std     0.019656   3.450412   0.020474   0.042995  \n",
      "min     0.122440  24.067251  -0.025000  -0.061117  \n",
      "25%     0.151904  27.745744  -0.010036  -0.018291  \n",
      "50%     0.168026  31.689824   0.003999   0.007015  \n",
      "75%     0.175205  32.814956   0.017534   0.035175  \n",
      "max     0.199315  37.243327   0.037462   0.079188  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on spodosols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 64.0\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.181398   3.140114   0.779113   0.880473   0.412119   0.180965   \n",
      "std     0.226818   0.688425   0.046961   0.025426   0.108204   0.098489   \n",
      "min     1.776501   2.128022   0.677271   0.820691   0.268197   0.071930   \n",
      "25%     2.026443   2.670819   0.752034   0.863932   0.326987   0.106931   \n",
      "50%     2.153188   2.983713   0.780577   0.884321   0.386405   0.149414   \n",
      "75%     2.367428   3.278641   0.818189   0.898756   0.483470   0.233907   \n",
      "max     2.553907   4.704971   0.843790   0.914914   0.681660   0.464660   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.223380  37.213633  -0.008654  -0.013208  \n",
      "std     0.050800   4.705197   0.028514   0.043828  \n",
      "min     0.148266  31.124797  -0.073955  -0.085358  \n",
      "25%     0.184255  32.850098  -0.031603  -0.049806  \n",
      "50%     0.215128  37.193228  -0.010030  -0.018675  \n",
      "75%     0.268383  40.618369   0.015755   0.023395  \n",
      "max     0.314407  45.255888   0.037978   0.053857  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on undefined\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 1553.6\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.292085   3.140178   0.809147   0.896603   0.756287   0.646963   \n",
      "std     0.061811   0.126350   0.010024   0.006282   0.280963   0.667752   \n",
      "min     2.200176   2.961841   0.793285   0.887092   0.533183   0.284284   \n",
      "25%     2.250938   3.058737   0.802503   0.892838   0.609880   0.372009   \n",
      "50%     2.294312   3.141787   0.809902   0.896849   0.712569   0.507759   \n",
      "75%     2.315500   3.190854   0.813364   0.899591   0.796151   0.633870   \n",
      "max     2.437377   3.432564   0.831563   0.912160   1.841250   3.390202   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.292059  31.645606   0.004302   0.007476  \n",
      "std     0.023362   1.247020   0.014321   0.025455  \n",
      "min     0.258135  29.001465  -0.016584  -0.031293  \n",
      "25%     0.272517  30.930875  -0.006044  -0.011075  \n",
      "50%     0.289404  31.708111   0.001588   0.002819  \n",
      "75%     0.306450  32.771565   0.014035   0.025830  \n",
      "max     0.339195  33.584201   0.036192   0.061086  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on ultisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 192.0\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    1.632526   2.265209   0.617841   0.763237   0.260545   0.071859   \n",
      "std     0.108925   0.250811   0.051179   0.038303   0.064686   0.036546   \n",
      "min     1.428423   1.788526   0.507145   0.672963   0.169072   0.028586   \n",
      "25%     1.568656   2.078099   0.591292   0.742977   0.216448   0.046878   \n",
      "50%     1.622599   2.291745   0.617950   0.762776   0.246636   0.060842   \n",
      "75%     1.695520   2.441266   0.650373   0.784582   0.298181   0.089221   \n",
      "max     1.858865   2.726562   0.708979   0.823643   0.394320   0.155488   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.135663  32.535614   0.017157   0.040477  \n",
      "std     0.020282   2.442567   0.013812   0.032344  \n",
      "min     0.107650  28.119987  -0.010331  -0.025534  \n",
      "25%     0.121366  31.080629   0.011006   0.025485  \n",
      "50%     0.136634  32.268478   0.015768   0.035893  \n",
      "75%     0.147672  33.884998   0.027038   0.058695  \n",
      "max     0.179378  38.015869   0.047486   0.105611  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on andisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 132.8\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    1.982082   2.557273   0.738402   0.856554   0.476687   0.248451   \n",
      "std     0.162387   0.330953   0.041831   0.025708   0.149459   0.166614   \n",
      "min     1.738631   1.923586   0.666517   0.798802   0.276538   0.076474   \n",
      "25%     1.859672   2.319364   0.708526   0.839169   0.382923   0.146634   \n",
      "50%     1.946365   2.628500   0.733634   0.856545   0.454000   0.206157   \n",
      "75%     2.118545   2.801427   0.775115   0.878573   0.557137   0.310493   \n",
      "max     2.280665   3.039670   0.806156   0.894918   0.833418   0.694586   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.229848  32.559297   0.015201   0.033912  \n",
      "std     0.026493   3.362845   0.023263   0.050849  \n",
      "min     0.175337  27.092579  -0.025021  -0.052350  \n",
      "25%     0.206640  30.636463  -0.003132  -0.006866  \n",
      "50%     0.234442  33.061846   0.017000   0.037901  \n",
      "75%     0.248909  35.253529   0.023922   0.056046  \n",
      "max     0.262889  38.183209   0.064641   0.131825  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on histosols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 80.0\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.093646   3.055053   0.758036   0.870220   0.866301   0.819420   \n",
      "std     0.274214   0.770695   0.056379   0.028888   0.269391   0.556293   \n",
      "min     1.656537   2.138313   0.630659   0.818658   0.570947   0.325981   \n",
      "25%     1.920327   2.384933   0.725072   0.850471   0.698688   0.488250   \n",
      "50%     2.058352   2.852104   0.760004   0.867652   0.787495   0.621255   \n",
      "75%     2.199841   3.507436   0.790265   0.882838   0.946578   0.896546   \n",
      "max     2.885291   4.842908   0.877941   0.932256   1.453742   2.113365   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.449648  45.448765   0.024104   0.034142  \n",
      "std     0.118631   7.463257   0.034970   0.048042  \n",
      "min     0.294663  34.994602  -0.071150  -0.081119  \n",
      "25%     0.362897  39.577908   0.011031   0.011864  \n",
      "50%     0.415026  44.506969   0.034138   0.050279  \n",
      "75%     0.547573  51.371342   0.046856   0.066266  \n",
      "max     0.660869  57.288700   0.067678   0.099684  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on oxisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 32.0\n",
      "            rpd      rpiq          r2      lccc      rmse       mse       mae  \\\n",
      "count  2.000000  8.000000    2.000000  2.000000  8.000000  8.000000  8.000000   \n",
      "mean   0.118367  0.026377 -134.414165  0.030174  0.087786  0.013825  0.083611   \n",
      "std    0.046499  0.051292   98.771373  0.054914  0.083623  0.022839  0.081877   \n",
      "min    0.085487  0.000000 -204.256073 -0.008656  0.009060  0.000082  0.009060   \n",
      "25%    0.101927  0.000000 -169.335119  0.010759  0.025124  0.000655  0.025124   \n",
      "50%    0.118367  0.000000 -134.414165  0.030174  0.067840  0.004688  0.065089   \n",
      "75%    0.134807  0.019050  -99.493211  0.049589  0.118719  0.014272  0.111745   \n",
      "max    0.151247  0.134818  -64.572257  0.069005  0.260096  0.067650  0.260096   \n",
      "\n",
      "             mape      bias        stb  \n",
      "count    8.000000  8.000000   8.000000  \n",
      "mean    65.191675 -0.177251        NaN  \n",
      "std     60.839932  0.164703        NaN  \n",
      "min      7.445914 -0.467451       -inf  \n",
      "25%     20.544780 -0.258737        NaN  \n",
      "50%     52.729677 -0.180752        NaN  \n",
      "75%     88.397117 -0.074745 -10.384091  \n",
      "max    193.393838  0.064107        inf  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on vertisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 94.4\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.039982   2.815862   0.744782   0.856862   0.273233   0.078001   \n",
      "std     0.271953   0.511288   0.063033   0.036968   0.059333   0.036126   \n",
      "min     1.577847   2.128196   0.592591   0.768764   0.182002   0.033125   \n",
      "25%     1.851439   2.431125   0.703943   0.837489   0.232396   0.054013   \n",
      "50%     1.992413   2.714564   0.744894   0.855618   0.253052   0.064036   \n",
      "75%     2.178701   3.148894   0.786316   0.881447   0.303549   0.092144   \n",
      "max     2.713312   4.022168   0.862141   0.923391   0.427089   0.182405   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.173488  26.855655  -0.001182  -0.003823  \n",
      "std     0.021406   3.272257   0.020647   0.055384  \n",
      "min     0.138288  21.111543  -0.052648  -0.154459  \n",
      "25%     0.157317  24.537949  -0.011511  -0.028144  \n",
      "50%     0.173311  26.192354  -0.001295  -0.003575  \n",
      "75%     0.188509  28.037003   0.009025   0.019823  \n",
      "max     0.208709  33.903366   0.040271   0.099009  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on aridisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 163.2\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    1.814492   2.405396   0.691015   0.821971   0.662013   0.489155   \n",
      "std     0.109405   0.241063   0.037256   0.022591   0.231457   0.352484   \n",
      "min     1.613652   1.941601   0.613362   0.776206   0.373065   0.139178   \n",
      "25%     1.750923   2.250058   0.671518   0.809034   0.485274   0.235720   \n",
      "50%     1.814514   2.415395   0.694165   0.819871   0.598650   0.359678   \n",
      "75%     1.885174   2.577828   0.716658   0.838137   0.773024   0.597940   \n",
      "max     2.036902   2.856824   0.757279   0.864022   1.208685   1.460919   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.300450  35.089144   0.008393   0.015292  \n",
      "std     0.059640   3.065990   0.021957   0.044950  \n",
      "min     0.205739  28.651589  -0.029821  -0.072086  \n",
      "25%     0.256783  33.024969  -0.006753  -0.014939  \n",
      "50%     0.295750  34.871383   0.012934   0.028759  \n",
      "75%     0.330856  37.073732   0.020540   0.042719  \n",
      "max     0.408710  41.000089   0.048986   0.085159  \n",
      "--------------------------------------------------------------------------------\n",
      "Test metrics on gelisols\n",
      "--------------------------------------------------------------------------------\n",
      "# of test samples: 60.8\n",
      "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.094545   3.140563   0.744767   0.860555   0.588931   0.365489   \n",
      "std     0.397299   1.088457   0.082866   0.047535   0.140113   0.173223   \n",
      "min     1.563934   1.337414   0.579469   0.746647   0.378497   0.143260   \n",
      "25%     1.827473   2.396965   0.692855   0.838851   0.495282   0.245315   \n",
      "50%     2.025772   2.948529   0.749510   0.854638   0.568492   0.323183   \n",
      "75%     2.235309   3.706015   0.795246   0.890133   0.710962   0.505690   \n",
      "max     3.026351   5.757985   0.888340   0.943058   0.834707   0.696736   \n",
      "\n",
      "             mae       mape       bias        stb  \n",
      "count  20.000000  20.000000  20.000000  20.000000  \n",
      "mean    0.308468  47.301260  -0.041032  -0.068766  \n",
      "std     0.062994   9.997578   0.036220   0.072472  \n",
      "min     0.194099  31.988180  -0.094267  -0.236790  \n",
      "25%     0.259407  38.336892  -0.074615  -0.102293  \n",
      "50%     0.303302  50.013152  -0.037441  -0.057775  \n",
      "75%     0.363768  55.542475  -0.021546  -0.025043  \n",
      "max     0.404708  60.821646   0.054468   0.089767  \n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "seeds = range(20)\n",
    "\n",
    "for k, v in tax_lookup.items():\n",
    "    print(80*'-')\n",
    "    print(f'Test metrics on {k}')\n",
    "    print(80*'-')\n",
    "    learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "    perfs_global, _, _, ns = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                               order=v,\n",
    "                                               src_dir_model=src_dir_model)\n",
    "\n",
    "    print(f'# of test samples: {ns.mean().item()}')\n",
    "    print(perfs_global.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Mollisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-30f1b341-e308-4ed7-ac12-8b5ffadb036f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.082906</td>\n",
       "      <td>2.750063</td>\n",
       "      <td>0.767633</td>\n",
       "      <td>0.868226</td>\n",
       "      <td>0.441974</td>\n",
       "      <td>0.200178</td>\n",
       "      <td>0.217482</td>\n",
       "      <td>27.362061</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.014927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105370</td>\n",
       "      <td>0.152068</td>\n",
       "      <td>0.022648</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.071353</td>\n",
       "      <td>0.074478</td>\n",
       "      <td>0.011783</td>\n",
       "      <td>1.171648</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.034156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.910614</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>0.725773</td>\n",
       "      <td>0.839147</td>\n",
       "      <td>0.351621</td>\n",
       "      <td>0.123637</td>\n",
       "      <td>0.198063</td>\n",
       "      <td>25.863001</td>\n",
       "      <td>-0.017837</td>\n",
       "      <td>-0.042497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.016213</td>\n",
       "      <td>2.645241</td>\n",
       "      <td>0.753751</td>\n",
       "      <td>0.858901</td>\n",
       "      <td>0.398954</td>\n",
       "      <td>0.159184</td>\n",
       "      <td>0.208970</td>\n",
       "      <td>26.442070</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>-0.005110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.073633</td>\n",
       "      <td>2.727649</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.867132</td>\n",
       "      <td>0.434792</td>\n",
       "      <td>0.189089</td>\n",
       "      <td>0.217809</td>\n",
       "      <td>27.306423</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.016067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.158195</td>\n",
       "      <td>2.803545</td>\n",
       "      <td>0.785071</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.462096</td>\n",
       "      <td>0.213538</td>\n",
       "      <td>0.226259</td>\n",
       "      <td>27.765553</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.036009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.297545</td>\n",
       "      <td>3.113704</td>\n",
       "      <td>0.810361</td>\n",
       "      <td>0.892300</td>\n",
       "      <td>0.693749</td>\n",
       "      <td>0.481287</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>30.162075</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.082286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30f1b341-e308-4ed7-ac12-8b5ffadb036f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-30f1b341-e308-4ed7-ac12-8b5ffadb036f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-30f1b341-e308-4ed7-ac12-8b5ffadb036f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "mean    2.082906   2.750063   0.767633   0.868226   0.441974   0.200178   \n",
       "std     0.105370   0.152068   0.022648   0.014038   0.071353   0.074478   \n",
       "min     1.910614   2.503716   0.725773   0.839147   0.351621   0.123637   \n",
       "25%     2.016213   2.645241   0.753751   0.858901   0.398954   0.159184   \n",
       "50%     2.073633   2.727649   0.767201   0.867132   0.434792   0.189089   \n",
       "75%     2.158195   2.803545   0.785071   0.877551   0.462096   0.213538   \n",
       "max     2.297545   3.113704   0.810361   0.892300   0.693749   0.481287   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  20.000000  20.000000  20.000000  20.000000  \n",
       "mean    0.217482  27.362061   0.006410   0.014927  \n",
       "std     0.011783   1.171648   0.014516   0.034156  \n",
       "min     0.198063  25.863001  -0.017837  -0.042497  \n",
       "25%     0.208970  26.442070  -0.002144  -0.005110  \n",
       "50%     0.217809  27.306423   0.006673   0.016067  \n",
       "75%     0.226259  27.765553   0.015423   0.036009  \n",
       "max     0.238283  30.162075   0.034060   0.082286  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "seeds = range(20)\n",
    "order = 1\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_global_mollisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                    order=order,\n",
    "                                                    src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_global_mollisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Gelisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5671725a-2b6f-4d6c-b0ce-dca87c3950d5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.064524</td>\n",
       "      <td>3.052019</td>\n",
       "      <td>0.742612</td>\n",
       "      <td>0.858711</td>\n",
       "      <td>0.584043</td>\n",
       "      <td>0.357354</td>\n",
       "      <td>0.308433</td>\n",
       "      <td>47.883342</td>\n",
       "      <td>-0.042954</td>\n",
       "      <td>-0.072261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.339669</td>\n",
       "      <td>0.922599</td>\n",
       "      <td>0.076703</td>\n",
       "      <td>0.045312</td>\n",
       "      <td>0.131164</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.061630</td>\n",
       "      <td>10.194859</td>\n",
       "      <td>0.037772</td>\n",
       "      <td>0.075571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.563934</td>\n",
       "      <td>1.337414</td>\n",
       "      <td>0.579469</td>\n",
       "      <td>0.746647</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.143260</td>\n",
       "      <td>0.194099</td>\n",
       "      <td>31.988180</td>\n",
       "      <td>-0.094267</td>\n",
       "      <td>-0.236790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.848496</td>\n",
       "      <td>2.458523</td>\n",
       "      <td>0.699683</td>\n",
       "      <td>0.841321</td>\n",
       "      <td>0.499613</td>\n",
       "      <td>0.249631</td>\n",
       "      <td>0.266246</td>\n",
       "      <td>38.717125</td>\n",
       "      <td>-0.076805</td>\n",
       "      <td>-0.105620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.025773</td>\n",
       "      <td>2.948530</td>\n",
       "      <td>0.749510</td>\n",
       "      <td>0.854638</td>\n",
       "      <td>0.568492</td>\n",
       "      <td>0.323183</td>\n",
       "      <td>0.303302</td>\n",
       "      <td>51.015234</td>\n",
       "      <td>-0.042409</td>\n",
       "      <td>-0.066190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.215420</td>\n",
       "      <td>3.613597</td>\n",
       "      <td>0.791696</td>\n",
       "      <td>0.889089</td>\n",
       "      <td>0.680335</td>\n",
       "      <td>0.464308</td>\n",
       "      <td>0.360686</td>\n",
       "      <td>55.561434</td>\n",
       "      <td>-0.021409</td>\n",
       "      <td>-0.028615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.023628</td>\n",
       "      <td>4.793782</td>\n",
       "      <td>0.888340</td>\n",
       "      <td>0.943058</td>\n",
       "      <td>0.834707</td>\n",
       "      <td>0.696736</td>\n",
       "      <td>0.404708</td>\n",
       "      <td>60.821658</td>\n",
       "      <td>0.054468</td>\n",
       "      <td>0.089767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5671725a-2b6f-4d6c-b0ce-dca87c3950d5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5671725a-2b6f-4d6c-b0ce-dca87c3950d5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5671725a-2b6f-4d6c-b0ce-dca87c3950d5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  18.000000  18.000000  18.000000  18.000000  18.000000  18.000000   \n",
       "mean    2.064524   3.052019   0.742612   0.858711   0.584043   0.357354   \n",
       "std     0.339669   0.922599   0.076703   0.045312   0.131164   0.160157   \n",
       "min     1.563934   1.337414   0.579469   0.746647   0.378497   0.143260   \n",
       "25%     1.848496   2.458523   0.699683   0.841321   0.499613   0.249631   \n",
       "50%     2.025773   2.948530   0.749510   0.854638   0.568492   0.323183   \n",
       "75%     2.215420   3.613597   0.791696   0.889089   0.680335   0.464308   \n",
       "max     3.023628   4.793782   0.888340   0.943058   0.834707   0.696736   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  18.000000  18.000000  18.000000  18.000000  \n",
       "mean    0.308433  47.883342  -0.042954  -0.072261  \n",
       "std     0.061630  10.194859   0.037772   0.075571  \n",
       "min     0.194099  31.988180  -0.094267  -0.236790  \n",
       "25%     0.266246  38.717125  -0.076805  -0.105620  \n",
       "50%     0.303302  51.015234  -0.042409  -0.066190  \n",
       "75%     0.360686  55.561434  -0.021409  -0.028615  \n",
       "max     0.404708  60.821658   0.054468   0.089767  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "seeds = range(20)\n",
    "order = 12\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_global_gelisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                   order = order,\n",
    "                                                   src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_global_gelisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Vertisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-84497dc8-6884-4a92-8856-dc4f9acf68d1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.039982</td>\n",
       "      <td>2.815862</td>\n",
       "      <td>0.744782</td>\n",
       "      <td>0.856862</td>\n",
       "      <td>0.273233</td>\n",
       "      <td>0.078001</td>\n",
       "      <td>0.173488</td>\n",
       "      <td>26.855653</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.271953</td>\n",
       "      <td>0.511288</td>\n",
       "      <td>0.063033</td>\n",
       "      <td>0.036968</td>\n",
       "      <td>0.059333</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>3.272256</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>0.055384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.577848</td>\n",
       "      <td>2.128196</td>\n",
       "      <td>0.592591</td>\n",
       "      <td>0.768764</td>\n",
       "      <td>0.182002</td>\n",
       "      <td>0.033125</td>\n",
       "      <td>0.138288</td>\n",
       "      <td>21.111539</td>\n",
       "      <td>-0.052648</td>\n",
       "      <td>-0.154459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.851439</td>\n",
       "      <td>2.431125</td>\n",
       "      <td>0.703943</td>\n",
       "      <td>0.837489</td>\n",
       "      <td>0.232396</td>\n",
       "      <td>0.054013</td>\n",
       "      <td>0.157317</td>\n",
       "      <td>24.537946</td>\n",
       "      <td>-0.011511</td>\n",
       "      <td>-0.028144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.992414</td>\n",
       "      <td>2.714563</td>\n",
       "      <td>0.744894</td>\n",
       "      <td>0.855618</td>\n",
       "      <td>0.253052</td>\n",
       "      <td>0.064036</td>\n",
       "      <td>0.173311</td>\n",
       "      <td>26.192354</td>\n",
       "      <td>-0.001295</td>\n",
       "      <td>-0.003575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.178701</td>\n",
       "      <td>3.148894</td>\n",
       "      <td>0.786316</td>\n",
       "      <td>0.881447</td>\n",
       "      <td>0.303549</td>\n",
       "      <td>0.092144</td>\n",
       "      <td>0.188509</td>\n",
       "      <td>28.037003</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.019823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.713312</td>\n",
       "      <td>4.022168</td>\n",
       "      <td>0.862141</td>\n",
       "      <td>0.923391</td>\n",
       "      <td>0.427089</td>\n",
       "      <td>0.182405</td>\n",
       "      <td>0.208709</td>\n",
       "      <td>33.903363</td>\n",
       "      <td>0.040271</td>\n",
       "      <td>0.099009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84497dc8-6884-4a92-8856-dc4f9acf68d1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-84497dc8-6884-4a92-8856-dc4f9acf68d1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-84497dc8-6884-4a92-8856-dc4f9acf68d1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "mean    2.039982   2.815862   0.744782   0.856862   0.273233   0.078001   \n",
       "std     0.271953   0.511288   0.063033   0.036968   0.059333   0.036126   \n",
       "min     1.577848   2.128196   0.592591   0.768764   0.182002   0.033125   \n",
       "25%     1.851439   2.431125   0.703943   0.837489   0.232396   0.054013   \n",
       "50%     1.992414   2.714563   0.744894   0.855618   0.253052   0.064036   \n",
       "75%     2.178701   3.148894   0.786316   0.881447   0.303549   0.092144   \n",
       "max     2.713312   4.022168   0.862141   0.923391   0.427089   0.182405   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  20.000000  20.000000  20.000000  20.000000  \n",
       "mean    0.173488  26.855653  -0.001182  -0.003823  \n",
       "std     0.021406   3.272256   0.020647   0.055384  \n",
       "min     0.138288  21.111539  -0.052648  -0.154459  \n",
       "25%     0.157317  24.537946  -0.011511  -0.028144  \n",
       "50%     0.173311  26.192354  -0.001295  -0.003575  \n",
       "75%     0.188509  28.037003   0.009025   0.019823  \n",
       "max     0.208709  33.903363   0.040271   0.099009  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/all/models')\n",
    "seeds = range(20)\n",
    "order = 10\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_global_vertisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                   order = order,\n",
    "                                                   src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_global_vertisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test on Mollisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " 0.0254631  0.02506297 0.02542974]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.01703691177108154 | Validation loss: 0.03745046781440233\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.01636921336912379 | Validation loss: 0.029602385222398\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.01618192989227115 | Validation loss: 0.02575919661542465\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.016052027107501518 | Validation loss: 0.02995985579387895\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.01630766587521957 | Validation loss: 0.036192602445853164\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.016913761476016774 | Validation loss: 0.043863068752247714\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.01821006020264966 | Validation loss: 0.028571049008389998\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.0200055970297176 | Validation loss: 0.03199511515940058\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.01871272486881638 | Validation loss: 0.0318490580238145\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.01749704931957685 | Validation loss: 0.04083131170221444\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.016811477638096834 | Validation loss: 0.036076820102231254\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.016043470830333476 | Validation loss: 0.032022082824902286\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592 ]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.01610600998130988 | Validation loss: 0.024903937371383453\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.015034414651062415 | Validation loss: 0.02793283073295807\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.01557806123010054 | Validation loss: 0.028809203798400945\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.01631182431984617 | Validation loss: 0.02934042409319302\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.017696567615304064 | Validation loss: 0.04410623669110496\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.0189071940410198 | Validation loss: 0.05060041310458348\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.018668438469496916 | Validation loss: 0.030868354253470898\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.017217437183598475 | Validation loss: 0.04276854185194805\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.016072474085554785 | Validation loss: 0.037263108963339495\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.015596854644922577 | Validation loss: 0.02746792192217605\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.015756507012612966 | Validation loss: 0.025396741316493214\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.01487494743220052 | Validation loss: 0.02894811505644486\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.014975935009745311 | Validation loss: 0.034921294720522286\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.01575764608664476 | Validation loss: 0.035113322580682824\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.01644058152272993 | Validation loss: 0.041267819253021275\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.018470817106794945 | Validation loss: 0.05743228978124158\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.01725016314802425 | Validation loss: 0.028639680866537422\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.016041831985800243 | Validation loss: 0.0426863385685559\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.014942902400709536 | Validation loss: 0.03376636066441906\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.015016029911990069 | Validation loss: 0.02791190057479102\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.015328187172358133 | Validation loss: 0.024651944123465438\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.014163121622892058 | Validation loss: 0.029626857617805744\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.01449333244212428 | Validation loss: 0.02998114056114493\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.015647320470259504 | Validation loss: 0.042618878442665625\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.01614186215043372 | Validation loss: 0.029807858361766255\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.01824572786346686 | Validation loss: 0.03824911529904809\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.01688562039171859 | Validation loss: 0.03462533713800126\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.015406720159689382 | Validation loss: 0.027114455973536802\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.015270027453650017 | Validation loss: 0.03875390183309029\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.014476752381923856 | Validation loss: 0.028192470909963394\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.015035120354091026 | Validation loss: 0.024417148613981133\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.01392070555641335 | Validation loss: 0.027935292764470494\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.014260541837738484 | Validation loss: 0.038586160296510005\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.015028002004766342 | Validation loss: 0.03351751615389668\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.015989612158844056 | Validation loss: 0.06615991421557706\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.017272643091119064 | Validation loss: 0.04459637872360904\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.01664832424638527 | Validation loss: 0.04218823045235256\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.015222868038227363 | Validation loss: 0.04500981189053634\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.014712775491025983 | Validation loss: 0.03708768857578779\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.014625412261835774 | Validation loss: 0.028367176780412937\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.014733538981907221 | Validation loss: 0.024383184050434624\n",
      "Validation loss (ends of cycles): [0.09721728 0.04142509 0.03426372 0.03054056 0.02912892 0.02823934\n",
      " 0.02746296 0.02677113 0.02637389 0.02613157 0.02631584 0.02581771\n",
      " 0.0254631  0.02506297 0.02542974 0.0257592  0.02490394 0.02539674\n",
      " 0.02465194 0.02441715 0.02438318]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 11\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.23029391500886862 | Validation loss: 0.2263513808803899\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.1298492884460261 | Validation loss: 0.07065250990646225\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.06877072449258673 | Validation loss: 0.06425394796367202\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.061665143847950105 | Validation loss: 0.06861826524670635\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.058033256072217855 | Validation loss: 0.0625213146475809\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.055559850668882936 | Validation loss: 0.08042318240872451\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0519350595560258 | Validation loss: 0.04697381053119898\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.048388288273074766 | Validation loss: 0.11100427699940545\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04608837391696567 | Validation loss: 0.07308738904872111\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04391489770051425 | Validation loss: 0.10111017578414508\n",
      "Validation loss (ends of cycles): [0.22635138]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0414222451229769 | Validation loss: 0.04029037490753191\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04222617237412227 | Validation loss: 0.05305742405887161\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.042907760452055105 | Validation loss: 0.04166700045711228\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.043801927363606004 | Validation loss: 0.11412931340081352\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04472829210685521 | Validation loss: 0.05646906566939184\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.046035228539409674 | Validation loss: 0.04421177878975868\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.043207717533185475 | Validation loss: 0.042158282095832486\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.040840370588125736 | Validation loss: 0.038402439294649024\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03911800839279483 | Validation loss: 0.042062657086976936\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03648519006792486 | Validation loss: 0.03675581481573837\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.034782703212515365 | Validation loss: 0.03432650278721537\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.035905772971549656 | Validation loss: 0.03975177935457656\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.037213405435223404 | Validation loss: 0.03562755955915366\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03865513603043992 | Validation loss: 0.11057296767830849\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03931918034587449 | Validation loss: 0.0369640770368278\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.039821294779942285 | Validation loss: 0.045848207348691564\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.038668711453948804 | Validation loss: 0.042994737558599026\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03670025159551845 | Validation loss: 0.04410590357812388\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03455162040389529 | Validation loss: 0.034312766577516286\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.032675492149452125 | Validation loss: 0.03203583408945373\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03106907684719417 | Validation loss: 0.03267500549554825\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.03156270837502145 | Validation loss: 0.03273458672421319\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.03294173397532687 | Validation loss: 0.03579591427530561\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.03416234837481525 | Validation loss: 0.03482184831851295\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.03490711622909317 | Validation loss: 0.050317411577062945\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.03727862610655829 | Validation loss: 0.03666511264496616\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.03482806728774212 | Validation loss: 0.04250362701714039\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.0331079214924901 | Validation loss: 0.036069130791085105\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.031223616937584266 | Validation loss: 0.03177707828581333\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.029834562658930454 | Validation loss: 0.03063737727435572\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.027999205529932084 | Validation loss: 0.030917407213045017\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.028703854284892843 | Validation loss: 0.029636949034673826\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.029869006272799117 | Validation loss: 0.033834958083129356\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.031108734364492623 | Validation loss: 0.03308493041965578\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.03213380325615891 | Validation loss: 0.03766348745141711\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.03385435201121661 | Validation loss: 0.06962902418204717\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.032608688203239346 | Validation loss: 0.03770013412992869\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.03003532220268758 | Validation loss: 0.03331893762307508\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.029064043122154427 | Validation loss: 0.030430115326972946\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.027540825190007445 | Validation loss: 0.03614342541966055\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.02594501869109406 | Validation loss: 0.02946952005316104\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.026418998663321258 | Validation loss: 0.02940933160217745\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.027487545262840463 | Validation loss: 0.031994336284697056\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.02819501824025822 | Validation loss: 0.03246153142702367\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.029957246288990345 | Validation loss: 0.04025745957291552\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.03173960026020441 | Validation loss: 0.036120299715548754\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.029832631266274588 | Validation loss: 0.038290795271417925\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.02817412253991678 | Validation loss: 0.03558004015524473\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.026782008531556382 | Validation loss: 0.03356315528175661\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.0255094195689582 | Validation loss: 0.035678128977971416\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.024149992295003277 | Validation loss: 0.029395997058600187\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.024749841983997968 | Validation loss: 0.033817525553916185\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.025393155030527612 | Validation loss: 0.030692029611340592\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.02652351208016034 | Validation loss: 0.03221092772270952\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.0284297236192184 | Validation loss: 0.05041489251224058\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.029818793497525337 | Validation loss: 0.05839175677725247\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.0283982064877826 | Validation loss: 0.035961875425917764\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.026623119829161986 | Validation loss: 0.031139185119952475\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.025278138358342816 | Validation loss: 0.028934034053236246\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.024122276865854497 | Validation loss: 0.03136570605316332\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396  ]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.02287417855227321 | Validation loss: 0.030645103260342563\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.023657108806603686 | Validation loss: 0.03877665574795434\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.02401751356305388 | Validation loss: 0.03245842074310141\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.0250010577000193 | Validation loss: 0.031518987978675535\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.0268116102750769 | Validation loss: 0.03552219265007547\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.0279629380249123 | Validation loss: 0.030541598929890564\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.026884139392418953 | Validation loss: 0.040323719648378234\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.025402794497435897 | Validation loss: 0.030501875061807886\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.024031604201025594 | Validation loss: 0.029525416969720806\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.02290574425634572 | Validation loss: 0.03327698606465544\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451 ]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02188899334921403 | Validation loss: 0.02941775681184871\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.022578422642668815 | Validation loss: 0.03252738980310304\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.02271142152395493 | Validation loss: 0.029476982774212956\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.02377902618722945 | Validation loss: 0.028569937317765186\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.025357732032539278 | Validation loss: 0.029860973424677337\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.02673468557262142 | Validation loss: 0.030372924005080546\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.025845129039077982 | Validation loss: 0.030393184827906743\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.024658304374150144 | Validation loss: 0.03845536522567272\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.023075178920526088 | Validation loss: 0.04256981957171645\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.022049285026600328 | Validation loss: 0.02848286697241877\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.021476434740593763 | Validation loss: 0.027760279697499106\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.021816314943891957 | Validation loss: 0.03098618079509054\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.022551793093997533 | Validation loss: 0.028238401216055666\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.02308060474527197 | Validation loss: 0.058629569464496205\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.02402396142725053 | Validation loss: 0.03021541171308075\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.025654969449208035 | Validation loss: 0.031186954717018774\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.024714468421823368 | Validation loss: 0.03125418509755816\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.023940351401735855 | Validation loss: 0.03264554476897631\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.022440335754971435 | Validation loss: 0.028461951529607177\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.021251822916442543 | Validation loss: 0.02842777268961072\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.020347900972588033 | Validation loss: 0.028554076927581003\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.020555462850999785 | Validation loss: 0.02733869418235762\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.021171740918776126 | Validation loss: 0.029619899511869465\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.02220173811585438 | Validation loss: 0.0321574957509126\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.023414065705506297 | Validation loss: 0.044605502991804054\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.024443574798694714 | Validation loss: 0.07190789042838983\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.023700900576493846 | Validation loss: 0.03251718570079122\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.02247146178960679 | Validation loss: 0.0362437991425395\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.022056101961439947 | Validation loss: 0.0278464819171599\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.020700616911583678 | Validation loss: 0.028671946642654284\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.01958366129282347 | Validation loss: 0.027183285082823465\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.019975665894268854 | Validation loss: 0.028586781550464884\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.020705172235555038 | Validation loss: 0.03128787662301745\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.02107667978436542 | Validation loss: 0.030051063679690872\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.022610661867112528 | Validation loss: 0.04852165055594274\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.023561506458308276 | Validation loss: 0.06899371064667191\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.0233066179448875 | Validation loss: 0.04566473427361676\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.021634616137565513 | Validation loss: 0.04841635775353227\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.020792985552679596 | Validation loss: 0.03520082090316074\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.019773167533406275 | Validation loss: 0.03094479950544025\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.01904741757950647 | Validation loss: 0.026810297376609275\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.019134292223801214 | Validation loss: 0.02902802246223603\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.02028068473820037 | Validation loss: 0.03634647925251296\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.021072582406090286 | Validation loss: 0.040620867628604174\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.022031776501032396 | Validation loss: 0.06249447592667171\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.023192723667839678 | Validation loss: 0.03574155330924051\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.02227437675635262 | Validation loss: 0.07030766138008662\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.020985245057268113 | Validation loss: 0.03414160332509449\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.019905526866772917 | Validation loss: 0.033079460595867465\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.01939150384756002 | Validation loss: 0.029185153178072402\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103 ]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.01826417470366005 | Validation loss: 0.026967533764296343\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.018468463992154818 | Validation loss: 0.031554549100941846\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.019273764086599515 | Validation loss: 0.033024010847189596\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.019958532049994523 | Validation loss: 0.030514458859605447\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.02119076339626397 | Validation loss: 0.03831826276811106\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.02249043524583302 | Validation loss: 0.07168984226882458\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.021602770646776612 | Validation loss: 0.03142295591533184\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.02026850242182855 | Validation loss: 0.035348252459828346\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.01939214747474809 | Validation loss: 0.030120848145868098\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.018289907337582813 | Validation loss: 0.029019300393494114\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.01761306544038944 | Validation loss: 0.026833688773747002\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.017443752841933106 | Validation loss: 0.028782523203907267\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.018654791837012987 | Validation loss: 0.028098878982876028\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.019123788587976157 | Validation loss: 0.03758659366784351\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.020131324921409045 | Validation loss: 0.029096519175384725\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.02155784703980435 | Validation loss: 0.03155151415350182\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.020959843873856514 | Validation loss: 0.058786045626870224\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.019591272861613492 | Validation loss: 0.03170138590836099\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.01901846544673227 | Validation loss: 0.027040495770052075\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.017848043925908764 | Validation loss: 0.030091944117365137\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.017326634074372006 | Validation loss: 0.026434293615498712\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.017380717077966387 | Validation loss: 0.029182153953505412\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.017231200886057403 | Validation loss: 0.030624900878007923\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.018560519161808297 | Validation loss: 0.03458159559938524\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.019787704303285213 | Validation loss: 0.05616758417870317\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.020979577028079004 | Validation loss: 0.06432282871433667\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.01990094185973389 | Validation loss: 0.028011672664433718\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.019052211199016348 | Validation loss: 0.03703709126317075\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.017911518242482733 | Validation loss: 0.03025994263589382\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.017369525834569723 | Validation loss: 0.02964053342917136\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.016482023953846316 | Validation loss: 0.025826098497158716\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.01659158083261937 | Validation loss: 0.02873330456869943\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.01691494110453026 | Validation loss: 0.027677631338260004\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.017991937094981352 | Validation loss: 0.04074185960260885\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.018723719999406155 | Validation loss: 0.030231072633926357\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.020276146350292172 | Validation loss: 0.043174420217318196\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.019531042235562714 | Validation loss: 0.03581643250903913\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.018447091965746832 | Validation loss: 0.044610560472522466\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.017366024497063544 | Validation loss: 0.028554132840197\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.016597856987055723 | Validation loss: 0.03181937616318464\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261 ]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.0163807039247538 | Validation loss: 0.02585350861772895\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.01622825662597893 | Validation loss: 0.03245220848891352\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.01662803715004063 | Validation loss: 0.02856071732406105\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.01692930633595925 | Validation loss: 0.030707208572753837\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.018463662787666167 | Validation loss: 0.03464765181498868\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.019447343614202264 | Validation loss: 0.03708033948870642\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.018903135850162404 | Validation loss: 0.03530201914587191\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.01786497042786966 | Validation loss: 0.03155623323151043\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.016593552877884207 | Validation loss: 0.028088790896747793\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.015996790239284558 | Validation loss: 0.030268458050808737\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.015754348903182683 | Validation loss: 0.026033666616837894\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.015584597916804194 | Validation loss: 0.029790076526946256\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.016471456264986133 | Validation loss: 0.028614665381610394\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.01635930025987933 | Validation loss: 0.028159564627068385\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.01770466239933621 | Validation loss: 0.030318220911015357\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.019188743718801353 | Validation loss: 0.028221322023975\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.018213050498634697 | Validation loss: 0.03194981573947838\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.017296552865357116 | Validation loss: 0.040214699027793746\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.01603712801957821 | Validation loss: 0.03133211996672409\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.015969472931607102 | Validation loss: 0.029272598114662936\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.015087590197941697 | Validation loss: 0.024819848526801382\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.015174830080638451 | Validation loss: 0.03065409930422902\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.015296503244634203 | Validation loss: 0.031787786699299304\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.015783516960824286 | Validation loss: 0.03863530486289944\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.017338460010317403 | Validation loss: 0.027448586221518263\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.018564120671386276 | Validation loss: 0.032875948186431615\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.0181109133248437 | Validation loss: 0.03741413594356605\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.016321961830438273 | Validation loss: 0.03071831968346877\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.015929741894720288 | Validation loss: 0.0271761506529791\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.015383159098162399 | Validation loss: 0.026942191678764566\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.014515927291429805 | Validation loss: 0.024746225814202\n",
      "Validation loss (ends of cycles): [0.22635138 0.04029037 0.0343265  0.03267501 0.03091741 0.02946952\n",
      " 0.029396   0.0306451  0.02941776 0.02776028 0.02855408 0.02718329\n",
      " 0.0268103  0.02696753 0.02683369 0.02643429 0.0258261  0.02585351\n",
      " 0.02603367 0.02481985 0.02474623]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 12\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2637900622629444 | Validation loss: 0.2553698234260082\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.16978094037546804 | Validation loss: 0.08061320481023618\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07621631738145342 | Validation loss: 0.0826724971245442\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.061927354770150746 | Validation loss: 0.15350222560976232\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.05844452919114215 | Validation loss: 0.18728351167270116\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.055577036785150344 | Validation loss: 0.061056972414787324\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.05109959298948407 | Validation loss: 0.05445564238886748\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.04711966514059527 | Validation loss: 0.05329546033005629\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.044755841250934826 | Validation loss: 0.049413079262844155\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04260337657412054 | Validation loss: 0.04562658929665174\n",
      "Validation loss (ends of cycles): [0.25536982]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.040551968409042606 | Validation loss: 0.042534173532788246\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04085618404196462 | Validation loss: 0.042740331896181614\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04147623869616855 | Validation loss: 0.04740859594728265\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04226049646614534 | Validation loss: 0.06226673981707011\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04316315458340925 | Validation loss: 0.0443901874324573\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04279391495850284 | Validation loss: 0.060327282308467796\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.040710862269738184 | Validation loss: 0.05306545631693942\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0389511743522728 | Validation loss: 0.03878095995501748\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03631727450606432 | Validation loss: 0.03926701827107796\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03461150976599228 | Validation loss: 0.035595839111400504\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03270029825976624 | Validation loss: 0.034391016932204366\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.032733950070162054 | Validation loss: 0.033897070114367774\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03373902814409994 | Validation loss: 0.03615709903117802\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.035701153863990594 | Validation loss: 0.04238018014335206\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03666398287769633 | Validation loss: 0.053025079325639775\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03761743522637528 | Validation loss: 0.040236693840207796\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03550851068681913 | Validation loss: 0.03966777375899255\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03362350801179404 | Validation loss: 0.03458143038941281\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03134352446492264 | Validation loss: 0.03413207765801677\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03009785374679305 | Validation loss: 0.039865221961268356\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02849840317373937 | Validation loss: 0.031089227979204485\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.02911056212794322 | Validation loss: 0.03221330076589116\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.030087379848034033 | Validation loss: 0.03465819009579718\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.03126076528960876 | Validation loss: 0.034185842610895634\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.03246060802190289 | Validation loss: 0.03449801728129387\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.0336248841860637 | Validation loss: 0.053725846244820526\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.031415612436831 | Validation loss: 0.037223817381475656\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.030353294953191088 | Validation loss: 0.04098063687394772\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.028800708188973217 | Validation loss: 0.052007128085408895\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.02750550863532885 | Validation loss: 0.03739549871534109\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.02563297042721196 | Validation loss: 0.02868142564381872\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.02616311686852502 | Validation loss: 0.03076405274415655\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.027380637066657484 | Validation loss: 0.03508420488131898\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.02831252516711108 | Validation loss: 0.04598719187613044\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.029504929771005866 | Validation loss: 0.03175034220995648\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.03159959042663516 | Validation loss: 0.07434647623449564\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.030014191938918612 | Validation loss: 0.033090820303186774\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.02731679374284228 | Validation loss: 0.04520116526899593\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.0262628064155277 | Validation loss: 0.034826266346499324\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.024662429327333747 | Validation loss: 0.036349371408245394\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.02363490600983503 | Validation loss: 0.02791736850381962\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.023922254276737148 | Validation loss: 0.03908925157572542\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.025039831698619522 | Validation loss: 0.038353501952120235\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.0258442997924893 | Validation loss: 0.039914444155458896\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.027072010339362178 | Validation loss: 0.058511382301471064\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.028190431893927607 | Validation loss: 0.04542659169861248\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.027136848568313034 | Validation loss: 0.035249374906665513\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.026047875482573923 | Validation loss: 0.04109857104984777\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.02399788330863362 | Validation loss: 0.038921072048002055\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.022918796129072244 | Validation loss: 0.03578687781867172\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.02215754752413102 | Validation loss: 0.027188095430444394\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.022274650759648094 | Validation loss: 0.03344160255177745\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.02273730670482765 | Validation loss: 0.040124644458826096\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.02412148073375949 | Validation loss: 0.049811005658869235\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.025627500584974944 | Validation loss: 0.040126007582460134\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.027548986309875362 | Validation loss: 0.03670580784923264\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.025487642145153845 | Validation loss: 0.04663573281972536\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.02363148095606551 | Validation loss: 0.043784839500273974\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.022655583384563686 | Validation loss: 0.052880666351744106\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.021290708928998665 | Validation loss: 0.03772427241450974\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881 ]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.020642276361267937 | Validation loss: 0.026211059918361052\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.020620109798077508 | Validation loss: 0.03342627454549074\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.021390452662384825 | Validation loss: 0.04179773193651012\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.022596948584051507 | Validation loss: 0.05714830097609332\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.023441489446561346 | Validation loss: 0.06146918742784432\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.025943865520721265 | Validation loss: 0.07486197179449457\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.02424021406091659 | Validation loss: 0.03962666341768844\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.022813132383411955 | Validation loss: 0.0878413732030562\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.0214004504234202 | Validation loss: 0.036098147343311994\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.020235511447917593 | Validation loss: 0.030150488950312138\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.019402427983428786 | Validation loss: 0.025317091960459948\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.019609720393381862 | Validation loss: 0.033331186211268814\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.020388572548444454 | Validation loss: 0.046018143982759545\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.021280125978776078 | Validation loss: 0.04034127773983138\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.022711353089946967 | Validation loss: 0.03409364766308239\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.02453193292097162 | Validation loss: 0.08150071637438876\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.02312512177717589 | Validation loss: 0.050399243499019315\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.021199130441857734 | Validation loss: 0.04424859756337745\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.02017700191217697 | Validation loss: 0.04524432375494923\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.019404342235914366 | Validation loss: 0.0321854111805026\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.01875441001826211 | Validation loss: 0.024785825766489973\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.018730865393648384 | Validation loss: 0.02874932675955019\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.01903830907545109 | Validation loss: 0.049793892606560676\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.019815182877018264 | Validation loss: 0.03450491282689784\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.02128261590549941 | Validation loss: 0.03380642605147192\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.023460421202183977 | Validation loss: 0.04820313796933208\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.022719959653009048 | Validation loss: 0.04494413600436279\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.020536305857935415 | Validation loss: 0.07395922485738993\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.019407542622252274 | Validation loss: 0.04475157501708184\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.018422949634990107 | Validation loss: 0.029646523080633154\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.01798046646481342 | Validation loss: 0.02428950648754835\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.01773889101702373 | Validation loss: 0.026878266878026937\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.017975333689708217 | Validation loss: 0.0366562896940325\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.01922888393223527 | Validation loss: 0.03991323919035494\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.021070435833608212 | Validation loss: 0.02945878236953701\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.022624185305043513 | Validation loss: 0.039455125973160775\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.021214887884404014 | Validation loss: 0.04384329728782177\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.019580216475042253 | Validation loss: 0.033993630370657356\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.01824560266568774 | Validation loss: 0.034660790281902464\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.017534620688496694 | Validation loss: 0.02668355663107442\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.017282390727582368 | Validation loss: 0.023429298324377408\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.017200304728815792 | Validation loss: 0.028306924737989902\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.017457029978303534 | Validation loss: 0.039004568476229906\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.018181345012672396 | Validation loss: 0.037324963070984395\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.019610544768452403 | Validation loss: 0.05263229812096272\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.021719489280541657 | Validation loss: 0.10329486296645232\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.01992288611731247 | Validation loss: 0.03785114236442106\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.018402819861073484 | Validation loss: 0.037453519978693554\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.017824132010521677 | Validation loss: 0.04232800778533731\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.016857583978805345 | Validation loss: 0.027738639802139784\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293 ]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.016807740439212154 | Validation loss: 0.02330138411239854\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.01654348725845155 | Validation loss: 0.026412564745571996\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.016701313612326556 | Validation loss: 0.040867413128060956\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.017480136147937794 | Validation loss: 0.04603183016713176\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.01918658996302529 | Validation loss: 0.06513113408748593\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.02093221848973861 | Validation loss: 0.03038291127554008\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.019247326766129447 | Validation loss: 0.02680572501516768\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.018294873881165075 | Validation loss: 0.03915104289938297\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.017199005021031208 | Validation loss: 0.03470036167917507\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.01629751831903933 | Validation loss: 0.027707418227302178\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.016322553382153333 | Validation loss: 0.023030078088465546\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.01574328486517313 | Validation loss: 0.025941874748761102\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.01595830355736951 | Validation loss: 0.026795019695003117\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.01674997114278527 | Validation loss: 0.032723173864984086\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.018060487297246693 | Validation loss: 0.05259248175259147\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.019997143852128554 | Validation loss: 0.029469374334439635\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.018740022800078516 | Validation loss: 0.03724197328223714\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.017349006890918804 | Validation loss: 0.03206472541205585\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.016172322676505757 | Validation loss: 0.031014696528602923\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.015613548719674832 | Validation loss: 0.02622273386389549\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.015545637761563183 | Validation loss: 0.02298504370264709\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.015161730000289225 | Validation loss: 0.025938816684564308\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.015637453324972135 | Validation loss: 0.031124545161479285\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.016111005887555086 | Validation loss: 0.030322041545462395\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.017304543302081494 | Validation loss: 0.030219923222570548\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.01862924370673383 | Validation loss: 0.037004510373143215\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.01818571688403726 | Validation loss: 0.031312630412035754\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.016826508900429676 | Validation loss: 0.025547978468239307\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.015718223280103704 | Validation loss: 0.025384996768220196\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.01519297607250602 | Validation loss: 0.026305198087356985\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.015273088334799416 | Validation loss: 0.02278466535998242\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.014718375066936257 | Validation loss: 0.025838033728567616\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.015016126022240532 | Validation loss: 0.02520066301804036\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.015792525130199637 | Validation loss: 0.033811263440709026\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.016720541877302562 | Validation loss: 0.03242217247108264\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.01912484302370232 | Validation loss: 0.0287974347925878\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.017655496700451925 | Validation loss: 0.028920547171894993\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.016262264513158 | Validation loss: 0.026407693163491786\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.015085211046311537 | Validation loss: 0.026688831109952713\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.014528502254871282 | Validation loss: 0.026294793401445662\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.014659694655183778 | Validation loss: 0.022498133392738446\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.014347941379028896 | Validation loss: 0.026282045541198125\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.014263723311410379 | Validation loss: 0.03165137199019747\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.015251761050668625 | Validation loss: 0.027164413177940463\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.016272441059848677 | Validation loss: 0.029640018258110752\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.017680507535982107 | Validation loss: 0.029534945397504737\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.017353308862188326 | Validation loss: 0.02775626107385116\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.016023863398239982 | Validation loss: 0.02645636486288692\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.01442366958923849 | Validation loss: 0.02888286848818617\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.014458691623996989 | Validation loss: 0.027762371946924498\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.014444907576690319 | Validation loss: 0.0224708960325058\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.013903563040938333 | Validation loss: 0.025974585957426046\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.014055204803976212 | Validation loss: 0.027087848567004715\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.014722083803635799 | Validation loss: 0.03538105211087635\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.015269574309559728 | Validation loss: 0.030134127608367374\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.01760749817686344 | Validation loss: 0.032296386281294484\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.016067436583310005 | Validation loss: 0.02902796821269606\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.015462717003667886 | Validation loss: 0.027272284696144716\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.014177237382829793 | Validation loss: 0.0292433856853417\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.014127726506381503 | Validation loss: 0.02626519976183772\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709 ]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.014095621309827576 | Validation loss: 0.02228838494712753\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.013186523555080418 | Validation loss: 0.026403546100482345\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.013458487034779087 | Validation loss: 0.03125196968072227\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.014144350702003369 | Validation loss: 0.027981760312936137\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.01527679824790642 | Validation loss: 0.02871349892978157\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.01699371934351832 | Validation loss: 0.03547387194287564\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.016371346983741892 | Validation loss: 0.029047775315120816\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.014853418538882304 | Validation loss: 0.029449129876281534\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.013668709351617073 | Validation loss: 0.02833327539597771\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.013487473279162337 | Validation loss: 0.028004308929666877\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.013926172044565562 | Validation loss: 0.021582843336675848\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.013011930078675269 | Validation loss: 0.025751842485208596\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.012926986992928512 | Validation loss: 0.027355525941987122\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.013586024041117927 | Validation loss: 0.030126292291762575\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.015417855717467996 | Validation loss: 0.030633447425706044\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.016156206774687477 | Validation loss: 0.03374442298497472\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.014926741387966553 | Validation loss: 0.02969645961586918\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.014366873232685482 | Validation loss: 0.028297341195866466\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.013488575995254975 | Validation loss: 0.02735566158246781\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.012988377337038576 | Validation loss: 0.024107716279104352\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.013492380542235697 | Validation loss: 0.021781244115637883\n",
      "Validation loss (ends of cycles): [0.25536982 0.04253417 0.03439102 0.03108923 0.02868143 0.02791737\n",
      " 0.0271881  0.02621106 0.02531709 0.02478583 0.02428951 0.0234293\n",
      " 0.02330138 0.02303008 0.02298504 0.02278467 0.02249813 0.0224709\n",
      " 0.02228838 0.02158284 0.02178124]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 13\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.1930667787182088 | Validation loss: 0.1819245464823864\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.11809876634150135 | Validation loss: 0.07254008576273918\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.06877991482615471 | Validation loss: 0.0805306493960045\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.0631012045835354 | Validation loss: 0.05939289858495748\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.05970304113413606 | Validation loss: 0.056863684483148436\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.057145256518709414 | Validation loss: 0.052280418988731175\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.05306313728191415 | Validation loss: 0.046050683284799256\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.0496613971204782 | Validation loss: 0.04423446822221632\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04634420350574109 | Validation loss: 0.04120242133460663\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04390166444057713 | Validation loss: 0.03790110322060408\n",
      "Validation loss (ends of cycles): [0.18192455]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.042118645292155595 | Validation loss: 0.03582969597644276\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04284505806863308 | Validation loss: 0.03783127010144569\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04320145487709313 | Validation loss: 0.03778853467493146\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.044070840353260234 | Validation loss: 0.03866708637387664\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.045151009517056605 | Validation loss: 0.0393032300527449\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04501143620178408 | Validation loss: 0.0551495544474434\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.043016062821356615 | Validation loss: 0.07461784904201825\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0403907396849625 | Validation loss: 0.036994416266679764\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0384160772764257 | Validation loss: 0.03323371660102297\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.036099768840536776 | Validation loss: 0.03114642182158099\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.034705798651034736 | Validation loss: 0.030475210260461877\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03530205554347866 | Validation loss: 0.030978830186305224\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03607280668327395 | Validation loss: 0.03204350987518275\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03704883032763491 | Validation loss: 0.033623026132031726\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03808547970743812 | Validation loss: 0.034693438559770584\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0391657291474391 | Validation loss: 0.035492373906351904\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03752170335881564 | Validation loss: 0.03386187332647818\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.035501095694394746 | Validation loss: 0.033731289483882765\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03345001264266213 | Validation loss: 0.029232336277211154\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03161927238106728 | Validation loss: 0.028818574630551867\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.029917811959677812 | Validation loss: 0.02814174229624095\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.030526079264070305 | Validation loss: 0.028566454364745704\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.031387867964804174 | Validation loss: 0.028836151205555157\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.03292266800239378 | Validation loss: 0.03008065648652889\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.03430468309860753 | Validation loss: 0.030857506449575776\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.0357665854715267 | Validation loss: 0.031715808574248244\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.03369065793611261 | Validation loss: 0.033253426422123554\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.03193698752367375 | Validation loss: 0.030685135404820794\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.030100774111188187 | Validation loss: 0.0328261045118173\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.028577730930125226 | Validation loss: 0.027603525944330073\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.027103183905080874 | Validation loss: 0.02621252317395475\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.02798980980047158 | Validation loss: 0.02808722888154012\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.02831293367685712 | Validation loss: 0.02703952934179041\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.03022744706166642 | Validation loss: 0.029000978502962325\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.031051456407472797 | Validation loss: 0.030444668605923653\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.032409838005444225 | Validation loss: 0.03240376365957437\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.0312417740116314 | Validation loss: 0.04385587483368538\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.0294837532641024 | Validation loss: 0.03135377789537112\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.027556763897288818 | Validation loss: 0.02746297750208113\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.025821912870267215 | Validation loss: 0.027316523577879975\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.024773090705275537 | Validation loss: 0.02547023341887527\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.0251947464079273 | Validation loss: 0.026439772573886095\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.026060407675270524 | Validation loss: 0.030076515550414722\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.02717079576485011 | Validation loss: 0.03210321586165163\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.0285390988296392 | Validation loss: 0.030368766023053065\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.030521390408429563 | Validation loss: 0.029636258983777627\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.029085740456547663 | Validation loss: 0.03383835812133772\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.02747641666887366 | Validation loss: 0.029344955597210814\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.025737719032533315 | Validation loss: 0.032367387855494464\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.024154662392197216 | Validation loss: 0.026446550946544717\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.023362241928674737 | Validation loss: 0.024779479191810998\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.02353256069579903 | Validation loss: 0.028069672726646618\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.023943516661470033 | Validation loss: 0.026808338826177298\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.025357702752689316 | Validation loss: 0.02782607140640418\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.02692143836115696 | Validation loss: 0.030238174040008475\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.028473555140805486 | Validation loss: 0.03152257176461043\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.02699487556000145 | Validation loss: 0.047619540589275186\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.02489455645257721 | Validation loss: 0.028346326340127875\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.02349343087182057 | Validation loss: 0.028713655347625416\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.02266941082530788 | Validation loss: 0.02549334270534692\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.021703293554636897 | Validation loss: 0.024168244942470832\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.02196174266995216 | Validation loss: 0.02505502008177616\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.022541736478784255 | Validation loss: 0.03061453970494094\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.023306790157696423 | Validation loss: 0.029162768481506243\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.025527279784104653 | Validation loss: 0.04011596442648658\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.026604378816424586 | Validation loss: 0.033134016794738944\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.025109626031575762 | Validation loss: 0.027811537048331014\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.024159897717514207 | Validation loss: 0.027146308924312943\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.022194400768042827 | Validation loss: 0.026139097840145783\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.020927915326794798 | Validation loss: 0.025522519178964472\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02010866692479776 | Validation loss: 0.023466593240974127\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.02052211769457374 | Validation loss: 0.025561627552465157\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.02113350276189039 | Validation loss: 0.03258011841939555\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.02219036114687214 | Validation loss: 0.031846943552847264\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.023618147117370855 | Validation loss: 0.030687368647367867\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.02571254876651326 | Validation loss: 0.031185772821859078\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.024023403993294556 | Validation loss: 0.02871771556911645\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.022371417447468456 | Validation loss: 0.032844013155058575\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.02110713280889453 | Validation loss: 0.030332010584297003\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.020046295809122372 | Validation loss: 0.025618491763318027\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.019356959265637762 | Validation loss: 0.02316210146441504\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.019383190148415008 | Validation loss: 0.02634717537849038\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.01964251248888215 | Validation loss: 0.03047156140760139\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.02092831273164068 | Validation loss: 0.029502556693774683\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.02249736651322063 | Validation loss: 0.041276842148767576\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.024790223473112803 | Validation loss: 0.03220497692624728\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.02344361155160836 | Validation loss: 0.04404209623182261\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.021694707273676686 | Validation loss: 0.03870899168153604\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.02010431843524685 | Validation loss: 0.028377596702840593\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.018850200651783725 | Validation loss: 0.030515969972367638\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621 ]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.018333698068840467 | Validation loss: 0.022866654968648044\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.01845642739184657 | Validation loss: 0.027251339827974636\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.018576808184461325 | Validation loss: 0.03162037546711939\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.019754362957818166 | Validation loss: 0.030628113283051386\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.020817349562231375 | Validation loss: 0.03573075723316935\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.023296759935209944 | Validation loss: 0.03717152795030011\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.021819874887563744 | Validation loss: 0.04560715691358955\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.02036462054987039 | Validation loss: 0.03906206234737679\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.018941566669287122 | Validation loss: 0.02962375542631856\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.017907579606208875 | Validation loss: 0.025408087988142616\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.017118974164964593 | Validation loss: 0.023260762639067793\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.017573000283493678 | Validation loss: 0.027502070560499473\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.018396547465224046 | Validation loss: 0.02959495989812745\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.018755224162750706 | Validation loss: 0.032247422745934236\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.020228182037874146 | Validation loss: 0.02642800127742467\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.021511572188868816 | Validation loss: 0.029202266009869398\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.02057119747813867 | Validation loss: 0.028558928381513665\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.019055473568792246 | Validation loss: 0.04045708553382644\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.018233418848593624 | Validation loss: 0.035353186505812186\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.017574515155687625 | Validation loss: 0.030527102243569162\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.017130743881345403 | Validation loss: 0.022757303659562713\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.016826823591349686 | Validation loss: 0.025191029671717574\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.016850333925032494 | Validation loss: 0.03346616526444753\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.018192078241584253 | Validation loss: 0.047841162869223845\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.019429788616847018 | Validation loss: 0.038563721226873224\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.021230487420926897 | Validation loss: 0.03223240699757029\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.02028775199373462 | Validation loss: 0.03193793335446605\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.018140019360473568 | Validation loss: 0.02840865741449374\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.017588311623857947 | Validation loss: 0.031451340212866115\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.016793060888137135 | Validation loss: 0.024730978499132174\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573 ]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.016106459407173857 | Validation loss: 0.022969599364808312\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.016320625238348634 | Validation loss: 0.024469188311033778\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.016396109552635832 | Validation loss: 0.02705709463744252\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.01734909897252005 | Validation loss: 0.031023156035829474\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.01820811143297018 | Validation loss: 0.05327481610907449\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.019724413256484028 | Validation loss: 0.03707399743574637\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.019301350711255657 | Validation loss: 0.03078989284457984\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.017556924152435087 | Validation loss: 0.028413188471286384\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.016612043256434252 | Validation loss: 0.025720094572062844\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.016134973512772394 | Validation loss: 0.027998936052123707\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696 ]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.015872289350598444 | Validation loss: 0.022802802872050693\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.015516076696922584 | Validation loss: 0.024056544630891748\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.015703990444426937 | Validation loss: 0.0258673885492263\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.01668028063425908 | Validation loss: 0.030482867594670365\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.017571322114339896 | Validation loss: 0.03652796528681561\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.019416323446725704 | Validation loss: 0.03051293613734069\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.018579784489940014 | Validation loss: 0.03504657910929786\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.016626955300797615 | Validation loss: 0.028009693586715945\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.016029098249819813 | Validation loss: 0.029912206999681615\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.015353832416692558 | Validation loss: 0.023703449750664057\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028 ]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.014883509628018555 | Validation loss: 0.021929300873091927\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.01512682895385185 | Validation loss: 0.023662176673059112\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.014992564006195384 | Validation loss: 0.02634827120022641\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.015701996538864105 | Validation loss: 0.02868762995219893\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.016893358146581723 | Validation loss: 0.026929746002510743\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.01879451618130718 | Validation loss: 0.034038130538883035\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.017982265125123822 | Validation loss: 0.03113197583567213\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.0168210576383435 | Validation loss: 0.03469343262690085\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.015516883760158504 | Validation loss: 0.025853695451385446\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.014961472537596615 | Validation loss: 0.0240436724766537\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293 ]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.01467474195535998 | Validation loss: 0.022376254062961648\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.014463082505199983 | Validation loss: 0.02429829292965156\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.014489603282085487 | Validation loss: 0.024390017406808004\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.014960362051366543 | Validation loss: 0.03855056495026306\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.016004944959541363 | Validation loss: 0.027088109058914362\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.017839457700979344 | Validation loss: 0.027761828982167773\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.016734649878641476 | Validation loss: 0.02709192072075826\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.015986402323279452 | Validation loss: 0.02521773434623524\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.014949820494773437 | Validation loss: 0.028413208270514453\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.014427602009809746 | Validation loss: 0.025387840138541326\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.014175291431649606 | Validation loss: 0.0222464417003923\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.014155056855964417 | Validation loss: 0.025276930174893804\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.013586985551733143 | Validation loss: 0.02459180938010966\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.014965886816534461 | Validation loss: 0.026229242380294535\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.01641260800617082 | Validation loss: 0.02679949150317245\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.017782501397863488 | Validation loss: 0.029586961009988078\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.01672653796493399 | Validation loss: 0.04584400052273715\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.015437085578712274 | Validation loss: 0.02551957640658926\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.014381962241034727 | Validation loss: 0.027132539078593254\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.014216822250841223 | Validation loss: 0.024544751720020064\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.013737957268877297 | Validation loss: 0.02210113792507737\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.013467943696875354 | Validation loss: 0.024385717973389006\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.013636469504586897 | Validation loss: 0.023889954443331116\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.01405429927708239 | Validation loss: 0.02557995884368817\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.01594532945440436 | Validation loss: 0.02542697093277066\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.017133247770597132 | Validation loss: 0.026300121409197647\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.01628313361743123 | Validation loss: 0.027368043307904846\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.014727997079453601 | Validation loss: 0.026379065864064074\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.014223979732819966 | Validation loss: 0.025630985352176207\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.013516011896866317 | Validation loss: 0.025777899901624077\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.013738666835944264 | Validation loss: 0.02189505234774616\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.013424585791950931 | Validation loss: 0.025334533924857777\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.013389314419342851 | Validation loss: 0.0272432800244402\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.013665561119512636 | Validation loss: 0.025919210579660203\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.015035620595955727 | Validation loss: 0.027025571368910647\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.016460462674802664 | Validation loss: 0.02844604704942968\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.015581737829334273 | Validation loss: 0.026644890472568846\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.014256568825138467 | Validation loss: 0.027392463344666693\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.013787354469033224 | Validation loss: 0.027793116833048838\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.013205509652782764 | Validation loss: 0.024656895755065814\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.012971938055540835 | Validation loss: 0.02208654147883256\n",
      "Validation loss (ends of cycles): [0.18192455 0.0358297  0.03047521 0.02814174 0.02621252 0.02547023\n",
      " 0.02477948 0.02416824 0.02346659 0.0231621  0.02286665 0.02326076\n",
      " 0.0227573  0.0229696  0.0228028  0.0219293  0.02237625 0.02224644\n",
      " 0.02210114 0.02189505 0.02208654]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 14\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.12228791815478628 | Validation loss: 0.1054369103577402\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08345528741980471 | Validation loss: 0.06988698575231764\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.06868993813489996 | Validation loss: 0.06038115518512549\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.06472492747281383 | Validation loss: 0.054184967582976376\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06308126072512894 | Validation loss: 0.13389017957228203\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.060122362686306 | Validation loss: 0.17819979400546462\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.055708695059203034 | Validation loss: 0.049357822785774864\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.052122626143197216 | Validation loss: 0.05457461749513944\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.049281957006551386 | Validation loss: 0.04812982305884361\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04547054481851619 | Validation loss: 0.040970456407026005\n",
      "Validation loss (ends of cycles): [0.10543691]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.043980468833077005 | Validation loss: 0.038650386242402926\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04430758124747412 | Validation loss: 0.03830565043069698\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04522882570035574 | Validation loss: 0.038157600081629224\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04559288569915343 | Validation loss: 0.039070602782346586\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04618575094585738 | Validation loss: 0.03879287556089737\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04678545226683704 | Validation loss: 0.04081984361012777\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0438527973951787 | Validation loss: 0.03588898617912222\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.041553754484417234 | Validation loss: 0.04742350084362207\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03968509051161326 | Validation loss: 0.03589330261780156\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.0379939789058474 | Validation loss: 0.03520316261522196\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03644957100760525 | Validation loss: 0.032497044583713566\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03638568487230355 | Validation loss: 0.03465360265087198\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03805654572428969 | Validation loss: 0.032737655737609776\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03894560454352721 | Validation loss: 0.04324095565135832\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.04015268617105193 | Validation loss: 0.03395191821511145\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04047986775697247 | Validation loss: 0.03522724899704809\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.038433285021200414 | Validation loss: 0.039532348375629495\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03698946019195444 | Validation loss: 0.03227757614243914\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03505631038404214 | Validation loss: 0.03230649564001295\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.033620467814579 | Validation loss: 0.03223192781485893\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03178789988690155 | Validation loss: 0.030227238519324198\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.03240554717679819 | Validation loss: 0.030879514326375944\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.03349711600978806 | Validation loss: 0.031250168948813724\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.034300035622909786 | Validation loss: 0.031172568392422464\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.03561411505731625 | Validation loss: 0.035509896154205\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.035965731965635364 | Validation loss: 0.037915085131923355\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.03542989279650818 | Validation loss: 0.03176571290802072\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.03348708177757699 | Validation loss: 0.04181969317573088\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.032106196553241914 | Validation loss: 0.029362012459724036\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.030441816941630548 | Validation loss: 0.029443050079323626\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.028951545102082615 | Validation loss: 0.027948243957426813\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.02955955447159647 | Validation loss: 0.02866512626685478\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.030229009668971223 | Validation loss: 0.029844649274040153\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.031777786844357 | Validation loss: 0.029600525412846496\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.03273992139724939 | Validation loss: 0.03340166689897025\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.03442120896198037 | Validation loss: 0.03195419293586855\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.03261524584235215 | Validation loss: 0.0315606604433722\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.03111256630258347 | Validation loss: 0.029374615530724877\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.029745885477651182 | Validation loss: 0.029515129479545134\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.028203826734599303 | Validation loss: 0.027964135907866335\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.02741435214089669 | Validation loss: 0.02691561297548038\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.02772532103628647 | Validation loss: 0.028478951848767423\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.028424028443490585 | Validation loss: 0.028361442671329888\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.028827177053998884 | Validation loss: 0.03354962459868855\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.030674236258719026 | Validation loss: 0.02970579656323901\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.03177045115337866 | Validation loss: 0.029968159445733937\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.03085786577435286 | Validation loss: 0.029443291947245598\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.02905051384832922 | Validation loss: 0.02832780895685708\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.02775720836276688 | Validation loss: 0.027414744236954936\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.02642078349760514 | Validation loss: 0.02696808020549792\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.025448385527645185 | Validation loss: 0.02623730621956013\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.025777471902954385 | Validation loss: 0.0270316894683573\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.026880092464569138 | Validation loss: 0.02829531321509017\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.02747904618338841 | Validation loss: 0.029463160783052444\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.029024402031720412 | Validation loss: 0.030591066995704616\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.03056626948641568 | Validation loss: 0.03102897756077625\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.02904554159282063 | Validation loss: 0.03191932718510981\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.02752647077573872 | Validation loss: 0.02758541154778666\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.026392553770172644 | Validation loss: 0.027598489075899124\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.024825475034796124 | Validation loss: 0.027477786013925518\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.02389349034851099 | Validation loss: 0.025481665368985246\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.024810818970445694 | Validation loss: 0.0258330095352398\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.025311344723421628 | Validation loss: 0.026171780950217334\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.026799078659737498 | Validation loss: 0.027304544803444988\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.02739570381468147 | Validation loss: 0.029297827053125256\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.02901256551996358 | Validation loss: 0.03376480612765859\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.027922531536863585 | Validation loss: 0.031056516907281347\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.026418042119319845 | Validation loss: 0.02869530960365578\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.024494294804043887 | Validation loss: 0.02879283722076151\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.023640055410049068 | Validation loss: 0.025536342110070918\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02292518796611244 | Validation loss: 0.024924556069352007\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.022964879946132017 | Validation loss: 0.025884984164602227\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.024365100468228745 | Validation loss: 0.02613054729860138\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.024947036228074534 | Validation loss: 0.026084760548891844\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.026113691951746378 | Validation loss: 0.029804825920749595\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.02732016758357243 | Validation loss: 0.028548474147639894\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.026706833134942907 | Validation loss: 0.02933104053415634\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.02524915052108406 | Validation loss: 0.028903299735652074\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.023512191850904043 | Validation loss: 0.02478121103787864\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.0227027192325672 | Validation loss: 0.024699235369485838\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.02195627812099287 | Validation loss: 0.024092087763603085\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.022029758766066374 | Validation loss: 0.024637732999744238\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.02214197951134264 | Validation loss: 0.02562687725380615\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.023682874324542236 | Validation loss: 0.025451905301047698\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.0249582189759921 | Validation loss: 0.026010741134760557\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.026149797552393945 | Validation loss: 0.029183034267690446\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.025613661204290584 | Validation loss: 0.029470085580315854\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.02367819772987831 | Validation loss: 0.02606600364325223\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.022782948269410346 | Validation loss: 0.025672975306709606\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.02164456419025858 | Validation loss: 0.024995564966014138\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.020927290345080257 | Validation loss: 0.023898190818727016\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.020838848724052672 | Validation loss: 0.024341666174155695\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.02157418485882321 | Validation loss: 0.02549752972468182\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.02215171731796449 | Validation loss: 0.027644692678694373\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.023533740482194636 | Validation loss: 0.02766746641309173\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.02504744735826564 | Validation loss: 0.03269333695923841\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.02399577457851511 | Validation loss: 0.026279965553570678\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.022653392671296995 | Validation loss: 0.025546919761432543\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.021445270692096736 | Validation loss: 0.024732283109592065\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.02024538448928454 | Validation loss: 0.024346998823737656\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.020088354396308218 | Validation loss: 0.023614108493482625\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.020159628817705603 | Validation loss: 0.023966707841113762\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.02066820897767699 | Validation loss: 0.024948603266643152\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.021366572594345826 | Validation loss: 0.02552163469846602\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.022936123722361597 | Validation loss: 0.027489966126503767\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.02412121834960289 | Validation loss: 0.027228782815789734\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.023651672402260508 | Validation loss: 0.025690350233128777\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.021718305630261094 | Validation loss: 0.027504071075883176\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.020649877437428247 | Validation loss: 0.02455218826179151\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.02011051802986824 | Validation loss: 0.024612521649234824\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.019264938839809682 | Validation loss: 0.023097472437829884\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.019316403066542576 | Validation loss: 0.02441213297209254\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.019887421912384955 | Validation loss: 0.02437097444716427\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.020397775916276666 | Validation loss: 0.027738412048805644\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.02181776584792367 | Validation loss: 0.02554246283101815\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.023289408678628082 | Validation loss: 0.032003110343659366\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.022173569053108615 | Validation loss: 0.025447359757015\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.02074599415593879 | Validation loss: 0.02787999822585671\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.019872636575918128 | Validation loss: 0.02400662622380036\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.0191136942778873 | Validation loss: 0.023773066078623135\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.018855253730453853 | Validation loss: 0.022893787844589463\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.018389209398878663 | Validation loss: 0.023396671577184287\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.01898594183574726 | Validation loss: 0.024825873094852322\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.019571426332118065 | Validation loss: 0.03233330883085728\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.020783360165399993 | Validation loss: 0.027140274229976866\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.022257593823823987 | Validation loss: 0.02835464863865464\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.02122602771922219 | Validation loss: 0.024941712376420146\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.020035431871750976 | Validation loss: 0.02537165092373336\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.019187722245337276 | Validation loss: 0.025162397124977025\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.018331920370323267 | Validation loss: 0.024782232664249563\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.018385974502917832 | Validation loss: 0.022544961250214664\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.01765795958008829 | Validation loss: 0.023712188798796247\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.018420973618522407 | Validation loss: 0.023575709166902083\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.019126582281224852 | Validation loss: 0.025220924919402157\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.01961053673526257 | Validation loss: 0.026265721975101367\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.02205375614538183 | Validation loss: 0.02857490042569461\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.020693585647618383 | Validation loss: 0.025435910698164393\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.01922064234797911 | Validation loss: 0.02503646554908267\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.018394955019353003 | Validation loss: 0.024210673067028874\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.017704154535128577 | Validation loss: 0.0237839009474825\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.017490101173686667 | Validation loss: 0.022615981495214835\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 151\n",
      "Training loss: 0.017312443612627987 | Validation loss: 0.02387337434898924\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 152\n",
      "Training loss: 0.017103146326281432 | Validation loss: 0.023941471551855404\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 153\n",
      "Training loss: 0.018206471003728304 | Validation loss: 0.02593925995407281\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 154\n",
      "Training loss: 0.019363362325082827 | Validation loss: 0.02541112327189357\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 155\n",
      "Training loss: 0.020997219991574927 | Validation loss: 0.025342813727480394\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 156\n",
      "Training loss: 0.019623544086073714 | Validation loss: 0.027581549491043442\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 157\n",
      "Training loss: 0.01856963353125545 | Validation loss: 0.025144454367734766\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 158\n",
      "Training loss: 0.017901460539807026 | Validation loss: 0.02464213549952816\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 159\n",
      "Training loss: 0.017084604182197313 | Validation loss: 0.023683185516684142\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598]\n",
      "------------------------------\n",
      "Epoch: 160\n",
      "Training loss: 0.01706684923473352 | Validation loss: 0.022672119033005502\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 161\n",
      "Training loss: 0.01673129112914023 | Validation loss: 0.023932423822029873\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 162\n",
      "Training loss: 0.017171250194371716 | Validation loss: 0.02393032217191325\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 163\n",
      "Training loss: 0.017759795351771683 | Validation loss: 0.026334970016722328\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 164\n",
      "Training loss: 0.01868446520340394 | Validation loss: 0.025735552940103743\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 165\n",
      "Training loss: 0.019864219000441878 | Validation loss: 0.026584884738204657\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 166\n",
      "Training loss: 0.019493408527649272 | Validation loss: 0.02552235554213877\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 167\n",
      "Training loss: 0.0178543335580214 | Validation loss: 0.02480841196935486\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 168\n",
      "Training loss: 0.017060703226948172 | Validation loss: 0.024558599520888593\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 169\n",
      "Training loss: 0.01673183792717452 | Validation loss: 0.023999774656086055\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212]\n",
      "------------------------------\n",
      "Epoch: 170\n",
      "Training loss: 0.01663350821964867 | Validation loss: 0.02244613198908391\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 171\n",
      "Training loss: 0.016103041254180838 | Validation loss: 0.02298469769044055\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 172\n",
      "Training loss: 0.01630746733351815 | Validation loss: 0.02470569105611907\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 173\n",
      "Training loss: 0.017092946812707354 | Validation loss: 0.024231813089163216\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 174\n",
      "Training loss: 0.018157666808418262 | Validation loss: 0.02737403698955421\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 175\n",
      "Training loss: 0.019655547646154476 | Validation loss: 0.027440111980670027\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 176\n",
      "Training loss: 0.01850139775230148 | Validation loss: 0.025333901763790183\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 177\n",
      "Training loss: 0.017738321042672648 | Validation loss: 0.026279438459486874\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 178\n",
      "Training loss: 0.01703572527853757 | Validation loss: 0.02614127082267293\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 179\n",
      "Training loss: 0.0160271596784393 | Validation loss: 0.02310966721011533\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613]\n",
      "------------------------------\n",
      "Epoch: 180\n",
      "Training loss: 0.016043832913661998 | Validation loss: 0.022625376642854127\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 181\n",
      "Training loss: 0.015809292392048046 | Validation loss: 0.023693999517019147\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 182\n",
      "Training loss: 0.016034167735057513 | Validation loss: 0.024256648495793343\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 183\n",
      "Training loss: 0.016748781766500173 | Validation loss: 0.024400403792107547\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 184\n",
      "Training loss: 0.017791693662936852 | Validation loss: 0.02896435482910386\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 185\n",
      "Training loss: 0.019187645261683237 | Validation loss: 0.029182869561568455\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 186\n",
      "Training loss: 0.018036984204413083 | Validation loss: 0.024585284361684764\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 187\n",
      "Training loss: 0.0170689941084815 | Validation loss: 0.02560491528775957\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 188\n",
      "Training loss: 0.016152752020085853 | Validation loss: 0.025931633905404143\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 189\n",
      "Training loss: 0.015870558355397326 | Validation loss: 0.022788437876712392\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538]\n",
      "------------------------------\n",
      "Epoch: 190\n",
      "Training loss: 0.01561052555301628 | Validation loss: 0.022453050267089297\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 191\n",
      "Training loss: 0.015502604122114618 | Validation loss: 0.02315064930115585\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 192\n",
      "Training loss: 0.015278358620069012 | Validation loss: 0.023724095206017846\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 193\n",
      "Training loss: 0.01631206663219425 | Validation loss: 0.02449931452671687\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 194\n",
      "Training loss: 0.01718841513951983 | Validation loss: 0.02486682427978074\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 195\n",
      "Training loss: 0.01918403732370797 | Validation loss: 0.02663724600440926\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 196\n",
      "Training loss: 0.018109782920711165 | Validation loss: 0.02764201440193035\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 197\n",
      "Training loss: 0.016803728043654462 | Validation loss: 0.026375215983501187\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 198\n",
      "Training loss: 0.015610241978542834 | Validation loss: 0.02521015338047787\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 199\n",
      "Training loss: 0.015274432143272182 | Validation loss: 0.023028226431321214\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305]\n",
      "------------------------------\n",
      "Epoch: 200\n",
      "Training loss: 0.015197526428818218 | Validation loss: 0.022503956462498063\n",
      "Validation loss (ends of cycles): [0.10543691 0.03865039 0.03249704 0.03022724 0.02794824 0.02691561\n",
      " 0.02623731 0.02548167 0.02492456 0.02409209 0.02389819 0.02361411\n",
      " 0.02309747 0.02289379 0.02254496 0.02261598 0.02267212 0.02244613\n",
      " 0.02262538 0.02245305 0.02250396]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 15\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.15279348028695536 | Validation loss: 0.14506536836807543\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.0992763647545687 | Validation loss: 0.08038875761513527\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.06854788238853819 | Validation loss: 0.07798023822789009\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.06264783143393907 | Validation loss: 0.06590831738251907\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06060648671076124 | Validation loss: 0.11362000554800034\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.05775577272571292 | Validation loss: 0.16101208042639953\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.05431262623171816 | Validation loss: 0.05958796557612144\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05047389544425947 | Validation loss: 0.05647731199860573\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04694410883312525 | Validation loss: 0.04427507806282777\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04459618917300633 | Validation loss: 0.04090933745297102\n",
      "Validation loss (ends of cycles): [0.14506537]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04245714181921019 | Validation loss: 0.03974307392938779\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04292430049977322 | Validation loss: 0.04144837728773172\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.043996825655884586 | Validation loss: 0.042766958904954105\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04482655116330515 | Validation loss: 0.046519542686068095\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04563175601062746 | Validation loss: 0.040774742857767984\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.044861849880049584 | Validation loss: 0.047626798757566854\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0434767716914898 | Validation loss: 0.05095124581398872\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0410251230046696 | Validation loss: 0.045552261030444734\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03894493085622546 | Validation loss: 0.03951437076410422\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03676822814366475 | Validation loss: 0.036260907824795977\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03533417369551987 | Validation loss: 0.0353879089682148\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03569188686502486 | Validation loss: 0.037464807741343975\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03668812884070612 | Validation loss: 0.03660466314221804\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.037543252314090246 | Validation loss: 0.041893985194082446\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03912920638285426 | Validation loss: 0.05439267536768547\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.040333300218106764 | Validation loss: 0.03696370225113172\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03786210742918586 | Validation loss: 0.03918445941347342\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03594942207670646 | Validation loss: 0.037800938105927065\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.0335141976216906 | Validation loss: 0.03604494651349691\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03202974798557488 | Validation loss: 0.03684409148991108\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.031134030864638115 | Validation loss: 0.03303491732535454\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.03132372897359644 | Validation loss: 0.05156394910927002\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.03225202846382311 | Validation loss: 0.03565296494903473\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.033366888260039 | Validation loss: 0.04518821405676695\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.03463769986139618 | Validation loss: 0.0466082334661713\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.035970898729707546 | Validation loss: 0.035931507841898844\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.033702543582239376 | Validation loss: 0.039530598415205113\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.03226060464390015 | Validation loss: 0.035033257463230535\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.030439186973218253 | Validation loss: 0.03421707866856685\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.028959295595524764 | Validation loss: 0.03411362642565599\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.028355397585673854 | Validation loss: 0.03144805756612466\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.028508084403871283 | Validation loss: 0.034525489721160665\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.02908853772152894 | Validation loss: 0.03552745999051975\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.030279508965308607 | Validation loss: 0.0412328397282041\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.03161932406607668 | Validation loss: 0.04157860963963545\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.032413170465573606 | Validation loss: 0.03746562243367617\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.03155249784802377 | Validation loss: 0.03418631808689007\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.029698396640780726 | Validation loss: 0.0433016954562985\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.02817893827613066 | Validation loss: 0.032205368463809676\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.026418235477225983 | Validation loss: 0.031983047186468654\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.02586102904940424 | Validation loss: 0.03005178444660627\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.025684328844039307 | Validation loss: 0.0325129132789488\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.026494500268748413 | Validation loss: 0.035909466803647004\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.02768527107079502 | Validation loss: 0.03419312082517605\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.029022690255632283 | Validation loss: 0.0756713766604662\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.030787047692518003 | Validation loss: 0.05678711172479849\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.029405175485772643 | Validation loss: 0.03094115208547849\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.027223537728823872 | Validation loss: 0.03423833968834235\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.025910726122497788 | Validation loss: 0.03238570869255524\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.024620303329879696 | Validation loss: 0.032775767075900845\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.024254488279884644 | Validation loss: 0.02928792341397359\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.023843600908633668 | Validation loss: 0.03604337021421928\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.024692987382170642 | Validation loss: 0.03211906702759174\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.025721851939916128 | Validation loss: 0.0374490234714288\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.027135886828818543 | Validation loss: 0.03476525499270512\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.028328854775410674 | Validation loss: 0.06682556294477902\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.027366366840687842 | Validation loss: 0.03239961333859425\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.02528798375052479 | Validation loss: 0.0460323214244384\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.024234542412012214 | Validation loss: 0.03999455079722863\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.02321765907033373 | Validation loss: 0.03608835130356825\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.0221617315492408 | Validation loss: 0.028585847467184067\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.022177805008012273 | Validation loss: 0.030094460082741883\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.022704321559262178 | Validation loss: 0.03219181979791476\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.023915208310552456 | Validation loss: 0.031648835382209375\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.02473944200858896 | Validation loss: 0.03561788174108817\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.027072073639132957 | Validation loss: 0.03957040794193745\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.025478746504466302 | Validation loss: 0.04291144543542312\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.02397834242280075 | Validation loss: 0.03551239255242623\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.022762301095161842 | Validation loss: 0.035777714963142686\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.021810038727877835 | Validation loss: 0.03426963635362112\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.021307517216363658 | Validation loss: 0.028310040656763773\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.021035488740152677 | Validation loss: 0.04373047608309067\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.021291860884530583 | Validation loss: 0.044406829974972285\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.022908585479762027 | Validation loss: 0.034169698492265664\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.024446617925034362 | Validation loss: 0.034328691518077485\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.025942979482292888 | Validation loss: 0.060097817689753495\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.024310822792818793 | Validation loss: 0.03501157462596893\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.022634415940083714 | Validation loss: 0.03473313940832248\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.021089079691783377 | Validation loss: 0.03671561274677515\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.0206126121479852 | Validation loss: 0.038643558796208635\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.02040816003999669 | Validation loss: 0.027402662170621064\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.019678906183161957 | Validation loss: 0.034164513962773174\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.020458108885025205 | Validation loss: 0.043395852025311724\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.02149372065801792 | Validation loss: 0.04887792203002251\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.022725580859672925 | Validation loss: 0.03178696441822327\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.024382544900120994 | Validation loss: 0.04414457856462552\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.022855830495717072 | Validation loss: 0.041788846684189945\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.021017923792786445 | Validation loss: 0.039392892127999894\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.020251478479519063 | Validation loss: 0.035716857617864244\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.01932824108945696 | Validation loss: 0.034940155796133555\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.020082863217393155 | Validation loss: 0.026871812601502124\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.018461225810743536 | Validation loss: 0.03337421948806597\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.01906144128818261 | Validation loss: 0.044319855908934884\n",
      "Validation loss (ends of cycles): [0.14506537 0.03974307 0.03538791 0.03303492 0.03144806 0.03005178\n",
      " 0.02928792 0.02858585 0.02831004 0.02740266 0.02687181]\n",
      "------------------------------\n",
      "Epoch: 103\n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/losses')\n",
    "dest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/models')\n",
    "\n",
    "order = 1\n",
    "seeds = range(20)\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "learners.train((X, y, depth_order[:, -1]), \n",
    "               order=order,\n",
    "               dest_dir_loss=dest_dir_loss,\n",
    "               dest_dir_model=dest_dir_model,\n",
    "               n_epochs=n_epochs,\n",
    "               sc_kwargs=params_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-faf7c6e0-895b-4206-b5f8-7daf521d3601\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.103741</td>\n",
       "      <td>2.777945</td>\n",
       "      <td>0.772714</td>\n",
       "      <td>0.873048</td>\n",
       "      <td>0.454543</td>\n",
       "      <td>0.212442</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>26.963759</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.019571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.085642</td>\n",
       "      <td>0.136850</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.080627</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>1.352271</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.031140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.923080</td>\n",
       "      <td>2.550751</td>\n",
       "      <td>0.729316</td>\n",
       "      <td>0.846127</td>\n",
       "      <td>0.360252</td>\n",
       "      <td>0.129782</td>\n",
       "      <td>0.200901</td>\n",
       "      <td>25.281143</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>-0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.058124</td>\n",
       "      <td>2.688481</td>\n",
       "      <td>0.763672</td>\n",
       "      <td>0.865870</td>\n",
       "      <td>0.408399</td>\n",
       "      <td>0.166824</td>\n",
       "      <td>0.206547</td>\n",
       "      <td>25.622950</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>-0.009841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.111893</td>\n",
       "      <td>2.778513</td>\n",
       "      <td>0.775535</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.183224</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>27.126652</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.022086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.166518</td>\n",
       "      <td>2.872471</td>\n",
       "      <td>0.786733</td>\n",
       "      <td>0.881705</td>\n",
       "      <td>0.470035</td>\n",
       "      <td>0.220976</td>\n",
       "      <td>0.221762</td>\n",
       "      <td>27.655639</td>\n",
       "      <td>0.017485</td>\n",
       "      <td>0.041642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.256515</td>\n",
       "      <td>3.048478</td>\n",
       "      <td>0.803395</td>\n",
       "      <td>0.891445</td>\n",
       "      <td>0.664916</td>\n",
       "      <td>0.442113</td>\n",
       "      <td>0.240433</td>\n",
       "      <td>30.713740</td>\n",
       "      <td>0.039076</td>\n",
       "      <td>0.092671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faf7c6e0-895b-4206-b5f8-7daf521d3601')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-faf7c6e0-895b-4206-b5f8-7daf521d3601 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-faf7c6e0-895b-4206-b5f8-7daf521d3601');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "mean    2.103741   2.777945   0.772714   0.873048   0.454543   0.212442   \n",
       "std     0.085642   0.136850   0.019085   0.011321   0.078355   0.080627   \n",
       "min     1.923080   2.550751   0.729316   0.846127   0.360252   0.129782   \n",
       "25%     2.058124   2.688481   0.763672   0.865870   0.408399   0.166824   \n",
       "50%     2.111893   2.778513   0.775535   0.873508   0.428046   0.183224   \n",
       "75%     2.166518   2.872471   0.786733   0.881705   0.470035   0.220976   \n",
       "max     2.256515   3.048478   0.803395   0.891445   0.664916   0.442113   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  20.000000  20.000000  20.000000  20.000000  \n",
       "mean    0.214790  26.963759   0.008362   0.019571  \n",
       "std     0.010765   1.352271   0.013197   0.031140  \n",
       "min     0.200901  25.281143  -0.008894  -0.021487  \n",
       "25%     0.206547  25.622950  -0.004228  -0.009841  \n",
       "50%     0.212339  27.126652   0.009768   0.022086  \n",
       "75%     0.221762  27.655639   0.017485   0.041642  \n",
       "max     0.240433  30.713740   0.039076   0.092671  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/mollisols/models')\n",
    "seeds = range(20)\n",
    "order = 1\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_local_mollisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                   order = order,\n",
    "                                                   src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_local_mollisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test on Gelisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Seed: 0\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.44518339904871856 | Validation loss: 0.41080141067504883\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.4287309023466977 | Validation loss: 0.4031795859336853\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.4021740339019082 | Validation loss: 0.3900226950645447\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.37238084186207165 | Validation loss: 0.37106457352638245\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.3336567445234819 | Validation loss: 0.3466283082962036\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.28763867101886054 | Validation loss: 0.31739506125450134\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.2438681653954766 | Validation loss: 0.28248274326324463\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.21182731132615695 | Validation loss: 0.2361939251422882\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.19058756801215085 | Validation loss: 0.1982204169034958\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.17826715314930136 | Validation loss: 0.16747665405273438\n",
      "Validation loss (ends of cycles): [0.41080141]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.17355785789814862 | Validation loss: 0.1784353256225586\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.16932719945907593 | Validation loss: 0.15043631196022034\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.15943419459191235 | Validation loss: 0.12366648763418198\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.14414918016303668 | Validation loss: 0.14775261282920837\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.12919475544582715 | Validation loss: 0.10592546314001083\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.1137502295049754 | Validation loss: 0.09366549551486969\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.09954378415237773 | Validation loss: 0.13239187002182007\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.09346607327461243 | Validation loss: 0.09203055500984192\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.08725625784559683 | Validation loss: 0.08837558329105377\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.08394723622636362 | Validation loss: 0.08583536744117737\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08184474774382332 | Validation loss: 0.08680430799722672\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.07987467301162807 | Validation loss: 0.09593759477138519\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.07881784202022986 | Validation loss: 0.15475192666053772\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.08277476172555577 | Validation loss: 0.11691433191299438\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.07523983920162375 | Validation loss: 0.09792322665452957\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.08776683563535864 | Validation loss: 0.14275996387004852\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.08074803311716426 | Validation loss: 0.09750550240278244\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07898637584664604 | Validation loss: 0.08195220679044724\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.07386642017147758 | Validation loss: 0.08875860273838043\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06991859241812066 | Validation loss: 0.08599168062210083\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.07041569883850488 | Validation loss: 0.08440250158309937\n",
      "Validation loss (ends of cycles): [0.41080141 0.17843533 0.08680431 0.0844025 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 1\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2712295380505649 | Validation loss: 0.29694056510925293\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.2557703337886117 | Validation loss: 0.2916700839996338\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.23915838382460855 | Validation loss: 0.2821319103240967\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.21806168962608685 | Validation loss: 0.27006006240844727\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.19106543605977838 | Validation loss: 0.2555059790611267\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.1593422293663025 | Validation loss: 0.23977532982826233\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.1341805329377001 | Validation loss: 0.21363312005996704\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.11576782234690407 | Validation loss: 0.17318867146968842\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.10450286960059946 | Validation loss: 0.14469444751739502\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.0977388471364975 | Validation loss: 0.1382673680782318\n",
      "Validation loss (ends of cycles): [0.29694057]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0953068191354925 | Validation loss: 0.1350218504667282\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.09208406778899106 | Validation loss: 0.13349907100200653\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.08720466291362589 | Validation loss: 0.1306271106004715\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08318295160477812 | Validation loss: 0.12258781492710114\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.0795554786243222 | Validation loss: 0.1179313138127327\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0752499575980685 | Validation loss: 0.21455103158950806\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07196846248751337 | Validation loss: 0.10781153291463852\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07144239443269643 | Validation loss: 0.11611378192901611\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.061659027060324494 | Validation loss: 0.09441842883825302\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.0629522180692716 | Validation loss: 0.09655869007110596\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.056854997338219124 | Validation loss: 0.09356872737407684\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.05831065249036659 | Validation loss: 0.10779251158237457\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.05809896981174296 | Validation loss: 0.08884022384881973\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.05916342342441732 | Validation loss: 0.11716677248477936\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.060772972012108024 | Validation loss: 0.09863302856683731\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06027982553297823 | Validation loss: 0.10262470692396164\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06343780009245331 | Validation loss: 0.10198401659727097\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.057109395719387314 | Validation loss: 0.08557749539613724\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05680512789298187 | Validation loss: 0.08958760648965836\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05255689133297314 | Validation loss: 0.08726699650287628\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05449707111851736 | Validation loss: 0.0852150097489357\n",
      "Validation loss (ends of cycles): [0.29694057 0.13502185 0.09356873 0.08521501]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 2\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.3403178215026855 | Validation loss: 0.22224114835262299\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.3237610936164856 | Validation loss: 0.2175069898366928\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.3036596342921257 | Validation loss: 0.2090708464384079\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.278908571600914 | Validation loss: 0.1986195296049118\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.24769878536462783 | Validation loss: 0.19036133587360382\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.2119302600622177 | Validation loss: 0.1850602626800537\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.17966288328170776 | Validation loss: 0.17428995668888092\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.1580403283238411 | Validation loss: 0.14990781247615814\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.1425566166639328 | Validation loss: 0.12038183957338333\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.1347818359732628 | Validation loss: 0.10062086582183838\n",
      "Validation loss (ends of cycles): [0.22224115]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.13022587075829506 | Validation loss: 0.09236151725053787\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.128421900421381 | Validation loss: 0.08484034240245819\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.12211894914507866 | Validation loss: 0.0809514969587326\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.11671075597405434 | Validation loss: 0.09415645897388458\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.1062808632850647 | Validation loss: 0.0794791579246521\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.09774378426373005 | Validation loss: 0.07385452091693878\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.09059763960540294 | Validation loss: 0.06823520362377167\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.09244940280914307 | Validation loss: 0.10553069412708282\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0844466209411621 | Validation loss: 0.06962426751852036\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.08120766542851925 | Validation loss: 0.06540507078170776\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08298086747527122 | Validation loss: 0.06289994716644287\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.07931055799126625 | Validation loss: 0.06145286187529564\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.07616247236728668 | Validation loss: 0.05724290385842323\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.07831368632614613 | Validation loss: 0.061430856585502625\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.08261116072535515 | Validation loss: 0.1134483739733696\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.07242584079504014 | Validation loss: 0.07263419032096863\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.08369440548121929 | Validation loss: 0.05704887583851814\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07497833706438542 | Validation loss: 0.05711729824542999\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06945027969777584 | Validation loss: 0.05534932762384415\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06933295093476773 | Validation loss: 0.054547473788261414\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.06596663519740105 | Validation loss: 0.05319888889789581\n",
      "Validation loss (ends of cycles): [0.22224115 0.09236152 0.06289995 0.05319889]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 3\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.3599785728888078 | Validation loss: 0.4432780146598816\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.3369556678967042 | Validation loss: 0.43250834941864014\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.30670858242295007 | Validation loss: 0.4114508032798767\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.272465627301823 | Validation loss: 0.3817843198776245\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.23151018267328088 | Validation loss: 0.3536441922187805\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.18663159283724698 | Validation loss: 0.32844647765159607\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.15000829168341376 | Validation loss: 0.28304171562194824\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.12784207612276077 | Validation loss: 0.21613815426826477\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.11503372409126976 | Validation loss: 0.16611164808273315\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10725069452415813 | Validation loss: 0.15188439190387726\n",
      "Validation loss (ends of cycles): [0.44327801]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10527931221506813 | Validation loss: 0.13959506154060364\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.1036420301957564 | Validation loss: 0.1270444095134735\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.09993583234873685 | Validation loss: 0.1236078217625618\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.09465839510614221 | Validation loss: 0.13082820177078247\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08756625313650478 | Validation loss: 0.10408846288919449\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0838624509898099 | Validation loss: 0.08756735175848007\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.08244303072040732 | Validation loss: 0.09559406340122223\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0741471800614487 | Validation loss: 0.10337742418050766\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07662723992358554 | Validation loss: 0.06411650031805038\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06983052329583601 | Validation loss: 0.07078924030065536\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.0687717121433128 | Validation loss: 0.06904434412717819\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06762443178079346 | Validation loss: 0.06313017755746841\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.0678871947933327 | Validation loss: 0.067040354013443\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06613235378807242 | Validation loss: 0.07850442826747894\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06488648564978079 | Validation loss: 0.051457930356264114\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0654671046544205 | Validation loss: 0.061340026557445526\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06411620466546579 | Validation loss: 0.06412345170974731\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.060056577013297516 | Validation loss: 0.05233113467693329\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05766375464471904 | Validation loss: 0.04654216393828392\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05332683399319649 | Validation loss: 0.04950576648116112\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05111788822845979 | Validation loss: 0.05081142485141754\n",
      "Validation loss (ends of cycles): [0.44327801 0.13959506 0.06904434 0.05081142]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 4\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.5454315461895682 | Validation loss: 0.4730074107646942\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.5257494151592255 | Validation loss: 0.4644114375114441\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.49964225021275604 | Validation loss: 0.4493078589439392\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.4701995172283866 | Validation loss: 0.4282127916812897\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.43141826445406134 | Validation loss: 0.4020078182220459\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.3827357996593822 | Validation loss: 0.362201452255249\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.3341675915501334 | Validation loss: 0.3249988853931427\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.2958235442638397 | Validation loss: 0.2761434316635132\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.2697145681489598 | Validation loss: 0.25178632140159607\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.25373841009356757 | Validation loss: 0.22395309805870056\n",
      "Validation loss (ends of cycles): [0.47300741]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.24752120673656464 | Validation loss: 0.22707553207874298\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.24136914312839508 | Validation loss: 0.2116468846797943\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.2281808080998334 | Validation loss: 0.2141018956899643\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.20625835250724445 | Validation loss: 0.2063857465982437\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.1818149442022497 | Validation loss: 0.1678750216960907\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.1542614088817076 | Validation loss: 0.24311645328998566\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.13340282982045953 | Validation loss: 0.11346697062253952\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.11361510171131654 | Validation loss: 0.08446861058473587\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.10257813706994057 | Validation loss: 0.1038476824760437\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.09851626374504784 | Validation loss: 0.10009600967168808\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.09389783983880823 | Validation loss: 0.10324864834547043\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.09191824529658664 | Validation loss: 0.08838817477226257\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.08743048831820488 | Validation loss: 0.11275480687618256\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.08301741765304045 | Validation loss: 0.08835740387439728\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.07729637080972845 | Validation loss: 0.15197023749351501\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.07531545412811366 | Validation loss: 0.19605480134487152\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.0744694381613623 | Validation loss: 0.08726125210523605\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07172296805815263 | Validation loss: 0.08883035182952881\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05978721922094172 | Validation loss: 0.10437580943107605\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.058711225844242355 | Validation loss: 0.07623975723981857\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05624144473536448 | Validation loss: 0.0785088762640953\n",
      "Validation loss (ends of cycles): [0.47300741 0.22707553 0.10324865 0.07850888]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 5\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.3115223288536072 | Validation loss: 0.35713261365890503\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.29661754220724107 | Validation loss: 0.3485311195254326\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.27701484113931657 | Validation loss: 0.3320363536477089\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.2542939618229866 | Validation loss: 0.3089185282588005\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.2231126084923744 | Validation loss: 0.28109800815582275\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.18675014078617097 | Validation loss: 0.2502268999814987\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.15313852950930595 | Validation loss: 0.2070472240447998\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.13002245053648948 | Validation loss: 0.19130465388298035\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.11442725360393524 | Validation loss: 0.20052699744701385\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10721210837364196 | Validation loss: 0.21451064944267273\n",
      "Validation loss (ends of cycles): [0.35713261]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10336671397089958 | Validation loss: 0.21087735146284103\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.0994449395686388 | Validation loss: 0.20340285822749138\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.09221443757414818 | Validation loss: 0.22622137889266014\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08567270934581757 | Validation loss: 0.2773045003414154\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.07911113798618316 | Validation loss: 0.43483686447143555\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.07443125247955322 | Validation loss: 0.1533229500055313\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.06726926937699318 | Validation loss: 0.1722557358443737\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.06305484026670456 | Validation loss: 0.21744874492287636\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.057345937564969064 | Validation loss: 0.2609431743621826\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.05563645977526903 | Validation loss: 0.2218475341796875\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.052960326336324214 | Validation loss: 0.19616811349987984\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.05901392139494419 | Validation loss: 0.1624758467078209\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.050959855690598486 | Validation loss: 0.22551586478948593\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.051076297834515574 | Validation loss: 0.2302572764456272\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05112339500337839 | Validation loss: 0.21048244833946228\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05467013940215111 | Validation loss: 0.12336333841085434\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.057834042236208916 | Validation loss: 0.2566063143312931\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04988208692520857 | Validation loss: 0.22633014991879463\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05284797567874193 | Validation loss: 0.21418332308530807\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.045893807895481586 | Validation loss: 0.1591845452785492\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.0433046093210578 | Validation loss: 0.14972137287259102\n",
      "Validation loss (ends of cycles): [0.35713261 0.21087735 0.19616811 0.14972137]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 6\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.5947945535182952 | Validation loss: 0.5636097490787506\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.5739877611398697 | Validation loss: 0.5552766025066376\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.5444172233343124 | Validation loss: 0.5409025847911835\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.5102303147315979 | Validation loss: 0.5223296880722046\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.4671860933303833 | Validation loss: 0.5054818987846375\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.4151140213012695 | Validation loss: 0.49210914969444275\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.3648500770330429 | Validation loss: 0.4792882353067398\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.3277033656835556 | Validation loss: 0.4582698494195938\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.30160123109817505 | Validation loss: 0.40672582387924194\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.28555808067321775 | Validation loss: 0.3488713800907135\n",
      "Validation loss (ends of cycles): [0.56360975]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.2787675619125366 | Validation loss: 0.31419089436531067\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.27361820340156556 | Validation loss: 0.2943081259727478\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.26058380156755445 | Validation loss: 0.26867610216140747\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.24124586433172227 | Validation loss: 0.24037369340658188\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.21548341661691667 | Validation loss: 0.22535283118486404\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.18438775539398194 | Validation loss: 0.1685718446969986\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.16061918511986734 | Validation loss: 0.13537318259477615\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.14224363416433333 | Validation loss: 0.13322928547859192\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.13033997938036918 | Validation loss: 0.12760929018259048\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.12404320240020753 | Validation loss: 0.13474282249808311\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.11867297813296318 | Validation loss: 0.13434157520532608\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.11793936267495156 | Validation loss: 0.13654564321041107\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.11286920569837093 | Validation loss: 0.12956713140010834\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.10548075810074806 | Validation loss: 0.12283502891659737\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.09794336780905724 | Validation loss: 0.10408468917012215\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0924295324832201 | Validation loss: 0.15008461475372314\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.09017585180699825 | Validation loss: 0.07894822582602501\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.08053090944886207 | Validation loss: 0.07444114610552788\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.07959246821701527 | Validation loss: 0.09047586098313332\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.0734288364648819 | Validation loss: 0.07841610908508301\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.07281158864498138 | Validation loss: 0.07585301622748375\n",
      "Validation loss (ends of cycles): [0.56360975 0.31419089 0.13434158 0.07585302]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 7\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.24966259002685548 | Validation loss: 0.24267390370368958\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.235618394613266 | Validation loss: 0.2372320294380188\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.21840053498744966 | Validation loss: 0.22739332169294357\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.19703763723373413 | Validation loss: 0.21525921672582626\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.1717545986175537 | Validation loss: 0.20377518981695175\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.14265027046203613 | Validation loss: 0.19772907346487045\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.11750846356153488 | Validation loss: 0.1927606761455536\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.10381745249032974 | Validation loss: 0.18614127486944199\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.0941122256219387 | Validation loss: 0.16012199968099594\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.09131616652011872 | Validation loss: 0.15815239399671555\n",
      "Validation loss (ends of cycles): [0.2426739]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.08770758658647537 | Validation loss: 0.15310294181108475\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.08528880327939987 | Validation loss: 0.14396170154213905\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.08279691264033318 | Validation loss: 0.14130394160747528\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08135882690548897 | Validation loss: 0.14968570321798325\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08186716139316559 | Validation loss: 0.17318671941757202\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08467233590781689 | Validation loss: 0.18237532302737236\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0764297217130661 | Validation loss: 0.18119005113840103\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07494740672409535 | Validation loss: 0.1301913782954216\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07010909467935562 | Validation loss: 0.14423485472798347\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.0687748871743679 | Validation loss: 0.15484707802534103\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.0645689457654953 | Validation loss: 0.15342670306563377\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.062006055191159246 | Validation loss: 0.15138789266347885\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06060033775866032 | Validation loss: 0.13369300588965416\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.061561013013124465 | Validation loss: 0.13530557602643967\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06617009490728379 | Validation loss: 0.16354938223958015\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06215674802660942 | Validation loss: 0.20111528038978577\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.059966203197836875 | Validation loss: 0.14292334020137787\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.059527568891644476 | Validation loss: 0.1250341236591339\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05725446715950966 | Validation loss: 0.12287440150976181\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.054586905613541604 | Validation loss: 0.1597551889717579\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05266690887510776 | Validation loss: 0.1473530475050211\n",
      "Validation loss (ends of cycles): [0.2426739  0.15310294 0.1534267  0.14735305]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 8\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.41844989494843915 | Validation loss: 0.46415480971336365\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.39472986351359973 | Validation loss: 0.4504581391811371\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.35734637623483484 | Validation loss: 0.42483583092689514\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.31263969296758826 | Validation loss: 0.3877894878387451\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.2572383663871072 | Validation loss: 0.35185202956199646\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.19959831779653375 | Validation loss: 0.32558348774909973\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.15444783595475284 | Validation loss: 0.30798062682151794\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.12643937360156665 | Validation loss: 0.2347472906112671\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.1116909838535569 | Validation loss: 0.19630743563175201\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10401421514424411 | Validation loss: 0.1785627156496048\n",
      "Validation loss (ends of cycles): [0.46415481]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10056576505303383 | Validation loss: 0.16850918531417847\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.0980552001432939 | Validation loss: 0.16101451218128204\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.09317369014024734 | Validation loss: 0.15137232840061188\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08474765955047174 | Validation loss: 0.19075323641300201\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08182131465185773 | Validation loss: 0.12207912653684616\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08076543767343868 | Validation loss: 0.16828617453575134\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07709567960013043 | Validation loss: 0.13723145425319672\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07455341958186844 | Validation loss: 0.17370207607746124\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.06813155622644858 | Validation loss: 0.1326078176498413\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06662491912191565 | Validation loss: 0.1284124106168747\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06775809993798082 | Validation loss: 0.12708702683448792\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06268392300063913 | Validation loss: 0.1257689744234085\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06307559223337607 | Validation loss: 0.1285398006439209\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06152626160870899 | Validation loss: 0.10883551836013794\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06122888353737918 | Validation loss: 0.1875065714120865\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06612172418019989 | Validation loss: 0.20511069893836975\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06252996826713736 | Validation loss: 0.1681954264640808\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06012651967731389 | Validation loss: 0.12634092569351196\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.052851350639354096 | Validation loss: 0.1592557728290558\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05158912593668157 | Validation loss: 0.12818372249603271\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.052270400591871956 | Validation loss: 0.12273656576871872\n",
      "Validation loss (ends of cycles): [0.46415481 0.16850919 0.12708703 0.12273657]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 9\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.33450397036292334 | Validation loss: 0.4183863401412964\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.31508046659556305 | Validation loss: 0.4055172950029373\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.28755233775485645 | Validation loss: 0.38339458405971527\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.2563350403850729 | Validation loss: 0.35465146601200104\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.21771724657578903 | Validation loss: 0.32462094724178314\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.17677603594281457 | Validation loss: 0.2886727899312973\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.14331864734942262 | Validation loss: 0.25660841912031174\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.12253618409687822 | Validation loss: 0.20420953631401062\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.10970718989318068 | Validation loss: 0.16134043782949448\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10511686839163303 | Validation loss: 0.136260487139225\n",
      "Validation loss (ends of cycles): [0.41838634]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10223364101892168 | Validation loss: 0.12221920490264893\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.09944640675728972 | Validation loss: 0.10605774819850922\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.09476278620687398 | Validation loss: 0.09146244078874588\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.09061784432692961 | Validation loss: 0.07169642113149166\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08670035394077952 | Validation loss: 0.07171276770532131\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08408490454100749 | Validation loss: 0.07400976028293371\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07897272638299248 | Validation loss: 0.07414662837982178\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07920655997639353 | Validation loss: 0.060708372853696346\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0753878229721026 | Validation loss: 0.07837508991360664\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.07477120868861675 | Validation loss: 0.08723119460046291\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.07195909118110483 | Validation loss: 0.07401845417916775\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06806200789287686 | Validation loss: 0.064169991761446\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.07544434239918535 | Validation loss: 0.05839283112436533\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06904576837339184 | Validation loss: 0.052823279052972794\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.07263032600960949 | Validation loss: 0.05023655481636524\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06975930573588068 | Validation loss: 0.0573732815682888\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.07377178052609618 | Validation loss: 0.07613628916442394\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06863558673384515 | Validation loss: 0.06069220509380102\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.0684154127639803 | Validation loss: 0.07957470044493675\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06068193370645696 | Validation loss: 0.05438768491148949\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.056117918638681825 | Validation loss: 0.0544932559132576\n",
      "Validation loss (ends of cycles): [0.41838634 0.1222192  0.07401845 0.05449326]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2340872816064141 | Validation loss: 0.27015864849090576\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.21558139405467294 | Validation loss: 0.26325294375419617\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.19359120591120285 | Validation loss: 0.2500206530094147\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.17328757318583402 | Validation loss: 0.23665626347064972\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.1474518593062054 | Validation loss: 0.23030684888362885\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.12017566642977974 | Validation loss: 0.21531414985656738\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.10158516398885033 | Validation loss: 0.17602409422397614\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.0888047841462222 | Validation loss: 0.15436150133609772\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.08242148025469347 | Validation loss: 0.09863794595003128\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.07809551805257797 | Validation loss: 0.09693825244903564\n",
      "Validation loss (ends of cycles): [0.27015865]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0748989030041478 | Validation loss: 0.09435625374317169\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.07554080574349924 | Validation loss: 0.12675826251506805\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.07253992252729156 | Validation loss: 0.0890277624130249\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.06745774806900458 | Validation loss: 0.09933421015739441\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.06639396636323495 | Validation loss: 0.25477135181427\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.07153629342263396 | Validation loss: 0.12218687683343887\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.06362383981997316 | Validation loss: 0.09838857501745224\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.06499294090000066 | Validation loss: 0.12082856893539429\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.057603581385179 | Validation loss: 0.08688703179359436\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.054477095265280114 | Validation loss: 0.07416538149118423\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.053827235983176666 | Validation loss: 0.07726097851991653\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.05040800148113207 | Validation loss: 0.12319447845220566\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04939531450244514 | Validation loss: 0.07437314093112946\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04975901036099954 | Validation loss: 0.2664039731025696\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05050407112999396 | Validation loss: 0.07585630565881729\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.052467319098385895 | Validation loss: 0.07166603952646255\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04566661116074432 | Validation loss: 0.11543244868516922\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.048686319454149765 | Validation loss: 0.29796749353408813\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.04886364056305452 | Validation loss: 0.18778347969055176\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.04836133596572009 | Validation loss: 0.06662750244140625\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.042628409510309044 | Validation loss: 0.06421707570552826\n",
      "Validation loss (ends of cycles): [0.27015865 0.09435625 0.07726098 0.06421708]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 11\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.22647233442826706 | Validation loss: 0.23534537851810455\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.2113880677656694 | Validation loss: 0.23379206657409668\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.19402675059708682 | Validation loss: 0.23131398856639862\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.1751593459736217 | Validation loss: 0.22928957641124725\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.15052058547735214 | Validation loss: 0.22850777208805084\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.12454998357729478 | Validation loss: 0.22395320236682892\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.10516942563382062 | Validation loss: 0.19217549264431\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.09278485009616072 | Validation loss: 0.15953890979290009\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.08566174080426042 | Validation loss: 0.12811169028282166\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.08283901891925118 | Validation loss: 0.12298424541950226\n",
      "Validation loss (ends of cycles): [0.23534538]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.08166876401413571 | Validation loss: 0.12157569825649261\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.07987238669937308 | Validation loss: 0.12463139742612839\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.07590313425118272 | Validation loss: 0.11578209698200226\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.0747046714479273 | Validation loss: 0.124772809445858\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.07252140851183371 | Validation loss: 0.11757127940654755\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.07997047291560606 | Validation loss: 0.11393731087446213\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07546812854707241 | Validation loss: 0.1232791543006897\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07227142189036716 | Validation loss: 0.11599378287792206\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.06743957068432462 | Validation loss: 0.10658861696720123\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06617797978899696 | Validation loss: 0.10020831972360611\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06662786921316927 | Validation loss: 0.09910339117050171\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.0641032149168578 | Validation loss: 0.09659582376480103\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06450243294239044 | Validation loss: 0.09174611419439316\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06076653200117024 | Validation loss: 0.0868479311466217\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.057067283344539727 | Validation loss: 0.1249217838048935\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06456780433654785 | Validation loss: 0.10062471777200699\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.060377268797971985 | Validation loss: 0.07824037969112396\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.05700255642560395 | Validation loss: 0.08699680119752884\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.053185255182060326 | Validation loss: 0.07531373202800751\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05329576574943282 | Validation loss: 0.07017888128757477\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.051533097231929954 | Validation loss: 0.06938274204730988\n",
      "Validation loss (ends of cycles): [0.23534538 0.1215757  0.09910339 0.06938274]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 12\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.31496341824531554 | Validation loss: 0.3085462301969528\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.296749347448349 | Validation loss: 0.3101869523525238\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.2731067031621933 | Validation loss: 0.31405508518218994\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.24493784904479982 | Validation loss: 0.3216366618871689\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.21210817396640777 | Validation loss: 0.33603934943675995\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.17602172642946243 | Validation loss: 0.3545747697353363\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.14613406211137772 | Validation loss: 0.35567839443683624\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.1257573790848255 | Validation loss: 0.3237410634756088\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.11371021643280983 | Validation loss: 0.26202917098999023\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10723200663924218 | Validation loss: 0.22564251720905304\n",
      "Validation loss (ends of cycles): [0.30854623]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10520545840263366 | Validation loss: 0.20360779762268066\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.10288014262914658 | Validation loss: 0.20160969346761703\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.09802041873335839 | Validation loss: 0.20292669162154198\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.09413954019546508 | Validation loss: 0.17354870960116386\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08832933753728867 | Validation loss: 0.22394151240587234\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08517145812511444 | Validation loss: 0.16865815967321396\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.08164194636046887 | Validation loss: 0.16437028348445892\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.08001556545495987 | Validation loss: 0.20558318868279457\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0778819728642702 | Validation loss: 0.183094821870327\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.07783219516277314 | Validation loss: 0.1785370595753193\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.07695055566728115 | Validation loss: 0.17569859325885773\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.07580824568867683 | Validation loss: 0.20672567933797836\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.07075491286814213 | Validation loss: 0.17038242146372795\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.07162440195679665 | Validation loss: 0.17492885142564774\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06862989962100982 | Validation loss: 0.1177842915058136\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06372749842703343 | Validation loss: 0.09121009334921837\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06439065597951413 | Validation loss: 0.11329861357808113\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.05956905409693718 | Validation loss: 0.1521657519042492\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.054637243039906025 | Validation loss: 0.065576933324337\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06076151393353939 | Validation loss: 0.058655871078372\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05368628203868866 | Validation loss: 0.06157389655709267\n",
      "Validation loss (ends of cycles): [0.30854623 0.2036078  0.17569859 0.0615739 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 13\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.3849785625934601 | Validation loss: 0.45114198327064514\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.3704269663854079 | Validation loss: 0.44007259607315063\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.3476231748407537 | Validation loss: 0.4194194972515106\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.31672785905274475 | Validation loss: 0.39160406589508057\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.27650914002548566 | Validation loss: 0.35673192143440247\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.2297362427819859 | Validation loss: 0.31353193521499634\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.18639830432154916 | Validation loss: 0.27488814294338226\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.15894355489449066 | Validation loss: 0.2300477847456932\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.1421263353391127 | Validation loss: 0.18800196796655655\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.13018270107832822 | Validation loss: 0.1685379222035408\n",
      "Validation loss (ends of cycles): [0.45114198]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.1273436485366388 | Validation loss: 0.15773064270615578\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.12412191317840056 | Validation loss: 0.15657923743128777\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.11569374122402885 | Validation loss: 0.14440815895795822\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.10674315149133856 | Validation loss: 0.08752488531172276\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.09485080702738329 | Validation loss: 0.08425725065171719\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08512867479161783 | Validation loss: 0.06908361706882715\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07970026880502701 | Validation loss: 0.12750844284892082\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07524278997020288 | Validation loss: 0.07422960735857487\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07073394175280225 | Validation loss: 0.06022882554680109\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06288122284141454 | Validation loss: 0.06339698284864426\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.061651048165830696 | Validation loss: 0.06467864662408829\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06111889738928188 | Validation loss: 0.06646665744483471\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.05873148346489126 | Validation loss: 0.06867950409650803\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.060050982643257485 | Validation loss: 0.061418455094099045\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.060610766437920655 | Validation loss: 0.05618366505950689\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0633573637089946 | Validation loss: 0.04861140996217728\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06116855178367008 | Validation loss: 0.07926485501229763\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.05911888656291095 | Validation loss: 0.058403012342751026\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.05666702342304317 | Validation loss: 0.042423720471560955\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05089438266374848 | Validation loss: 0.03783706063404679\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05125425891442732 | Validation loss: 0.04267655219882727\n",
      "Validation loss (ends of cycles): [0.45114198 0.15773064 0.06467865 0.04267655]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 14\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.41509213149547575 | Validation loss: 0.38505399227142334\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.39179774820804597 | Validation loss: 0.37304168939590454\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.35843105912208556 | Validation loss: 0.35156017541885376\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.31765433847904206 | Validation loss: 0.32422196865081787\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.2673868998885155 | Validation loss: 0.2933422923088074\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.21240467727184295 | Validation loss: 0.26028379797935486\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.1678038567304611 | Validation loss: 0.2316894382238388\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.1405745640397072 | Validation loss: 0.1750834584236145\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.12251835465431213 | Validation loss: 0.1384737491607666\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.11450750157237052 | Validation loss: 0.10572589933872223\n",
      "Validation loss (ends of cycles): [0.38505399]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10943077206611633 | Validation loss: 0.08536231517791748\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.10874983444809913 | Validation loss: 0.06326284259557724\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.10184967592358589 | Validation loss: 0.05090981721878052\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.09422398544847965 | Validation loss: 0.05360788851976395\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.09096146337687969 | Validation loss: 0.1130673959851265\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0848462775349617 | Validation loss: 0.1005302146077156\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.08346045427024365 | Validation loss: 0.04016828536987305\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.08220213800668716 | Validation loss: 0.05280933529138565\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07907124496996402 | Validation loss: 0.04327217862010002\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.07301834337413311 | Validation loss: 0.041828058660030365\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.0685610394924879 | Validation loss: 0.04052760452032089\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06799696236848832 | Validation loss: 0.036265864968299866\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06856372691690922 | Validation loss: 0.03583543002605438\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06802004836499691 | Validation loss: 0.03715496510267258\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06912181153893471 | Validation loss: 0.04452311620116234\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0747891653329134 | Validation loss: 0.12103398144245148\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06961565390229225 | Validation loss: 0.17962095141410828\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06380176991224289 | Validation loss: 0.07026810944080353\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06431049816310405 | Validation loss: 0.0371607169508934\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06408782787621022 | Validation loss: 0.04384800046682358\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05868135690689087 | Validation loss: 0.03950566053390503\n",
      "Validation loss (ends of cycles): [0.38505399 0.08536232 0.0405276  0.03950566]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 15\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.497423380613327 | Validation loss: 0.5236005187034607\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.47451717203313654 | Validation loss: 0.5123611688613892\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.4432302469556982 | Validation loss: 0.4904400110244751\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.4015417017719962 | Validation loss: 0.4578700065612793\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.34881053458560596 | Validation loss: 0.41512584686279297\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.2884210998361761 | Validation loss: 0.37235116958618164\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.23100726983763956 | Validation loss: 0.3235102891921997\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.19382140717723154 | Validation loss: 0.24588394165039062\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.16921459138393402 | Validation loss: 0.187604621052742\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.15417007424614645 | Validation loss: 0.20316246151924133\n",
      "Validation loss (ends of cycles): [0.52360052]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.14732958579605276 | Validation loss: 0.17677535116672516\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.1463459621776234 | Validation loss: 0.14159077405929565\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.13318475064906207 | Validation loss: 0.181656152009964\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.12301303310827776 | Validation loss: 0.10158068686723709\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.10729351978410374 | Validation loss: 0.09514954686164856\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.09595484286546707 | Validation loss: 0.08995315432548523\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.08417292159389365 | Validation loss: 0.0888112485408783\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.08165155740624125 | Validation loss: 0.0818200558423996\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0723276978189295 | Validation loss: 0.08610425889492035\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.07191044092178345 | Validation loss: 0.08774658292531967\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06833597780628638 | Validation loss: 0.08297054469585419\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06698174876245586 | Validation loss: 0.07742486894130707\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06470589475198225 | Validation loss: 0.07569701969623566\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06563659706576304 | Validation loss: 0.090906523168087\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06569409421221777 | Validation loss: 0.08477133512496948\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06704503907398744 | Validation loss: 0.09006834775209427\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.07161630249836227 | Validation loss: 0.07447994500398636\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06389857828617096 | Validation loss: 0.0818374902009964\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.060700812631032684 | Validation loss: 0.06722551584243774\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05577064026147127 | Validation loss: 0.07123492658138275\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05637003786184571 | Validation loss: 0.0676247850060463\n",
      "Validation loss (ends of cycles): [0.52360052 0.17677535 0.08297054 0.06762479]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 16\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.47987140308726917 | Validation loss: 0.4334615617990494\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.45977337793870404 | Validation loss: 0.42151939868927\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.42812349579551 | Validation loss: 0.39918772876262665\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.3865386030890725 | Validation loss: 0.3691086918115616\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.33454837040467694 | Validation loss: 0.3325386643409729\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.27320032634518365 | Validation loss: 0.29823416471481323\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.22226029634475708 | Validation loss: 0.2558329254388809\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.18792442774230783 | Validation loss: 0.22173385322093964\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.16651706397533417 | Validation loss: 0.18216048181056976\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.1563743305477229 | Validation loss: 0.15323598682880402\n",
      "Validation loss (ends of cycles): [0.43346156]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.15220273286104202 | Validation loss: 0.14952246099710464\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.14907611906528473 | Validation loss: 0.15525143593549728\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.14079013602300125 | Validation loss: 0.12134148925542831\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.12847822565924039 | Validation loss: 0.15095221251249313\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.11500959301536734 | Validation loss: 0.09931726008653641\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.10405388474464417 | Validation loss: 0.12409628182649612\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.10094385120001706 | Validation loss: 0.10372162610292435\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.09531495787880638 | Validation loss: 0.09211777150630951\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.08857555281032216 | Validation loss: 0.09498467668890953\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.0935635573484681 | Validation loss: 0.09667631611227989\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08716208182952621 | Validation loss: 0.09361663460731506\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.086543853987347 | Validation loss: 0.0879531130194664\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.08200569958849387 | Validation loss: 0.08755558729171753\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.085517230020328 | Validation loss: 0.08203435316681862\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.0809919996695085 | Validation loss: 0.07936260104179382\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.08055772056633775 | Validation loss: 0.08800634741783142\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.07741718976335092 | Validation loss: 0.08297253772616386\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07764130017974159 | Validation loss: 0.07773812860250473\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.07542369074442169 | Validation loss: 0.0761253871023655\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.07147530669515784 | Validation loss: 0.07653047516942024\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.07272762432694435 | Validation loss: 0.07702647522091866\n",
      "Validation loss (ends of cycles): [0.43346156 0.14952246 0.09361663 0.07702648]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 17\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.5610803931951522 | Validation loss: 0.5742125511169434\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.5384890228509903 | Validation loss: 0.5626022219657898\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.5053218245506287 | Validation loss: 0.5410920977592468\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.4658749431371689 | Validation loss: 0.5129314661026001\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.4172980934381485 | Validation loss: 0.4840275049209595\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.3584092140197754 | Validation loss: 0.45695602893829346\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.3014706075191498 | Validation loss: 0.43395182490348816\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.26016793251037595 | Validation loss: 0.3845180869102478\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.23394290506839752 | Validation loss: 0.3097478747367859\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.21567025035619736 | Validation loss: 0.23616959154605865\n",
      "Validation loss (ends of cycles): [0.57421255]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.20982832312583924 | Validation loss: 0.19686532020568848\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.20462629497051238 | Validation loss: 0.1726258248090744\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.19207933992147447 | Validation loss: 0.17744328081607819\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.17268036156892777 | Validation loss: 0.12105994671583176\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.14910297393798827 | Validation loss: 0.07750903069972992\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.12406309843063354 | Validation loss: 0.06648653000593185\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.1091826967895031 | Validation loss: 0.051474422216415405\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.09453097954392434 | Validation loss: 0.09065815806388855\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.08696112185716628 | Validation loss: 0.05250278487801552\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.08315690755844116 | Validation loss: 0.047353047877550125\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08215439356863499 | Validation loss: 0.04667212441563606\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.08283475637435914 | Validation loss: 0.04465808719396591\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.07634258382022381 | Validation loss: 0.05145259574055672\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.07885098718106746 | Validation loss: 0.055073946714401245\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.07556320875883102 | Validation loss: 0.0855465829372406\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.07224686443805695 | Validation loss: 0.0590750053524971\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.07677309662103653 | Validation loss: 0.04134538397192955\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06744715496897698 | Validation loss: 0.03748737648129463\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06501720324158669 | Validation loss: 0.03147929161787033\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06560679152607918 | Validation loss: 0.031396884471178055\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.06533972807228565 | Validation loss: 0.027407711371779442\n",
      "Validation loss (ends of cycles): [0.57421255 0.19686532 0.04667212 0.02740771]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 18\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.5691605454141443 | Validation loss: 0.608923077583313\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.5476431304758246 | Validation loss: 0.6018098592758179\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.5234504071148959 | Validation loss: 0.589514970779419\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.49886292490092193 | Validation loss: 0.5730634927749634\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.46711285276846454 | Validation loss: 0.5529573559761047\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.42809410799633374 | Validation loss: 0.5257182121276855\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.387255614454096 | Validation loss: 0.49099910259246826\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.35546800223263825 | Validation loss: 0.43925637006759644\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.3322257250547409 | Validation loss: 0.4052078127861023\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.3180681358684193 | Validation loss: 0.38430100679397583\n",
      "Validation loss (ends of cycles): [0.60892308]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.31182124533436517 | Validation loss: 0.37427014112472534\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.30706333301284094 | Validation loss: 0.35669952630996704\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.29335934736511926 | Validation loss: 0.3390655219554901\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.27410248057408765 | Validation loss: 0.31075388193130493\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.2465149394490502 | Validation loss: 0.2842131555080414\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.2152640623125163 | Validation loss: 0.22721347212791443\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.18463559042323718 | Validation loss: 0.24249565601348877\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.16122723370790482 | Validation loss: 0.15358808636665344\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.14679778096350757 | Validation loss: 0.17089605331420898\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.13611172512173653 | Validation loss: 0.15524759888648987\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.13251594386317514 | Validation loss: 0.17342492938041687\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.12898269871419127 | Validation loss: 0.16011351346969604\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.12305541607466611 | Validation loss: 0.1949668526649475\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.11235611580989578 | Validation loss: 0.10236149281263351\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.10381116582588716 | Validation loss: 0.13342885673046112\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.08670141505585476 | Validation loss: 0.09188296645879745\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.08180331498045813 | Validation loss: 0.12372232973575592\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07200923278419809 | Validation loss: 0.07592625916004181\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.066264333457432 | Validation loss: 0.0874938890337944\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.06131140947003256 | Validation loss: 0.07372075319290161\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.06025365598245778 | Validation loss: 0.07412300258874893\n",
      "Validation loss (ends of cycles): [0.60892308 0.37427014 0.17342493 0.074123  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 19\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.5367445856332779 | Validation loss: 0.4483487606048584\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.5206190437078476 | Validation loss: 0.44038158655166626\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.4976606220006943 | Validation loss: 0.4257300943136215\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.4683428943157196 | Validation loss: 0.4045237749814987\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.43049020171165464 | Validation loss: 0.382641464471817\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.3816151052713394 | Validation loss: 0.35842086374759674\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.3326111823320389 | Validation loss: 0.3364621549844742\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.29561977237463 | Validation loss: 0.318710595369339\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.26997964084148407 | Validation loss: 0.2970084324479103\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.2537916973233223 | Validation loss: 0.27319707721471786\n",
      "Validation loss (ends of cycles): [0.44834876]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.24661508649587632 | Validation loss: 0.25705454498529434\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.2426185429096222 | Validation loss: 0.22602836042642593\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.22818358689546586 | Validation loss: 0.2005477249622345\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.2082375779747963 | Validation loss: 0.19975723326206207\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.1813648521900177 | Validation loss: 0.1895284503698349\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.15327624678611756 | Validation loss: 0.1219119057059288\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.12782533541321756 | Validation loss: 0.062152622267603874\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.10957009494304656 | Validation loss: 0.1485811546444893\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0981064923107624 | Validation loss: 0.06758278980851173\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.0926601156592369 | Validation loss: 0.09994453191757202\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08913373351097106 | Validation loss: 0.1018301472067833\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.086388049274683 | Validation loss: 0.08931709080934525\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.08263651877641678 | Validation loss: 0.1043701171875\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.07813577800989151 | Validation loss: 0.061198340728878975\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.07279523089528084 | Validation loss: 0.0668635182082653\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06805610843002796 | Validation loss: 0.5645378977060318\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06686716079711914 | Validation loss: 0.06682540383189917\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.059197721630334856 | Validation loss: 0.0642244964838028\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06171116679906845 | Validation loss: 0.2145145758986473\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.056293224170804027 | Validation loss: 0.05585295893251896\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.0526149595156312 | Validation loss: 0.053398482501506805\n",
      "Validation loss (ends of cycles): [0.44834876 0.25705454 0.10183015 0.05339848]\n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/losses')\n",
    "dest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/models')\n",
    "\n",
    "order = 12\n",
    "seeds = range(20) \n",
    "n_epochs = 31\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "learners.train((X, y, depth_order[:, -1]), \n",
    "               order=order,\n",
    "               dest_dir_loss=dest_dir_loss,\n",
    "               dest_dir_model=dest_dir_model,\n",
    "               n_epochs=n_epochs,\n",
    "               sc_kwargs=params_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-776114b6-f015-41e3-a955-951841ff2099\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.887493</td>\n",
       "      <td>2.754606</td>\n",
       "      <td>0.691143</td>\n",
       "      <td>0.805993</td>\n",
       "      <td>0.657956</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>0.346751</td>\n",
       "      <td>51.508686</td>\n",
       "      <td>-0.009657</td>\n",
       "      <td>-0.016915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.330355</td>\n",
       "      <td>0.728265</td>\n",
       "      <td>0.088094</td>\n",
       "      <td>0.057573</td>\n",
       "      <td>0.173942</td>\n",
       "      <td>0.263369</td>\n",
       "      <td>0.094363</td>\n",
       "      <td>12.377707</td>\n",
       "      <td>0.048598</td>\n",
       "      <td>0.078858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.547348</td>\n",
       "      <td>1.282034</td>\n",
       "      <td>0.572637</td>\n",
       "      <td>0.714614</td>\n",
       "      <td>0.376275</td>\n",
       "      <td>0.141583</td>\n",
       "      <td>0.181809</td>\n",
       "      <td>34.942499</td>\n",
       "      <td>-0.114649</td>\n",
       "      <td>-0.215123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.642766</td>\n",
       "      <td>2.256554</td>\n",
       "      <td>0.619215</td>\n",
       "      <td>0.753152</td>\n",
       "      <td>0.574179</td>\n",
       "      <td>0.329684</td>\n",
       "      <td>0.303541</td>\n",
       "      <td>43.296409</td>\n",
       "      <td>-0.029528</td>\n",
       "      <td>-0.041320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.817055</td>\n",
       "      <td>2.608120</td>\n",
       "      <td>0.690513</td>\n",
       "      <td>0.805246</td>\n",
       "      <td>0.617902</td>\n",
       "      <td>0.381848</td>\n",
       "      <td>0.335921</td>\n",
       "      <td>47.255385</td>\n",
       "      <td>-0.001811</td>\n",
       "      <td>-0.002683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.033550</td>\n",
       "      <td>3.186851</td>\n",
       "      <td>0.752098</td>\n",
       "      <td>0.844316</td>\n",
       "      <td>0.728023</td>\n",
       "      <td>0.530550</td>\n",
       "      <td>0.386050</td>\n",
       "      <td>61.034292</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>0.030791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.791070</td>\n",
       "      <td>4.049294</td>\n",
       "      <td>0.867742</td>\n",
       "      <td>0.920004</td>\n",
       "      <td>1.118009</td>\n",
       "      <td>1.249944</td>\n",
       "      <td>0.565283</td>\n",
       "      <td>73.572367</td>\n",
       "      <td>0.079008</td>\n",
       "      <td>0.092876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-776114b6-f015-41e3-a955-951841ff2099')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-776114b6-f015-41e3-a955-951841ff2099 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-776114b6-f015-41e3-a955-951841ff2099');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  18.000000  18.000000  18.000000  18.000000  18.000000  18.000000   \n",
       "mean    1.887493   2.754606   0.691143   0.805993   0.657956   0.461482   \n",
       "std     0.330355   0.728265   0.088094   0.057573   0.173942   0.263369   \n",
       "min     1.547348   1.282034   0.572637   0.714614   0.376275   0.141583   \n",
       "25%     1.642766   2.256554   0.619215   0.753152   0.574179   0.329684   \n",
       "50%     1.817055   2.608120   0.690513   0.805246   0.617902   0.381848   \n",
       "75%     2.033550   3.186851   0.752098   0.844316   0.728023   0.530550   \n",
       "max     2.791070   4.049294   0.867742   0.920004   1.118009   1.249944   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  18.000000  18.000000  18.000000  18.000000  \n",
       "mean    0.346751  51.508686  -0.009657  -0.016915  \n",
       "std     0.094363  12.377707   0.048598   0.078858  \n",
       "min     0.181809  34.942499  -0.114649  -0.215123  \n",
       "25%     0.303541  43.296409  -0.029528  -0.041320  \n",
       "50%     0.335921  47.255385  -0.001811  -0.002683  \n",
       "75%     0.386050  61.034292   0.016324   0.030791  \n",
       "max     0.565283  73.572367   0.079008   0.092876  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/gelisols/models')\n",
    "order = 12\n",
    "seeds = range(20)\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_local_gelisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                  order = order,\n",
    "                                                  src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_local_gelisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test on Vertisols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Seed: 0\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.161204216511626 | Validation loss: 0.1679159700870514\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.15437143099935433 | Validation loss: 0.15762153267860413\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.13889641942162262 | Validation loss: 0.13728273659944534\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.11834369207683362 | Validation loss: 0.10279234126210213\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.09297808573434227 | Validation loss: 0.09408992528915405\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07264437330396552 | Validation loss: 0.055685702711343765\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06562904072435279 | Validation loss: 0.08587116375565529\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.055855809269767055 | Validation loss: 0.0615625474601984\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05285423092151943 | Validation loss: 0.05325787328183651\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05118850050003905 | Validation loss: 0.06249404326081276\n",
      "Validation loss (ends of cycles): [0.16791597]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04790492297003144 | Validation loss: 0.05141059495508671\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04692842046681203 | Validation loss: 0.05575957149267197\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04742170605612429 | Validation loss: 0.048377299681305885\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.045442073164801845 | Validation loss: 0.05056057125329971\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.049288692442994365 | Validation loss: 0.045949578285217285\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04942320875431362 | Validation loss: 0.046182602643966675\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.048454557790568 | Validation loss: 0.061180008575320244\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04570011774960317 | Validation loss: 0.05048673413693905\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04413507094508723 | Validation loss: 0.049398086965084076\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03897152105836492 | Validation loss: 0.04086455702781677\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03762085471106203 | Validation loss: 0.04161454364657402\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03897195544682051 | Validation loss: 0.04463193938136101\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03908969050175265 | Validation loss: 0.047162629663944244\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03908744160281984 | Validation loss: 0.05538894981145859\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03990407954705389 | Validation loss: 0.04268927872180939\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03997622842067167 | Validation loss: 0.045263996347784996\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04153041404328848 | Validation loss: 0.042628781870007515\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.034978562671887245 | Validation loss: 0.043797941878437996\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03331961276891984 | Validation loss: 0.038327883929014206\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03273361440944044 | Validation loss: 0.03366365935653448\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.029028360584848804 | Validation loss: 0.0335803534835577\n",
      "Validation loss (ends of cycles): [0.16791597 0.05141059 0.04161454 0.03358035]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 1\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08880109026243813 | Validation loss: 0.08211258985102177\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08559589107569895 | Validation loss: 0.07982487976551056\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08120929841932498 | Validation loss: 0.07661886140704155\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07261521545679946 | Validation loss: 0.07628549262881279\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06738892393676858 | Validation loss: 0.06264040432870388\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06283781206921528 | Validation loss: 0.05332220159471035\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0563666181344735 | Validation loss: 0.052990976721048355\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.053898568314157035 | Validation loss: 0.05359513498842716\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.0495483853707188 | Validation loss: 0.04845046065747738\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04790328227375683 | Validation loss: 0.04489264823496342\n",
      "Validation loss (ends of cycles): [0.08211259]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0470738457025666 | Validation loss: 0.044814372435212135\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04557334945390099 | Validation loss: 0.045520488172769547\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04488633739712991 | Validation loss: 0.045287614688277245\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04274003678246548 | Validation loss: 0.05250200070440769\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04469038634315917 | Validation loss: 0.06699041835963726\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04659118973895123 | Validation loss: 0.0444390494376421\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.045118058786580435 | Validation loss: 0.04487036541104317\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04162646614407238 | Validation loss: 0.04729745723307133\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.040547048084829986 | Validation loss: 0.039926101453602314\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03623779441573118 | Validation loss: 0.03939523547887802\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03616418807130111 | Validation loss: 0.03911025729030371\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.035900585745510305 | Validation loss: 0.03916540555655956\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03519123988716226 | Validation loss: 0.03917317185550928\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03432324999257138 | Validation loss: 0.04085913486778736\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.036748546616811505 | Validation loss: 0.041731780394911766\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.037517647111886425 | Validation loss: 0.09109430015087128\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03717169126397685 | Validation loss: 0.04643022455275059\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03365675114879483 | Validation loss: 0.039381884038448334\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03359140867465421 | Validation loss: 0.035449360497295856\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.029328686920435804 | Validation loss: 0.03771654795855284\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.029815007001161575 | Validation loss: 0.035608227364718914\n",
      "Validation loss (ends of cycles): [0.08211259 0.04481437 0.03911026 0.03560823]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 2\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08840935884250535 | Validation loss: 0.08210575208067894\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08507650593916576 | Validation loss: 0.08108918741345406\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07737381735609637 | Validation loss: 0.0804343322912852\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.06796439374900526 | Validation loss: 0.08102053900559743\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.05701384869300657 | Validation loss: 0.06451516598463058\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.04790849404202567 | Validation loss: 0.04559866711497307\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0425061976744069 | Validation loss: 0.05457633485396703\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.03917014981723494 | Validation loss: 0.10548477371533711\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.03815334942191839 | Validation loss: 0.07415188476443291\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.03634079255991512 | Validation loss: 0.03436201065778732\n",
      "Validation loss (ends of cycles): [0.08210575]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.03238243547578653 | Validation loss: 0.03496560640633106\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.031512254331674844 | Validation loss: 0.03387966255346934\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.03323024997694625 | Validation loss: 0.06266245618462563\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.03258724557235837 | Validation loss: 0.07572312156359355\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.03392490858419074 | Validation loss: 0.08760666350523631\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.032957878481182784 | Validation loss: 0.06539637347062428\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.03240760095003578 | Validation loss: 0.030444981530308723\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.030572137277987268 | Validation loss: 0.028518366316954296\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.02843097411096096 | Validation loss: 0.028718551620841026\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.025377622495094936 | Validation loss: 0.03340643892685572\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.02524500247091055 | Validation loss: 0.027380989864468575\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.025558336534433894 | Validation loss: 0.026460225383440655\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.023630426679220464 | Validation loss: 0.028433510412772495\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.02497979895108276 | Validation loss: 0.032619635264078774\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.026406725351181295 | Validation loss: 0.035625407472252846\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.026628990140226152 | Validation loss: 0.12885981798171997\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.026922656533618767 | Validation loss: 0.055682502686977386\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.025312546226713393 | Validation loss: 0.04670518139998118\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.023591533665441804 | Validation loss: 0.075415700674057\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.02143574645742774 | Validation loss: 0.021575671931107838\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.019714292811436787 | Validation loss: 0.024841646663844585\n",
      "Validation loss (ends of cycles): [0.08210575 0.03496561 0.02738099 0.02484165]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 3\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08366356927313302 | Validation loss: 0.06628272930781047\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08258866577556259 | Validation loss: 0.06598960359891255\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08011062364829213 | Validation loss: 0.06656766682863235\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07609734664622106 | Validation loss: 0.07429493218660355\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06887183612898777 | Validation loss: 0.08531180272499721\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.062297113827968896 | Validation loss: 0.05507988358537356\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.057449167300211754 | Validation loss: 0.05169703687230746\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05063312598749211 | Validation loss: 0.044730848322312035\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04662537633588439 | Validation loss: 0.05200311293204626\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.042545238313706296 | Validation loss: 0.040968768298625946\n",
      "Validation loss (ends of cycles): [0.06628273]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.040791098518591175 | Validation loss: 0.04076941559712092\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.03988300430539407 | Validation loss: 0.04062818984190623\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.041134035116747805 | Validation loss: 0.06347007056077321\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.040732931933904946 | Validation loss: 0.053751084953546524\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.0412690176775581 | Validation loss: 0.04342729101578394\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04279994739121512 | Validation loss: 0.03779313713312149\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04107574108791979 | Validation loss: 0.05226917316516241\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03682027963039122 | Validation loss: 0.04407886415719986\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03602115907951405 | Validation loss: 0.0353589312483867\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03072228841483593 | Validation loss: 0.03361495025455952\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03127634593922841 | Validation loss: 0.03227363092203935\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.030668180729997784 | Validation loss: 0.03339084858695666\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03049330334914358 | Validation loss: 0.033799403036634125\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.030560468372545745 | Validation loss: 0.052750845750172935\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.032240519398137144 | Validation loss: 0.11552038788795471\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03203717256455045 | Validation loss: 0.03324045240879059\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03040875906222745 | Validation loss: 0.04665656387805939\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.029416492551957305 | Validation loss: 0.04413521351913611\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.029010920001095848 | Validation loss: 0.03647958238919576\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.02751604928389976 | Validation loss: 0.050870560109615326\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02429521475967608 | Validation loss: 0.028451986610889435\n",
      "Validation loss (ends of cycles): [0.06628273 0.04076942 0.03227363 0.02845199]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 4\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.19473344401309364 | Validation loss: 0.15656746923923492\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.18613588496258385 | Validation loss: 0.14746378362178802\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.16706412207139165 | Validation loss: 0.12539705261588097\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.13910072571352908 | Validation loss: 0.08593828231096268\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.10771784460858295 | Validation loss: 0.06520361453294754\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07648744355691106 | Validation loss: 0.07435402646660805\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06046923661702558 | Validation loss: 0.08778238669037819\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05391533515955273 | Validation loss: 0.06520635634660721\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04810122106420366 | Validation loss: 0.06413957849144936\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.044574140816142686 | Validation loss: 0.045684026554226875\n",
      "Validation loss (ends of cycles): [0.15656747]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.045094708470921764 | Validation loss: 0.04497688636183739\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.043865959503148734 | Validation loss: 0.04489790461957455\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.044169505273825245 | Validation loss: 0.04815280996263027\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04281844169293579 | Validation loss: 0.08249081298708916\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.043214576024758186 | Validation loss: 0.05961386486887932\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.042521203917108084 | Validation loss: 0.0713415015488863\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0419846284938486 | Validation loss: 0.07541047409176826\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03957709375964968 | Validation loss: 0.05406338535249233\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.035520012049298534 | Validation loss: 0.10436020791530609\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.034939819456715336 | Validation loss: 0.03864527679979801\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03136764094233513 | Validation loss: 0.03485617786645889\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.031533444790463695 | Validation loss: 0.032069167122244835\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03128002505553396 | Validation loss: 0.03990967012941837\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.032222791231776536 | Validation loss: 0.06194067373871803\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.030879733495806392 | Validation loss: 0.04783654771745205\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.034811057640533695 | Validation loss: 0.19700831919908524\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03208483088957636 | Validation loss: 0.04298360459506512\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.02921375377397788 | Validation loss: 0.04345952346920967\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.02759896424648009 | Validation loss: 0.030260787345468998\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.02629519714728782 | Validation loss: 0.04085123725235462\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02536522258857363 | Validation loss: 0.028398994356393814\n",
      "Validation loss (ends of cycles): [0.15656747 0.04497689 0.03485618 0.02839899]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 5\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.09135209562049972 | Validation loss: 0.06946399062871933\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.0878267308904065 | Validation loss: 0.0675109475851059\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07988109977708922 | Validation loss: 0.06493373587727547\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.0666891232960754 | Validation loss: 0.06385781802237034\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.05818599214156469 | Validation loss: 0.06167275831103325\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.05256716679367754 | Validation loss: 0.07637954130768776\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.049445088331898056 | Validation loss: 0.22612106055021286\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.045817383771969214 | Validation loss: 0.050643378868699074\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.040001612777511276 | Validation loss: 0.0446147657930851\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.03829288575798273 | Validation loss: 0.04114661552011967\n",
      "Validation loss (ends of cycles): [0.06946399]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.03613738570776251 | Validation loss: 0.040317755192518234\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.034891982562839985 | Validation loss: 0.039226071909070015\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.03366432442433304 | Validation loss: 0.046774642542004585\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.03640781891428762 | Validation loss: 0.04789281450212002\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.03705685358080599 | Validation loss: 0.05857366323471069\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.035437823894123234 | Validation loss: 0.07393408939242363\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.03706711303028795 | Validation loss: 0.06966803222894669\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03401203018923601 | Validation loss: 0.04148573614656925\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03279454085148043 | Validation loss: 0.03339186776429415\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.029390734827352896 | Validation loss: 0.03046796005219221\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.029600716920362577 | Validation loss: 0.03065457008779049\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.029213293352060847 | Validation loss: 0.031029099598526955\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.028610273264348507 | Validation loss: 0.030708318576216698\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.028793518121043842 | Validation loss: 0.046449968591332436\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.029298070300784376 | Validation loss: 0.04168690741062164\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.030492768364234105 | Validation loss: 0.0755428783595562\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.030982901031772297 | Validation loss: 0.030249490402638912\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.028873344521141715 | Validation loss: 0.03989887796342373\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.02943373481846518 | Validation loss: 0.029455197043716908\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.025847461229811113 | Validation loss: 0.03161353338509798\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.024220067593786452 | Validation loss: 0.027339047752320766\n",
      "Validation loss (ends of cycles): [0.06946399 0.04031776 0.03065457 0.02733905]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 6\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08758864708636936 | Validation loss: 0.09045941010117531\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08511891274860031 | Validation loss: 0.08673127368092537\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.0793429758203657 | Validation loss: 0.08125056326389313\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07344777999739897 | Validation loss: 0.07784003764390945\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06621623725483292 | Validation loss: 0.08580468967556953\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06146987037439095 | Validation loss: 0.08310152217745781\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.05917700419300481 | Validation loss: 0.06960192322731018\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.054859228236110585 | Validation loss: 0.067630959674716\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05065024114753071 | Validation loss: 0.055472830310463905\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04813684611336181 | Validation loss: 0.05709882639348507\n",
      "Validation loss (ends of cycles): [0.09045941]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04541594505702194 | Validation loss: 0.053741781041026115\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.043925387784838676 | Validation loss: 0.055874619632959366\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.043821888142510465 | Validation loss: 0.05342106893658638\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04354088537787136 | Validation loss: 0.05569586902856827\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04596223790002497 | Validation loss: 0.08136944100260735\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04429164842555398 | Validation loss: 0.056143974885344505\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04604476484421052 | Validation loss: 0.058117739856243134\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04353812356528483 | Validation loss: 0.06109851598739624\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03982813224980706 | Validation loss: 0.04927295260131359\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03627215433669718 | Validation loss: 0.047167809680104256\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.034992945233457966 | Validation loss: 0.047399308532476425\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03621590147285085 | Validation loss: 0.04751015082001686\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.033725014152495486 | Validation loss: 0.047297827899456024\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03643197546664037 | Validation loss: 0.045731207355856895\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03781851704575514 | Validation loss: 0.04651406966149807\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03732413926014775 | Validation loss: 0.04698087275028229\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.037296586522930546 | Validation loss: 0.04507102258503437\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03556570381318268 | Validation loss: 0.042184218764305115\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.030846391461397473 | Validation loss: 0.04002702981233597\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03090105475367684 | Validation loss: 0.038508640602231026\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.028801383156525463 | Validation loss: 0.03822813369333744\n",
      "Validation loss (ends of cycles): [0.09045941 0.05374178 0.04739931 0.03822813]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 7\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08331045665239033 | Validation loss: 0.06520787129799525\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08203087198106866 | Validation loss: 0.06532517820596695\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08000684745217625 | Validation loss: 0.06518079092105229\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.0739791032515074 | Validation loss: 0.06506842374801636\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06385869787711847 | Validation loss: 0.05250853920976321\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.057416989615089016 | Validation loss: 0.05340991293390592\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.052579465861383234 | Validation loss: 0.029369194293394685\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.04429109718062376 | Validation loss: 0.04665235554178556\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04031965685518164 | Validation loss: 0.06348493695259094\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.03789979886067541 | Validation loss: 0.03177877189591527\n",
      "Validation loss (ends of cycles): [0.06520787]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.03492777098558451 | Validation loss: 0.030528849456459284\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.03243052214384079 | Validation loss: 0.025340224984878052\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.033256117744665394 | Validation loss: 0.030804906350870926\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.03333615707723718 | Validation loss: 0.035566401512672506\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.03184130630995098 | Validation loss: 0.03293635222750405\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.03302473133723987 | Validation loss: 0.028592127503846616\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.030961162704778344 | Validation loss: 0.02682249341160059\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.028243662789463997 | Validation loss: 0.027744205047686894\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.02676465332900223 | Validation loss: 0.02321198567127188\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.02272553192941766 | Validation loss: 0.02984648073712985\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.021555469567446334 | Validation loss: 0.023247383224467438\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.0220749583340397 | Validation loss: 0.021610831453775365\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.020930324485035318 | Validation loss: 0.023576893222828705\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.021814725116679545 | Validation loss: 0.025978229319055874\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.02283184808727942 | Validation loss: 0.025270794207851093\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.02307317018704979 | Validation loss: 0.08468196541070938\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.021557157663138288 | Validation loss: 0.027905408913890522\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.021273777182949215 | Validation loss: 0.02349347559114297\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.01986664131675896 | Validation loss: 0.0233243799302727\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.017681540676245566 | Validation loss: 0.022845523431897163\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.016350375782502324 | Validation loss: 0.02178852337722977\n",
      "Validation loss (ends of cycles): [0.06520787 0.03052885 0.02324738 0.02178852]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 8\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.14074529629004628 | Validation loss: 0.13468989357352257\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.13303319051077492 | Validation loss: 0.1283198483288288\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.11730245461589411 | Validation loss: 0.1113440953195095\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.09925832560187892 | Validation loss: 0.0883958488702774\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.08102804579232868 | Validation loss: 0.0767072718590498\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06754080539471224 | Validation loss: 0.0630732923746109\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0626953479490782 | Validation loss: 0.05822800286114216\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05980836462817694 | Validation loss: 0.057572031393647194\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05771961749384278 | Validation loss: 0.06076772231608629\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.0532009103580525 | Validation loss: 0.05623008869588375\n",
      "Validation loss (ends of cycles): [0.13468989]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05379903512565713 | Validation loss: 0.05519082024693489\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.052072555807076 | Validation loss: 0.05436007305979729\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.051279104265727495 | Validation loss: 0.06033742055296898\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05221212537665116 | Validation loss: 0.05254641734063625\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.052044751024559924 | Validation loss: 0.05064303055405617\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04771482081789719 | Validation loss: 0.18143466114997864\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.048779759654089025 | Validation loss: 0.049892572686076164\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04602725470536634 | Validation loss: 0.0489846533164382\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.043393145756501904 | Validation loss: 0.04021776653826237\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.037118491863733845 | Validation loss: 0.043297613970935345\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.036233729535811825 | Validation loss: 0.04204665496945381\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.036370223486109785 | Validation loss: 0.04038366116583347\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03642301369262369 | Validation loss: 0.04990543611347675\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03510999748189198 | Validation loss: 0.035950854420661926\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03829076278366541 | Validation loss: 0.08307567611336708\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04044385832783423 | Validation loss: 0.047016918659210205\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.037926588403551204 | Validation loss: 0.037543052807450294\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.034744165445628916 | Validation loss: 0.05590544827282429\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03150788292680916 | Validation loss: 0.03907494433224201\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.029333851839366713 | Validation loss: 0.030293073505163193\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.027970748414334497 | Validation loss: 0.030819999054074287\n",
      "Validation loss (ends of cycles): [0.13468989 0.05519082 0.04204665 0.03082   ]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 9\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.12100787852939807 | Validation loss: 0.14878312995036444\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.1156453094200084 | Validation loss: 0.14218211422363916\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.10426287549106698 | Validation loss: 0.12800817439953485\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08943178112569608 | Validation loss: 0.10652916630109151\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07398556525769986 | Validation loss: 0.0644477941095829\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06363658705040028 | Validation loss: 0.05572609603404999\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06081610623943178 | Validation loss: 0.09725653131802876\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05732113220974019 | Validation loss: 0.06353205566604932\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.053295500772564036 | Validation loss: 0.05046262095371882\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05417528699495291 | Validation loss: 0.047901748990019165\n",
      "Validation loss (ends of cycles): [0.14878313]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0489277304395249 | Validation loss: 0.046974229936798416\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.05104770844704226 | Validation loss: 0.046928669015566506\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04871545045783645 | Validation loss: 0.04629917008181413\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04950790577813199 | Validation loss: 0.06741906702518463\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.050524057428303515 | Validation loss: 0.0722799909611543\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.051111385226249695 | Validation loss: 0.08795048048098882\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.049861222603603414 | Validation loss: 0.04207110404968262\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04603122537465472 | Validation loss: 0.03900467542310556\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.043004892276305905 | Validation loss: 0.0376884446789821\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03993555070146134 | Validation loss: 0.03346223756670952\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.038372725444404704 | Validation loss: 0.033292777525881924\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03876232497982288 | Validation loss: 0.03255048921952645\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.038259138892355715 | Validation loss: 0.03326376589636008\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.039389371773914286 | Validation loss: 0.03649425941208998\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.037030863918756186 | Validation loss: 0.03978176477054755\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04088841525739745 | Validation loss: 0.04172546664873759\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.038719205234787966 | Validation loss: 0.03826622168223063\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03795238160283158 | Validation loss: 0.039025234058499336\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.0355978360105502 | Validation loss: 0.035915122057000794\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03182907368203527 | Validation loss: 0.03283580827216307\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03164514409084069 | Validation loss: 0.03219876562555631\n",
      "Validation loss (ends of cycles): [0.14878313 0.04697423 0.03329278 0.03219877]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.09328515749228627 | Validation loss: 0.0794174075126648\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08947268716598812 | Validation loss: 0.07702409103512764\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08151930766670328 | Validation loss: 0.07422645390033722\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07421657756755226 | Validation loss: 0.0915425568819046\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06644049580944211 | Validation loss: 0.15212642401456833\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06289171956871685 | Validation loss: 0.06643170863389969\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.060507005570750484 | Validation loss: 0.07660464942455292\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05770743657883845 | Validation loss: 0.04702441208064556\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.055238479160164534 | Validation loss: 0.04257218353450298\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04844514770727409 | Validation loss: 0.038986045867204666\n",
      "Validation loss (ends of cycles): [0.07941741]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04857376590371132 | Validation loss: 0.03860069438815117\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.0471659146837498 | Validation loss: 0.03836953267455101\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.048829075243127976 | Validation loss: 0.03513345122337341\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05086563390336538 | Validation loss: 0.03178618475794792\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04892818766989206 | Validation loss: 0.07313217967748642\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05062789891503359 | Validation loss: 0.04181492328643799\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04422282143250892 | Validation loss: 0.031231501139700413\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0414473704601589 | Validation loss: 0.0330012571066618\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.041221494541356436 | Validation loss: 0.03016264084726572\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.035920314490795135 | Validation loss: 0.024509469978511333\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03617992938349122 | Validation loss: 0.023955611512064934\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03439663468222869 | Validation loss: 0.024173706769943237\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03432707321879111 | Validation loss: 0.03172864858061075\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.035520500748565324 | Validation loss: 0.02993414457887411\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03720426637875406 | Validation loss: 0.036875439807772636\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03747940367381824 | Validation loss: 0.055036623030900955\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.035825867597994054 | Validation loss: 0.04328293725848198\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03354562186685048 | Validation loss: 0.025072680786252022\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03464834401874166 | Validation loss: 0.024809451773762703\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03116374404022568 | Validation loss: 0.033944932743906975\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02939593855683741 | Validation loss: 0.022330881096422672\n",
      "Validation loss (ends of cycles): [0.07941741 0.03860069 0.02395561 0.02233088]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 11\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.12622165640718058 | Validation loss: 0.13938650488853455\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.1200494174110262 | Validation loss: 0.13313303887844086\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.10607607701891347 | Validation loss: 0.12224111333489418\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.09052326138082303 | Validation loss: 0.10416682437062263\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07436744202124446 | Validation loss: 0.07375293970108032\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.063938575747766 | Validation loss: 0.07426713779568672\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.057538029590719623 | Validation loss: 0.06209025718271732\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05095285353691954 | Validation loss: 0.06709360145032406\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04912557217635607 | Validation loss: 0.0595396663993597\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04791054472719368 | Validation loss: 0.05402009002864361\n",
      "Validation loss (ends of cycles): [0.1393865]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04601734866829295 | Validation loss: 0.0537868607789278\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04642667229238309 | Validation loss: 0.05364326946437359\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04651502166923724 | Validation loss: 0.05703286826610565\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04464602313543621 | Validation loss: 0.07510419934988022\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04594061886401553 | Validation loss: 0.0557715930044651\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0421954180653158 | Validation loss: 0.04985055699944496\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04223741483139364 | Validation loss: 0.04710370860993862\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.038117557764053345 | Validation loss: 0.04934592917561531\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03437867579295447 | Validation loss: 0.05659086816012859\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03409294313506076 | Validation loss: 0.04514491185545921\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03242792100890687 | Validation loss: 0.04265304282307625\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03009496952750181 | Validation loss: 0.04248355142772198\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03202511644677112 | Validation loss: 0.0439732950180769\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03216276111963548 | Validation loss: 0.05873432569205761\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.032698783141217734 | Validation loss: 0.04619231075048447\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03633682802319527 | Validation loss: 0.072358887642622\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.033581405681999105 | Validation loss: 0.05562468618154526\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.030268098472764616 | Validation loss: 0.059526894241571426\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.02835421930802496 | Validation loss: 0.035784799605607986\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.024994256367024622 | Validation loss: 0.036349328234791756\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.023506488503986282 | Validation loss: 0.03648699168115854\n",
      "Validation loss (ends of cycles): [0.1393865  0.05378686 0.04265304 0.03648699]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 12\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.12465888732358028 | Validation loss: 0.11083460599184036\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.11687581594053068 | Validation loss: 0.1044333999355634\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.10341314363636468 | Validation loss: 0.09221626569827397\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08686307386348122 | Validation loss: 0.07851269468665123\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07083727340949209 | Validation loss: 0.0551288320372502\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06321360847275508 | Validation loss: 0.05201607135434946\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.059471154487446734 | Validation loss: 0.050572953497370086\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05677154197014476 | Validation loss: 0.048783741891384125\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05208255586483957 | Validation loss: 0.05494089797139168\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05054084868415406 | Validation loss: 0.042090740675727524\n",
      "Validation loss (ends of cycles): [0.11083461]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.046194194313628895 | Validation loss: 0.0418690579632918\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04912359110618893 | Validation loss: 0.03736517330010732\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.050159576250926444 | Validation loss: 0.03728324609498183\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04813679552784091 | Validation loss: 0.06099647656083107\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05095356658689285 | Validation loss: 0.042035351817806564\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05249298383530818 | Validation loss: 0.08353547627727191\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04622210692124147 | Validation loss: 0.05047812437017759\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04407538248128012 | Validation loss: 0.03657640082140764\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04126803819580298 | Validation loss: 0.045112963765859604\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03931908642775134 | Validation loss: 0.03320226073265076\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.040036032294952555 | Validation loss: 0.034371147553126015\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03779972744737997 | Validation loss: 0.034147227803866066\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03736349481991247 | Validation loss: 0.03392460756003857\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.036734214554981964 | Validation loss: 0.05457255865136782\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03623275096699791 | Validation loss: 0.041262177750468254\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.039135233744194635 | Validation loss: 0.06989621991912524\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.041533306927273146 | Validation loss: 0.08880491803089778\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.036787426638367926 | Validation loss: 0.05547218148907026\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.037730952602271974 | Validation loss: 0.04338609303037325\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03741005190501088 | Validation loss: 0.03517623494068781\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.03459033566085916 | Validation loss: 0.029467060541113217\n",
      "Validation loss (ends of cycles): [0.11083461 0.04186906 0.03437115 0.02946706]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 13\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.14787319733908302 | Validation loss: 0.12134905159473419\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.13971965289429614 | Validation loss: 0.1133270300924778\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.12202531177746623 | Validation loss: 0.09741818159818649\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.09899224105634187 | Validation loss: 0.0680413693189621\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07547486730312046 | Validation loss: 0.05193481966853142\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.059965328754563084 | Validation loss: 0.1478968784213066\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.058371061949353466 | Validation loss: 0.060820143669843674\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.0518569956092458 | Validation loss: 0.04691869765520096\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.04747700201053368 | Validation loss: 0.05568346567451954\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04481482358747407 | Validation loss: 0.044492047280073166\n",
      "Validation loss (ends of cycles): [0.12134905]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04322212032581631 | Validation loss: 0.041569143533706665\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04246603638718003 | Validation loss: 0.04150853492319584\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04089725978280369 | Validation loss: 0.03653017058968544\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.041098617801540775 | Validation loss: 0.04898947477340698\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04085124303635798 | Validation loss: 0.04935206472873688\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.041323604552369365 | Validation loss: 0.04239597171545029\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.039808995531577816 | Validation loss: 0.0840645469725132\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03703288322216586 | Validation loss: 0.03245018795132637\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03433209422387575 | Validation loss: 0.03348969016224146\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03210164145811608 | Validation loss: 0.030444078147411346\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03053837034263109 | Validation loss: 0.03115426003932953\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.031159722393280583 | Validation loss: 0.03271864727139473\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03416347729140207 | Validation loss: 0.045318085700273514\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03107438314902155 | Validation loss: 0.056804386898875237\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.033158473474414724 | Validation loss: 0.05310705862939358\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03283156532990305 | Validation loss: 0.05297096632421017\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.033077974362592945 | Validation loss: 0.03136811312288046\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03158471576477352 | Validation loss: 0.058228276669979095\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.02844038595886607 | Validation loss: 0.046033360064029694\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.026794205566770153 | Validation loss: 0.02960763592272997\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.025866988360097532 | Validation loss: 0.030549601651728153\n",
      "Validation loss (ends of cycles): [0.12134905 0.04156914 0.03115426 0.0305496 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 14\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.2685273501433824 | Validation loss: 0.2683800756931305\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.25670922037802246 | Validation loss: 0.2526206970214844\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.22681226542121485 | Validation loss: 0.21906188130378723\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.18514166144948258 | Validation loss: 0.16225957870483398\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.13773585306970695 | Validation loss: 0.0982358418405056\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.09757452673817936 | Validation loss: 0.16215666383504868\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0740412101149559 | Validation loss: 0.055965250357985497\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06161656583610334 | Validation loss: 0.062003035098314285\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.056791181431004874 | Validation loss: 0.05057838559150696\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05035581047597684 | Validation loss: 0.04332583770155907\n",
      "Validation loss (ends of cycles): [0.26838008]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.047245774467132594 | Validation loss: 0.04338065907359123\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.048102253459786116 | Validation loss: 0.04022406227886677\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04582173936069012 | Validation loss: 0.03690587542951107\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.041917656352253335 | Validation loss: 0.04160160571336746\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04336070749712618 | Validation loss: 0.0791056714951992\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04261575992170133 | Validation loss: 0.053798090666532516\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04363253075433405 | Validation loss: 0.040439238771796227\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04248141987543357 | Validation loss: 0.03731362521648407\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.038024303209232654 | Validation loss: 0.041773609817028046\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03374856283986255 | Validation loss: 0.03551979921758175\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.033305737042897625 | Validation loss: 0.03380383178591728\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03389084388158823 | Validation loss: 0.03323566913604736\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.0329377921788316 | Validation loss: 0.03338051959872246\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03327037482277343 | Validation loss: 0.032685402780771255\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03479252481146863 | Validation loss: 0.0644093994051218\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03432726771815827 | Validation loss: 0.049457062035799026\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03283493897240413 | Validation loss: 0.03840087540447712\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.029340636769407673 | Validation loss: 0.036333074793219566\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.028881219558809932 | Validation loss: 0.03344106115400791\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.026324676251725146 | Validation loss: 0.029291590675711632\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02539735272722809 | Validation loss: 0.029340913519263268\n",
      "Validation loss (ends of cycles): [0.26838008 0.04338066 0.03380383 0.02934091]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 15\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.0841492728183144 | Validation loss: 0.06075831688940525\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08169916311376973 | Validation loss: 0.06054982356727123\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07849763216156709 | Validation loss: 0.06086615286767483\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07423726802593783 | Validation loss: 0.06112533435225487\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06984295362704679 | Validation loss: 0.04825133830308914\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06283724523688618 | Validation loss: 0.04184510372579098\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.058544049725720755 | Validation loss: 0.041651615872979164\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05212017580082542 | Validation loss: 0.04360814392566681\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.052451391067159805 | Validation loss: 0.04157676547765732\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04793435246928742 | Validation loss: 0.03575167618691921\n",
      "Validation loss (ends of cycles): [0.06075832]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04477887306558458 | Validation loss: 0.035469865426421165\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.045943650466046836 | Validation loss: 0.0336600337177515\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04350362703400223 | Validation loss: 0.03219062741845846\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04283701863728071 | Validation loss: 0.03756018541753292\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04464582922427278 | Validation loss: 0.0390226636081934\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.046644166406047974 | Validation loss: 0.03757801093161106\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04258477227076104 | Validation loss: 0.03985445946455002\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04088323328055834 | Validation loss: 0.026787959039211273\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03871416957362702 | Validation loss: 0.025218220427632332\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03509655153672946 | Validation loss: 0.02421511523425579\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.035071449550358874 | Validation loss: 0.02311981562525034\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.034543048394353765 | Validation loss: 0.022894551046192646\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.03377740447850604 | Validation loss: 0.024682712741196156\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03353702306355301 | Validation loss: 0.029586568474769592\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03612418242387081 | Validation loss: 0.027507783845067024\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03587194492942408 | Validation loss: 0.027790222316980362\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03557291837703241 | Validation loss: 0.03710603527724743\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.033871732284560016 | Validation loss: 0.03677363134920597\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.03167560118201532 | Validation loss: 0.032423168420791626\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.02956923759101253 | Validation loss: 0.019483156502246857\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02946120588795135 | Validation loss: 0.018310876563191414\n",
      "Validation loss (ends of cycles): [0.06075832 0.03546987 0.02311982 0.01831088]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 16\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08156146030676992 | Validation loss: 0.0705137016872565\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.07990784237259313 | Validation loss: 0.06988636900981267\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07819349220708798 | Validation loss: 0.06863330366710822\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07480346470286972 | Validation loss: 0.0666625127196312\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06917970745187056 | Validation loss: 0.058160472040375076\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06264691466563627 | Validation loss: 0.08878007034460704\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06039848516842252 | Validation loss: 0.05209088449676832\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05595046673950396 | Validation loss: 0.04798128828406334\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05155527258389875 | Validation loss: 0.041437882309158645\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04763707479363993 | Validation loss: 0.043172294894854225\n",
      "Validation loss (ends of cycles): [0.0705137]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04461290363810564 | Validation loss: 0.04293485110004743\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04245269229929698 | Validation loss: 0.03906371258199215\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.043166923287667726 | Validation loss: 0.04080717754550278\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.042604215247066396 | Validation loss: 0.03541523963212967\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04206815513929254 | Validation loss: 0.03434837299088637\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.040814362760437164 | Validation loss: 0.034936813535750844\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04256990846050413 | Validation loss: 0.05865098908543587\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.038062922558502146 | Validation loss: 0.032220245649417244\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03491674932210069 | Validation loss: 0.03063141368329525\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.032406675727351716 | Validation loss: 0.031299490481615067\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.030975149993441607 | Validation loss: 0.03276701706151167\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03167034911089822 | Validation loss: 0.031768561651309334\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.02994556332889356 | Validation loss: 0.030046330144008\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03042360659884779 | Validation loss: 0.02618786444266637\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.033261800380913836 | Validation loss: 0.03181804623454809\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03491548017451638 | Validation loss: 0.03237322314331929\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03453618023348482 | Validation loss: 0.030259561104079086\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.02996809525709403 | Validation loss: 0.02521776221692562\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.029768657135336024 | Validation loss: 0.027145131180683773\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.02678287053774846 | Validation loss: 0.02985484277208646\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02542775310575962 | Validation loss: 0.029113321254650753\n",
      "Validation loss (ends of cycles): [0.0705137  0.04293485 0.03276702 0.02911332]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 17\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.18578452575537893 | Validation loss: 0.17607577641805014\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.17636994024117789 | Validation loss: 0.16514772176742554\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.15567576388518015 | Validation loss: 0.14257992307345072\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.1261256126066049 | Validation loss: 0.11006870617469151\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.09516125255160862 | Validation loss: 0.07898629705111186\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07224813931518131 | Validation loss: 0.07449610034624736\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06180248782038689 | Validation loss: 0.059722560147444405\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05495261256065634 | Validation loss: 0.05743814756472906\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.0541409340997537 | Validation loss: 0.05083008110523224\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.0495873944212993 | Validation loss: 0.05104871218403181\n",
      "Validation loss (ends of cycles): [0.17607578]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04791343791617288 | Validation loss: 0.049677314857641854\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.048474044952955514 | Validation loss: 0.04772906253735224\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04561085191865762 | Validation loss: 0.04465216274062792\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.04276795002321402 | Validation loss: 0.044079518566528954\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.046003543875283666 | Validation loss: 0.05234615504741669\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04754653500599994 | Validation loss: 0.057119290033976235\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04541455095426904 | Validation loss: 0.051875809828440346\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04212984825587935 | Validation loss: 0.061386716862519584\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04025129984236426 | Validation loss: 0.0377594760308663\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.037910942195190325 | Validation loss: 0.03676092314223448\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.035172241946889296 | Validation loss: 0.03536786511540413\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03531419661723905 | Validation loss: 0.03570879126588503\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.0355098739059435 | Validation loss: 0.033749821285406746\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.03708219093581041 | Validation loss: 0.034131928657492004\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.036728250690632396 | Validation loss: 0.08373745282491048\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.037703154815567866 | Validation loss: 0.16881321867307028\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04052116773608658 | Validation loss: 0.035450027945140995\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.03702345759504371 | Validation loss: 0.04713146264354388\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.033382929033703275 | Validation loss: 0.033809199929237366\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.030483958828780387 | Validation loss: 0.03201883099973202\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.028836593238843813 | Validation loss: 0.029221948857108753\n",
      "Validation loss (ends of cycles): [0.17607578 0.04967731 0.03536787 0.02922195]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 18\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.08226940329921872 | Validation loss: 0.08771190047264099\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.08080815602290004 | Validation loss: 0.08779796585440636\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.07893975392768257 | Validation loss: 0.08810674771666527\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07525817559737909 | Validation loss: 0.08791105076670647\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06839138034142946 | Validation loss: 0.0787418819963932\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06047361873482403 | Validation loss: 0.057681020349264145\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.05439754487260392 | Validation loss: 0.10234533622860909\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.048915984697247804 | Validation loss: 0.09288699552416801\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05011697858572006 | Validation loss: 0.07140225917100906\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04586619089700674 | Validation loss: 0.054284341633319855\n",
      "Validation loss (ends of cycles): [0.0877119]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04105820320546627 | Validation loss: 0.05086086876690388\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.040411756717060744 | Validation loss: 0.05188886821269989\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04021171529434229 | Validation loss: 0.052854619920253754\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.039001153291840306 | Validation loss: 0.053517796099185944\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.03992528409550065 | Validation loss: 0.11048462241888046\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0428880665843424 | Validation loss: 0.048895107582211494\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.03952205651684811 | Validation loss: 0.09723616763949394\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03598170609850632 | Validation loss: 0.054928792640566826\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.034703530959392846 | Validation loss: 0.04207434877753258\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.03246637443570714 | Validation loss: 0.04125319607555866\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03022692215285803 | Validation loss: 0.03823002055287361\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.031208522029613193 | Validation loss: 0.04041556641459465\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.029294442140350218 | Validation loss: 0.0428590402007103\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.0297195372220717 | Validation loss: 0.04256322421133518\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.03125792741775513 | Validation loss: 0.10660433769226074\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03019392794292224 | Validation loss: 0.06545785069465637\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.033481411537841746 | Validation loss: 0.03471088223159313\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.027845038640263834 | Validation loss: 0.07449803873896599\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.027143595397080247 | Validation loss: 0.03891387768089771\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.025757619964056892 | Validation loss: 0.041501617059111595\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02374906617363817 | Validation loss: 0.04013838246464729\n",
      "Validation loss (ends of cycles): [0.0877119  0.05086087 0.03823002 0.04013838]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 19\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.16255763173103333 | Validation loss: 0.14109364648660025\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.15516356999675432 | Validation loss: 0.13523547103007635\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.14172234551774132 | Validation loss: 0.12201743572950363\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.12399974134233263 | Validation loss: 0.09632302448153496\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.10233049053284857 | Validation loss: 0.06885181864102681\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.0784348054892487 | Validation loss: 0.05762290582060814\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06391300840510263 | Validation loss: 0.0725318193435669\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.05626908710433377 | Validation loss: 0.04346885159611702\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.051691514543361135 | Validation loss: 0.04434716080625852\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04746196946750084 | Validation loss: 0.04489594325423241\n",
      "Validation loss (ends of cycles): [0.14109365]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.043907886577977076 | Validation loss: 0.04307339588801066\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04457749778197871 | Validation loss: 0.04406307637691498\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.044939831313159734 | Validation loss: 0.08915746957063675\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.044208405539393425 | Validation loss: 0.036763026068607964\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.04433420538488361 | Validation loss: 0.04416805567840735\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.04537590737971994 | Validation loss: 0.4176284372806549\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.04425143709199296 | Validation loss: 0.18371537327766418\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.03873702914764484 | Validation loss: 0.0807472715775172\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.03545353727208243 | Validation loss: 0.042597355941931404\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.032974150549206466 | Validation loss: 0.033211088428894676\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.030094111027816933 | Validation loss: 0.033144605035583176\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.03217791558967696 | Validation loss: 0.03846348077058792\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.031263780780136585 | Validation loss: 0.030576524635155995\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.030434809832109347 | Validation loss: 0.04343028490742048\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.032326566986739635 | Validation loss: 0.13629954804976782\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.03342853072616789 | Validation loss: 0.06467030942440033\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.03306691503773133 | Validation loss: 0.041692071904738746\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.030296926179693803 | Validation loss: 0.049537912011146545\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.026243075573196013 | Validation loss: 0.031099140644073486\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.024759062979784276 | Validation loss: 0.026985854531327885\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.02487910890744792 | Validation loss: 0.029790397733449936\n",
      "Validation loss (ends of cycles): [0.14109365 0.0430734  0.03314461 0.0297904 ]\n"
     ]
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/losses')\n",
    "dest_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/models')\n",
    "\n",
    "order = 10\n",
    "seeds = range(20) \n",
    "n_epochs = 31\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "learners.train((X, y, depth_order[:, -1]), \n",
    "               order=order,\n",
    "               dest_dir_loss=dest_dir_loss,\n",
    "               dest_dir_model=dest_dir_model,\n",
    "               n_epochs=n_epochs,\n",
    "               sc_kwargs=params_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b6afa532-38a8-4e32-98e6-978b8dd3dff2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpd</th>\n",
       "      <th>rpiq</th>\n",
       "      <th>r2</th>\n",
       "      <th>lccc</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>bias</th>\n",
       "      <th>stb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.777816</td>\n",
       "      <td>2.448334</td>\n",
       "      <td>0.669128</td>\n",
       "      <td>0.800569</td>\n",
       "      <td>0.328906</td>\n",
       "      <td>0.110845</td>\n",
       "      <td>0.213575</td>\n",
       "      <td>30.958227</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.003632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.184440</td>\n",
       "      <td>0.352951</td>\n",
       "      <td>0.069418</td>\n",
       "      <td>0.041342</td>\n",
       "      <td>0.052977</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>0.023262</td>\n",
       "      <td>4.237755</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>0.065963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.473109</td>\n",
       "      <td>1.825536</td>\n",
       "      <td>0.532598</td>\n",
       "      <td>0.716435</td>\n",
       "      <td>0.252563</td>\n",
       "      <td>0.063788</td>\n",
       "      <td>0.179316</td>\n",
       "      <td>24.304497</td>\n",
       "      <td>-0.041362</td>\n",
       "      <td>-0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.615667</td>\n",
       "      <td>2.245532</td>\n",
       "      <td>0.611233</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>0.292217</td>\n",
       "      <td>0.085391</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>28.024661</td>\n",
       "      <td>-0.016770</td>\n",
       "      <td>-0.041078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.788574</td>\n",
       "      <td>2.443511</td>\n",
       "      <td>0.683310</td>\n",
       "      <td>0.807425</td>\n",
       "      <td>0.318792</td>\n",
       "      <td>0.101629</td>\n",
       "      <td>0.209331</td>\n",
       "      <td>29.388569</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.925041</td>\n",
       "      <td>2.621849</td>\n",
       "      <td>0.726185</td>\n",
       "      <td>0.828116</td>\n",
       "      <td>0.349539</td>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.231883</td>\n",
       "      <td>34.220246</td>\n",
       "      <td>0.018196</td>\n",
       "      <td>0.041607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.113319</td>\n",
       "      <td>3.132748</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.861943</td>\n",
       "      <td>0.458266</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>0.255553</td>\n",
       "      <td>40.589359</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>0.129690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6afa532-38a8-4e32-98e6-978b8dd3dff2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b6afa532-38a8-4e32-98e6-978b8dd3dff2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b6afa532-38a8-4e32-98e6-978b8dd3dff2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             rpd       rpiq         r2       lccc       rmse        mse  \\\n",
       "count  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "mean    1.777816   2.448334   0.669128   0.800569   0.328906   0.110845   \n",
       "std     0.184440   0.352951   0.069418   0.041342   0.052977   0.037720   \n",
       "min     1.473109   1.825536   0.532598   0.716435   0.252563   0.063788   \n",
       "25%     1.615667   2.245532   0.611233   0.773600   0.292217   0.085391   \n",
       "50%     1.788574   2.443511   0.683310   0.807425   0.318792   0.101629   \n",
       "75%     1.925041   2.621849   0.726185   0.828116   0.349539   0.122219   \n",
       "max     2.113319   3.132748   0.772750   0.861943   0.458266   0.210007   \n",
       "\n",
       "             mae       mape       bias        stb  \n",
       "count  20.000000  20.000000  20.000000  20.000000  \n",
       "mean    0.213575  30.958227  -0.001265  -0.003632  \n",
       "std     0.023262   4.237755   0.024421   0.065963  \n",
       "min     0.179316  24.304497  -0.041362  -0.115813  \n",
       "25%     0.197175  28.024661  -0.016770  -0.041078  \n",
       "50%     0.209331  29.388569   0.000355   0.000905  \n",
       "75%     0.231883  34.220246   0.018196   0.041607  \n",
       "max     0.255553  40.589359   0.043430   0.129690  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace following Paths with yours\n",
    "src_dir_model = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/models')\n",
    "order = 10\n",
    "seeds = range(20)\n",
    "learners = Learners(Model, tax_lookup, seeds=seeds, device=device)\n",
    "perfs_local_vertisols, _, _, _ = learners.evaluate((X, y, depth_order[:, -1]),\n",
    "                                                  order = order,\n",
    "                                                  src_dir_model=src_dir_model)\n",
    "\n",
    "perfs_local_vertisols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirzai.training.core import load_dumps\n",
    "\n",
    "# Replace following Paths with yours\n",
    "dest_dir_loss = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/train_eval/vertisols/losses')\n",
    "losses = load_dumps(dest_dir_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(losses[4]['valid']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile metrics for \"local vs. global\" Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(*dfs):\n",
    "    perfs = {'r2': {'mean': [], 'std': []},\n",
    "             'mape': {'mean': [], 'std': []}}\n",
    "    for df in dfs:\n",
    "        for metric in ['r2', 'mape']:\n",
    "            mean, std = df.describe().loc[['mean', 'std'], metric].items()\n",
    "            perfs[metric]['mean'].append(mean[1])\n",
    "            perfs[metric]['std'].append(std[1])\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = {}\n",
    "perfs['global'] = format_metrics(perfs_global_mollisols, perfs_global_gelisols, perfs_global_vertisols)\n",
    "perfs['local'] = format_metrics(perfs_local_mollisols, perfs_local_gelisols, perfs_local_vertisols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': {'r2': {'mean': [0.7676331517935623,\n",
       "    0.7426116984444495,\n",
       "    0.7447822285232857],\n",
       "   'std': [0.022648288190552323, 0.0767026908137805, 0.06303329177546012]},\n",
       "  'mape': {'mean': [27.362060844898224, 47.88334220647812, 26.8556547164917],\n",
       "   'std': [1.1716483447190629, 10.194857908308927, 3.2722567422732105]}},\n",
       " 'local': {'r2': {'mean': [0.7727136496500038,\n",
       "    0.7461132281184896,\n",
       "    0.7569868331639625],\n",
       "   'std': [0.01908464680719597, 0.08643306096790201, 0.05496486944699407]},\n",
       "  'mape': {'mean': [26.96375846862793, 43.78511905670166, 25.207100808620453],\n",
       "   'std': [1.352271118990351, 10.660068154956656, 2.427168862994315]}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn')\n",
    "with open(dest_dir/'global_vs_local.pickle', 'wb') as f:\n",
    "    pickle.dump(perfs, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
