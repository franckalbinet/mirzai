{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/franckalbinet/mirzai/blob/main/nbs/15_paper.cnn.learning_curve.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2. Learning curve (CNN)\n",
    "\n",
    "> Computing the learning curve of the CNN for the prediction of exchangeable potassium (K ex.), with increasing number of training examples and using all Soil Taxonomy Orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mirzai\n",
      "  Downloading mirzai-0.2.10-py3-none-any.whl (25 kB)\n",
      "Collecting matplotlib>=3.5.1\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 10.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mirzai) (4.64.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.12.1+cu113)\n",
      "Requirement already satisfied: fastcore in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.5.22)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.3.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.0.2)\n",
      "Collecting captum\n",
      "  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 70.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from mirzai) (0.13.1+cu113)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mirzai) (1.21.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (7.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (1.4.4)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.37.1-py3-none-any.whl (957 kB)\n",
      "\u001b[K     |████████████████████████████████| 957 kB 68.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.5.1->mirzai) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.5.1->mirzai) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.1->mirzai) (1.15.0)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->mirzai) (21.1.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mirzai) (2022.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->mirzai) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->mirzai) (3.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->mirzai) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->mirzai) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->mirzai) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->mirzai) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->mirzai) (2.10)\n",
      "Installing collected packages: fonttools, matplotlib, captum, mirzai\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "Successfully installed captum-0.5.0 fonttools-4.37.1 matplotlib-3.5.3 mirzai-0.2.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',  force_remount=False)\n",
    "    !pip install mirzai\n",
    "else:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python utils\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# mirzai utilities\n",
    "from mirzai.data.loading import load_kssl\n",
    "from mirzai.data.selection import (select_y, select_tax_order, select_X)\n",
    "from mirzai.data.transform import log_transform_y\n",
    "from mirzai.training.cnn import (Model, weights_init)\n",
    "from mirzai.data.torch import DataLoaders, SNV_transform\n",
    "from mirzai.training.cnn import Learner\n",
    "from mirzai.training.core import is_plateau\n",
    "from mirzai.training.metrics import eval_reg\n",
    "\n",
    "from fastcore.transform import compose\n",
    "\n",
    "# Data science stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning stack\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '/content/drive/MyDrive/research/predict-k-mirs-dl/data/potassium'\n",
    "fnames = ['spectra-features.npy', 'spectra-wavenumbers.npy', \n",
    "          'depth-order.npy', 'target.npy', \n",
    "          'tax-order-lu.pkl', 'spectra-id.npy']\n",
    "\n",
    "X, X_names, depth_order, y, tax_lookup, X_id = load_kssl(src_dir, fnames=fnames)\n",
    "\n",
    "data = X, y, X_id, depth_order\n",
    "\n",
    "transforms = [select_y, select_tax_order, select_X, log_transform_y]\n",
    "X, y, X_id, depth_order = compose(*transforms)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (40132, 1764)\n",
      "y shape: (40132,)\n",
      "Wavenumbers:\n",
      " [3999 3997 3995 ...  603  601  599]\n",
      "depth_order (first 3 rows):\n",
      " [[43.  2.]\n",
      " [ 0.  0.]\n",
      " [ 0.  1.]]\n",
      "Taxonomic order lookup:\n",
      " {'alfisols': 0, 'mollisols': 1, 'inceptisols': 2, 'entisols': 3, 'spodosols': 4, 'undefined': 5, 'ultisols': 6, 'andisols': 7, 'histosols': 8, 'oxisols': 9, 'vertisols': 10, 'aridisols': 11, 'gelisols': 12}\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "print(f'Wavenumbers:\\n {X_names}')\n",
    "print(f'depth_order (first 3 rows):\\n {depth_order[:3, :]}')\n",
    "print(f'Taxonomic order lookup:\\n {tax_lookup}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Is a GPU available?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "print(f'Runtime is: {device}')\n",
    "\n",
    "n_epochs = 151\n",
    "step_size_up = 5\n",
    "criterion = MSELoss() # Mean Squared Error loss\n",
    "base_lr, max_lr = 3e-5, 1e-3 # Based on learning rate finder\n",
    "delta = 1e-3 # Loss difference threshold for early stopping\n",
    "\n",
    "dest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/learning_curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = [500, 1000, 2000, 5000, 10000, 20000, 30000, X.shape[0]]\n",
    "split_ratio = 0.1\n",
    "seeds = range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837d294b213a4862886d5ac549fb1bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06806543636222093 | Validation loss: 0.0665476806461811\n",
      "Validation loss (ends of cycles): [0.29326259]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06351911573793878 | Validation loss: 0.06200351657574637\n",
      "Validation loss (ends of cycles): [0.29326259]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05975192000159968 | Validation loss: 0.05620013448622143\n",
      "Validation loss (ends of cycles): [0.29326259]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05616931922190053 | Validation loss: 0.053450687412630045\n",
      "Validation loss (ends of cycles): [0.29326259]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05372029852261675 | Validation loss: 0.05114826099260857\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.054583898482796475 | Validation loss: 0.05386886536552195\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05613075314673799 | Validation loss: 0.052798241344198846\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.0567820893501389 | Validation loss: 0.05200839581850328\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.057392622577485716 | Validation loss: 0.05300498377989259\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05863403260737246 | Validation loss: 0.05642465720966197\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05551970786097252 | Validation loss: 0.05141487086943367\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05263045195791378 | Validation loss: 0.04939890414345683\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.049820137990884764 | Validation loss: 0.049233734019492804\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04752113914422147 | Validation loss: 0.047601242239276566\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04555318841097153 | Validation loss: 0.044789811596274376\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04647652471265379 | Validation loss: 0.04712833372647302\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.0479339515235591 | Validation loss: 0.04763588224325264\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.048971499608465904 | Validation loss: 0.049810390854090975\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05068343192618156 | Validation loss: 0.04795939717115017\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05176023933558892 | Validation loss: 0.04928604261786269\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.04998573636603073 | Validation loss: 0.05063137840152832\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04781438338274551 | Validation loss: 0.049298560344859174\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.044998955462840655 | Validation loss: 0.047076730155631116\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.04269748058948938 | Validation loss: 0.043681511099924124\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.04123661962486583 | Validation loss: 0.04148694225832036\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.04170608106831592 | Validation loss: 0.04531938837547051\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04312830294171969 | Validation loss: 0.046768018224260265\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04452355271407368 | Validation loss: 0.051704760902283486\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.045750220440313426 | Validation loss: 0.04653846119579516\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04793604665217256 | Validation loss: 0.04717006236968333\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04609691420353258 | Validation loss: 0.05028725784729447\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04414579540094327 | Validation loss: 0.046487056699238326\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.04129064368374308 | Validation loss: 0.04598317131922956\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.03918340381903526 | Validation loss: 0.046684552441563526\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.03772568548212096 | Validation loss: 0.03923895272115866\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.038246894685121685 | Validation loss: 0.04450457794755174\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.03958957887546727 | Validation loss: 0.04442280089776767\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.04075290252044944 | Validation loss: 0.0414913749616397\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04319122280095221 | Validation loss: 0.04472611011251023\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.044247216745270546 | Validation loss: 0.04369983495327465\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.04255689712355182 | Validation loss: 0.05006042926719314\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.04059595685577134 | Validation loss: 0.04566892837746102\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.03882334366095431 | Validation loss: 0.04446525734506155\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.036582596561480205 | Validation loss: 0.04001459044714769\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03509080934814036 | Validation loss: 0.0384905424884014\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03569372744653999 | Validation loss: 0.039927559855737184\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.037237670164209966 | Validation loss: 0.04519388896592876\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.03838471407750419 | Validation loss: 0.0458477276066939\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.03979467994788812 | Validation loss: 0.04402068803054199\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.041505508775100904 | Validation loss: 0.04638113081455231\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.04012122852048108 | Validation loss: 0.042935863954194804\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.03744441115240432 | Validation loss: 0.04187299245805071\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03562752796884413 | Validation loss: 0.04327832126434435\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.033714442978625934 | Validation loss: 0.03990272723399756\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.03269251721821123 | Validation loss: 0.037310676289755\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03308601810962432 | Validation loss: 0.03998835182242226\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.034110041697099955 | Validation loss: 0.043654320042645724\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.03643204273630293 | Validation loss: 0.042565111332295236\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.037886392731124006 | Validation loss: 0.04307818798380986\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.039455032082645614 | Validation loss: 0.050652103890713895\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03784991302610149 | Validation loss: 0.05323615209444573\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.03613430108963209 | Validation loss: 0.0495827330374404\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03396667248185511 | Validation loss: 0.05136706980696896\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03226531962671223 | Validation loss: 0.03999559313320277\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.03079099055284109 | Validation loss: 0.03639809499707138\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03151882326329601 | Validation loss: 0.037147419167715204\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.032178009986965614 | Validation loss: 0.04826249839051774\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03405666328724143 | Validation loss: 0.04936403996850315\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03541494072718028 | Validation loss: 0.04829113718056888\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.03756211765103145 | Validation loss: 0.04617483740705147\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03559289298973669 | Validation loss: 0.04658271506298007\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03390944113907141 | Validation loss: 0.04650598888595899\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03154865592806297 | Validation loss: 0.043829943690645065\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03073635195699728 | Validation loss: 0.03725755015355453\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.0294124557080089 | Validation loss: 0.03590158830609238\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.02953582158756738 | Validation loss: 0.037577209700095024\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.030502555425291702 | Validation loss: 0.044688741823560314\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03210949529278325 | Validation loss: 0.05308352276813565\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03396462016468923 | Validation loss: 0.04562949912067045\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03592904591179752 | Validation loss: 0.045200854474515245\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03457508122463962 | Validation loss: 0.050139864481854854\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03230217536562352 | Validation loss: 0.04341460270970537\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.03038559539548509 | Validation loss: 0.04495927831974991\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.02869941036104862 | Validation loss: 0.03822105756977148\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.02780957057285885 | Validation loss: 0.03527055269009188\n",
      "Validation loss (ends of cycles): [0.29326259 0.05114826 0.04478981 0.04148694 0.03923895 0.03849054\n",
      " 0.03731068 0.03639809 0.03590159 0.03527055]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 9 | Size: 30000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.147111305290539 | Validation loss: 0.1315017819404602\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.09975460252577537 | Validation loss: 0.0852299884399947\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08679056662674013 | Validation loss: 0.08812916081617861\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08041144743267643 | Validation loss: 0.0723718693589463\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07665490742870852 | Validation loss: 0.06908508214880438\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07295063323782462 | Validation loss: 0.064691921192057\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06718138600944688 | Validation loss: 0.058289176620104736\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06265752670159074 | Validation loss: 0.05494928048814044\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05877749496139586 | Validation loss: 0.05109663200290764\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05578988529496679 | Validation loss: 0.04918082448489526\n",
      "Validation loss (ends of cycles): [0.13150178]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05317496091960684 | Validation loss: 0.04732894088853808\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.05387537818843205 | Validation loss: 0.04777663764269913\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05503765009588709 | Validation loss: 0.049427153915166853\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05634034368160524 | Validation loss: 0.048755222164532715\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.0574380914239507 | Validation loss: 0.05096663817325059\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0579996428844568 | Validation loss: 0.05888870698125923\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05525187691183467 | Validation loss: 0.048332023379557276\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05298996464913025 | Validation loss: 0.04795002014759709\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.050095231600693964 | Validation loss: 0.04421709517345709\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04788641859042017 | Validation loss: 0.04255331138915876\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04585613830162114 | Validation loss: 0.0418057950980523\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.046981786802950266 | Validation loss: 0.042653957995421744\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04777146606019845 | Validation loss: 0.044101270704584965\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04940902760350391 | Validation loss: 0.04385254315155394\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05038880116649364 | Validation loss: 0.04633155581267441\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05155313427461997 | Validation loss: 0.046872320508255676\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.0495440785699573 | Validation loss: 0.045494657645330706\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.047308244940971855 | Validation loss: 0.04535501838168677\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.04507004849147052 | Validation loss: 0.04133476996246506\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.042914133947832805 | Validation loss: 0.04083044673590099\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.040776250611892655 | Validation loss: 0.03877844817059881\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.04152484470733294 | Validation loss: 0.039483094083912235\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04290025286670578 | Validation loss: 0.040904018326717265\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.044265624253373395 | Validation loss: 0.04122531755882151\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.046046443509035986 | Validation loss: 0.04211513080141124\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.047449978132193024 | Validation loss: 0.04790202175431392\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04531032362903811 | Validation loss: 0.046201490764232245\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04320118561950757 | Validation loss: 0.040469889671486965\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.0409529635376346 | Validation loss: 0.039225977001821295\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.039055508706032444 | Validation loss: 0.03890094250878867\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.03767110928525462 | Validation loss: 0.03655863406465334\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.03817510909621457 | Validation loss: 0.03774878807804164\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.0393792031259325 | Validation loss: 0.03817844322937376\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.04070846887648498 | Validation loss: 0.04050976456526448\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.0424697090467242 | Validation loss: 0.041637066970853245\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.043809773583014154 | Validation loss: 0.04080562661675846\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.04217543903811786 | Validation loss: 0.03963810448699138\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.040034622934303786 | Validation loss: 0.03918085889342953\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.03804198447492366 | Validation loss: 0.03835276941604474\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.036407624813728036 | Validation loss: 0.03730193979161627\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03499086681557329 | Validation loss: 0.035120617181939234\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03556273958253625 | Validation loss: 0.03798426411607686\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03691161809562656 | Validation loss: 0.037254741152419765\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.03808860194648763 | Validation loss: 0.03941758612499518\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.03961590641227208 | Validation loss: 0.03795143950949697\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.041122380242143805 | Validation loss: 0.047902766505585\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.039530974404739315 | Validation loss: 0.03792385430458714\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.03773507730373622 | Validation loss: 0.037603619760450194\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03599328672219264 | Validation loss: 0.037073982134461406\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03398567610665371 | Validation loss: 0.03748219501884545\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.03271165021670688 | Validation loss: 0.03435478813069708\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03338361110685295 | Validation loss: 0.036062022934065144\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.034301962170406784 | Validation loss: 0.037316511702888154\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.03564163815401691 | Validation loss: 0.0389238071792266\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.0370434378567887 | Validation loss: 0.04136042470002876\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.03914995556819792 | Validation loss: 0.04118646508192315\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.03734212646040281 | Validation loss: 0.039161085907150714\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.03560297669058567 | Validation loss: 0.0376554255538127\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03367881167091821 | Validation loss: 0.03781347434748621\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03207173739608966 | Validation loss: 0.03692457919173381\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.03081994304237397 | Validation loss: 0.0337879949632813\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03131349330498396 | Validation loss: 0.03656278537476764\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.032406741111098154 | Validation loss: 0.036839769627241524\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03356948233773246 | Validation loss: 0.04138847341870561\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03554210637352968 | Validation loss: 0.03746587980319472\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.03756678962197743 | Validation loss: 0.04222078783547177\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03534613802269297 | Validation loss: 0.03830132688231328\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03376366636887389 | Validation loss: 0.04012722719241591\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03233612979468154 | Validation loss: 0.037108558964203384\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03046056817800395 | Validation loss: 0.036080013193628364\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.029702923919907524 | Validation loss: 0.03293276488342706\n",
      "Validation loss (ends of cycles): [0.13150178 0.04732894 0.0418058  0.03877845 0.03655863 0.03512062\n",
      " 0.03435479 0.03378799 0.03293276]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 9 | Size: 40132\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.26767727328596386 | Validation loss: 0.20993191188415594\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.11062715910595115 | Validation loss: 0.09041815471992029\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08798102279171699 | Validation loss: 0.083032216448172\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.0831462372530721 | Validation loss: 0.08245944924059168\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.0791002170138765 | Validation loss: 0.07641426572757484\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07637509745985681 | Validation loss: 0.07529779728007528\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.07119164006293231 | Validation loss: 0.074695995984088\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06693422179815807 | Validation loss: 0.06877124391957722\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.06382161711423298 | Validation loss: 0.06549916515308143\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.06060796096099643 | Validation loss: 0.06079392095582675\n",
      "Validation loss (ends of cycles): [0.20993191]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05856810573774471 | Validation loss: 0.057926995514900284\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.05894209550121638 | Validation loss: 0.06261971423122208\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.059919283873819576 | Validation loss: 0.06526916124651917\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.060730206374973644 | Validation loss: 0.06995311323388488\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.06151795924428528 | Validation loss: 0.07613310333242458\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.061776850015057 | Validation loss: 0.07298920982706864\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05926943988920608 | Validation loss: 0.06051817781959487\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.0569572605478658 | Validation loss: 0.06914879089897186\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.05450942960391775 | Validation loss: 0.06324264410454615\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.052381872527638644 | Validation loss: 0.0565843051052199\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927  ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.050279307501815904 | Validation loss: 0.05087755673227057\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.05125819389913671 | Validation loss: 0.055446823069875216\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.05232975868449554 | Validation loss: 0.06173066887180362\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.05332614857828113 | Validation loss: 0.07285340931431382\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05472646032526033 | Validation loss: 0.07538743981415719\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05555287960506096 | Validation loss: 0.06203558999814291\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.05368010373602761 | Validation loss: 0.06663409501841638\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.05148195093277636 | Validation loss: 0.06459290692500308\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.049331870113360134 | Validation loss: 0.05822600282530869\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.04742172895197382 | Validation loss: 0.05136353392318814\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.04593015410094045 | Validation loss: 0.04718089631173463\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.04645789550767669 | Validation loss: 0.05242063497415686\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04760350834387611 | Validation loss: 0.0537388277099987\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04892369482930251 | Validation loss: 0.05456192056294036\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.05001625064833779 | Validation loss: 0.053328140350305926\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.05126057072417942 | Validation loss: 0.05397078264669507\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04982032163039319 | Validation loss: 0.05863784631071365\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04786631208661152 | Validation loss: 0.06205748732045161\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.045445062373178156 | Validation loss: 0.05508627135932973\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.04398632775368829 | Validation loss: 0.05074058398934065\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809 ]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.04237788485722455 | Validation loss: 0.04485161622277403\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.04334347072886083 | Validation loss: 0.048917631247797896\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.04402575698153182 | Validation loss: 0.05648849759481649\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.0455412358104244 | Validation loss: 0.05538065110094252\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04707928725488953 | Validation loss: 0.058348986625143914\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.04844906317495455 | Validation loss: 0.052721294714550004\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.04671046115993339 | Validation loss: 0.06256921447615708\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.044951911013771405 | Validation loss: 0.06044869774342638\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.04295778853404446 | Validation loss: 0.05060311258498546\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.041293553852175514 | Validation loss: 0.04906327166981929\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03999335623683599 | Validation loss: 0.043351529924347335\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.04056466871131886 | Validation loss: 0.049954215410800105\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.04156503059505302 | Validation loss: 0.0522121147343279\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.04284362089416877 | Validation loss: 0.05801308550665864\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.0443785713149572 | Validation loss: 0.05014750747158464\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.04580919714882882 | Validation loss: 0.06235515744944589\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.044337539317629 | Validation loss: 0.05064553986674389\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.04247408428625387 | Validation loss: 0.05216136716504013\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.040845778049336465 | Validation loss: 0.044927841662305644\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03896903427222406 | Validation loss: 0.0456106700167983\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.03817753170022932 | Validation loss: 0.041548734382454273\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.0385868555479353 | Validation loss: 0.0453451023626644\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.03971990962663123 | Validation loss: 0.05200313806401945\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.04082011654753032 | Validation loss: 0.05165595175550047\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.04232948364260951 | Validation loss: 0.04828151575891317\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.04398110482519067 | Validation loss: 0.0522270905331964\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.04246964514636853 | Validation loss: 0.04821099462894212\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.04050930707319456 | Validation loss: 0.05413374576750582\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03861887257832183 | Validation loss: 0.045728032221704455\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03748417487477986 | Validation loss: 0.04113612045426812\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.036482414228870996 | Validation loss: 0.03996460937556967\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.036949899838687336 | Validation loss: 0.04259959054467952\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03779611252358286 | Validation loss: 0.05049621508316656\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03913594540263578 | Validation loss: 0.046180599443284814\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.04059691341890858 | Validation loss: 0.05154817353571411\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.04208160265473517 | Validation loss: 0.05156806101445603\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.0406858466682941 | Validation loss: 0.05118079272519171\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.039154747262685086 | Validation loss: 0.04794440525625132\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03741198778152466 | Validation loss: 0.045822479308838336\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03600720611793231 | Validation loss: 0.041700294241309166\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.034843648902111224 | Validation loss: 0.03910587075273547\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.03538154784450674 | Validation loss: 0.041343334930396713\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.036538641037250776 | Validation loss: 0.04486508937799825\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03746334522573908 | Validation loss: 0.04573550967054557\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03906626951345426 | Validation loss: 0.04793903403050077\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.04067838922652643 | Validation loss: 0.04968288084244834\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03920144755136603 | Validation loss: 0.04930177286465084\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03762676272317621 | Validation loss: 0.05426667930673709\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.0362003557594845 | Validation loss: 0.045433557897278695\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.034699298914404604 | Validation loss: 0.041561927959586666\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.03369743091248592 | Validation loss: 0.038345360271303\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.034240884567046256 | Validation loss: 0.04051130935879408\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.03505190609088974 | Validation loss: 0.04553813698281229\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.036325072192249626 | Validation loss: 0.04458336504093841\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.038167144151727166 | Validation loss: 0.05034423378848397\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.03953559986795995 | Validation loss: 0.04722861444528124\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.038086684345539044 | Validation loss: 0.04692284245274763\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.036556421646180996 | Validation loss: 0.042711537298375526\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.035050427166224404 | Validation loss: 0.04218942583002876\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.033719378285198 | Validation loss: 0.04147126843773686\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.03284708765889041 | Validation loss: 0.03807422106640529\n",
      "Validation loss (ends of cycles): [0.20993191 0.057927   0.05087756 0.0471809  0.04485162 0.04335153\n",
      " 0.04154873 0.03996461 0.03910587 0.03834536 0.03807422]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 500\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.28623229150588697 | Validation loss: 0.2876668721437454\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.2732484845014719 | Validation loss: 0.2794637233018875\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.24852608488156244 | Validation loss: 0.26532696187496185\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.21454166219784662 | Validation loss: 0.244778074324131\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.1760728319103901 | Validation loss: 0.227938711643219\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.13927876548125193 | Validation loss: 0.2178543582558632\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.1165735016648586 | Validation loss: 0.1460997760295868\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.1044723718212201 | Validation loss: 0.11919640749692917\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.09750367930302253 | Validation loss: 0.10581953823566437\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.09256000587573418 | Validation loss: 0.105472881346941\n",
      "Validation loss (ends of cycles): [0.28766687]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.08769601182295726 | Validation loss: 0.10440703853964806\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.08512237668037415 | Validation loss: 0.10394519194960594\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.08663272571105224 | Validation loss: 0.10312925651669502\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08722404648478214 | Validation loss: 0.10252366960048676\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08525520849686402 | Validation loss: 0.16640019416809082\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.09381065976161224 | Validation loss: 0.12001441791653633\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.09383911123642555 | Validation loss: 0.13745518773794174\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.09079675748944283 | Validation loss: 0.11658180505037308\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0856318806226437 | Validation loss: 0.10903191938996315\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.08159097685263707 | Validation loss: 0.10695850476622581\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.08112306835559699 | Validation loss: 0.10882006213068962\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.08342313222013988 | Validation loss: 0.11046990752220154\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.08023825918252651 | Validation loss: 0.10669990256428719\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.08050067350268364 | Validation loss: 0.11306899785995483\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.08110488578677177 | Validation loss: 0.10976016893982887\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0816067812534479 | Validation loss: 0.10279102995991707\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.08068692168364158 | Validation loss: 0.12279709428548813\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.07764218289118546 | Validation loss: 0.10749772936105728\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.07489511055442002 | Validation loss: 0.10868991166353226\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.07361829338165429 | Validation loss: 0.10824761912226677\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.07218144604792961 | Validation loss: 0.10803351551294327\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.07381307562956443 | Validation loss: 0.10806496813893318\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.0742212373476762 | Validation loss: 0.11222716048359871\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.07452364036670098 | Validation loss: 0.10684147849678993\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.07637788527286969 | Validation loss: 0.1159149706363678\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.0818782881475412 | Validation loss: 0.10239430144429207\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.07602285765684567 | Validation loss: 0.11685087531805038\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.07531523704528809 | Validation loss: 0.12537145614624023\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.07239608867810322 | Validation loss: 0.10199717059731483\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.06922513475784889 | Validation loss: 0.10814457386732101\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.06899523219236961 | Validation loss: 0.1046244278550148\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.06658756675628516 | Validation loss: 0.1013033427298069\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.06902540790346953 | Validation loss: 0.10858236253261566\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.06836868163484794 | Validation loss: 0.1024441346526146\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.07284962471861106 | Validation loss: 0.10861896350979805\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.07337845202821952 | Validation loss: 0.11208184063434601\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.07648678697072543 | Validation loss: 0.10931962355971336\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.06974261884505932 | Validation loss: 0.13788466900587082\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.06619262351439549 | Validation loss: 0.11117371916770935\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.06400956414066829 | Validation loss: 0.10756627842783928\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.06536509698400131 | Validation loss: 0.10626817867159843\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.06240379179899509 | Validation loss: 0.10504582896828651\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.0661077255812975 | Validation loss: 0.10221891477704048\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.06295406073331833 | Validation loss: 0.11193358525633812\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.06823232712653968 | Validation loss: 0.09892533719539642\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.06969980093149039 | Validation loss: 0.10674293711781502\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.07000684480254467 | Validation loss: 0.1276056095957756\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.07000288166678868 | Validation loss: 0.10634531080722809\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.06597055552097467 | Validation loss: 0.10050374269485474\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.06105187821846742 | Validation loss: 0.09869374707341194\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.06096246236791977 | Validation loss: 0.10020382329821587\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.06011223420500755 | Validation loss: 0.10638590157032013\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.06001588931450477 | Validation loss: 0.10041772574186325\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.05770640648328341 | Validation loss: 0.10693498328328133\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.06082365203362245 | Validation loss: 0.10736185312271118\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.06504335684271959 | Validation loss: 0.10069963335990906\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.06329144451480645 | Validation loss: 0.12001533806324005\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.060021109879016876 | Validation loss: 0.13025841116905212\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.05991259618447377 | Validation loss: 0.1112692691385746\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.051523296878888056 | Validation loss: 0.10381150618195534\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.05670847801061777 | Validation loss: 0.10544414073228836\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.05006153193803934 | Validation loss: 0.1040196605026722\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.053257113752456814 | Validation loss: 0.10596621409058571\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.052096135914325714 | Validation loss: 0.10098161175847054\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.05899054528428958 | Validation loss: 0.15640872716903687\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.06283879165466015 | Validation loss: 0.11133874207735062\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.06475053956875435 | Validation loss: 0.1269955411553383\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.05856200307607651 | Validation loss: 0.1232197992503643\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.0522953188763215 | Validation loss: 0.10317911207675934\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.050779332335178666 | Validation loss: 0.10657905414700508\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.05063657720501606 | Validation loss: 0.10991430655121803\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.046154147443863064 | Validation loss: 0.12377838045358658\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.04866070572573405 | Validation loss: 0.11358434334397316\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.04941498688780344 | Validation loss: 0.1076432652771473\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.05149089373075045 | Validation loss: 0.1143205277621746\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.054084719373629644 | Validation loss: 0.1442350260913372\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.0523872788135822 | Validation loss: 0.14174965769052505\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.049203543995435424 | Validation loss: 0.10664897784590721\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.04649185045407368 | Validation loss: 0.11035742238163948\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.04299872726775133 | Validation loss: 0.11943933367729187\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.04394056762640293 | Validation loss: 0.11714010313153267\n",
      "Validation loss (ends of cycles): [0.28766687 0.10440704 0.10882006 0.10803352 0.10462443 0.10626818\n",
      " 0.10020382 0.10544414 0.10991431 0.1171401 ]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 1000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.27114631694096786 | Validation loss: 0.3013191173473994\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.251861857680174 | Validation loss: 0.28458447754383087\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.21546687586949423 | Validation loss: 0.284496545791626\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.1699894041969226 | Validation loss: 0.21031304200490317\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.13458524501094452 | Validation loss: 0.08553920437892278\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.11567006145532314 | Validation loss: 0.14232793947060904\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.10996172233269765 | Validation loss: 0.10632145653168361\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.1118621499492572 | Validation loss: 0.08670407781998317\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.10521372522299106 | Validation loss: 0.09393906593322754\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.10288918677430886 | Validation loss: 0.08820493270953496\n",
      "Validation loss (ends of cycles): [0.30131912]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.10211293800519063 | Validation loss: 0.088161401450634\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.10041159792588307 | Validation loss: 0.08792162934939067\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.10152667130415256 | Validation loss: 0.08688186854124069\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.10256221446280296 | Validation loss: 0.10419053584337234\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.1035890977543134 | Validation loss: 0.0830913856625557\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.10162635978597861 | Validation loss: 0.10780499627192815\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.10166545398533344 | Validation loss: 0.09875310709079106\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.10068163562279481 | Validation loss: 0.11237562447786331\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.09774628596810195 | Validation loss: 0.08589743822813034\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.09629813982890202 | Validation loss: 0.08379857987165451\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614 ]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.0934074344829871 | Validation loss: 0.08416049679120381\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.09412454641782321 | Validation loss: 0.08251916865507762\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.09572084749547335 | Validation loss: 0.08353349069754283\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.09290660158372842 | Validation loss: 0.0896812950571378\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.09400351259570855 | Validation loss: 0.11157957216103871\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.09808041814428109 | Validation loss: 0.09437836209932964\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.09618092586214726 | Validation loss: 0.07646445681651433\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.09161735111131118 | Validation loss: 0.08044230192899704\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.08888172100369747 | Validation loss: 0.08159844825665157\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.08454665813881618 | Validation loss: 0.0776438241203626\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605 ]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.08237259147258905 | Validation loss: 0.07378115753332774\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.08362751325162557 | Validation loss: 0.07990996291240056\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.083226439757989 | Validation loss: 0.10914879540602367\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.08491195523394988 | Validation loss: 0.07139277209838231\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.08520169427188543 | Validation loss: 0.08888460695743561\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.0905728626709718 | Validation loss: 0.08219081163406372\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.08954568350544342 | Validation loss: 0.07995042701562245\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.08245866330197224 | Validation loss: 0.10690229882796605\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.08004418946802616 | Validation loss: 0.0655438502629598\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.07748245619810544 | Validation loss: 0.07324954122304916\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.07241358321446639 | Validation loss: 0.06766582901279132\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.07349490660887498 | Validation loss: 0.08220083763202031\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.07459167777918853 | Validation loss: 0.06816854948798816\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.075133565813303 | Validation loss: 0.06550999979178111\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.07938563565795238 | Validation loss: 0.17636031409104666\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.08100382424890995 | Validation loss: 0.14928263425827026\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.08009787734884483 | Validation loss: 0.19112283488114676\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.07215518289460586 | Validation loss: 0.06750130653381348\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.06909940305810708 | Validation loss: 0.06105370571215948\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.06814099848270416 | Validation loss: 0.06708964208761851\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.0647653667972638 | Validation loss: 0.06590079019467036\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.06436607255958594 | Validation loss: 0.06134747465451559\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.06368346626941974 | Validation loss: 0.05555027723312378\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.06528307368549016 | Validation loss: 0.10233555485804875\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.06762558365097412 | Validation loss: 0.06919782360394795\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.0683538823460157 | Validation loss: 0.09502405176560084\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.07198400795459747 | Validation loss: 0.14405548572540283\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.06737143417390493 | Validation loss: 0.07190001259247462\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.06260795833972785 | Validation loss: 0.062167766193548836\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.05951648993560901 | Validation loss: 0.07571763545274734\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.05508949851187376 | Validation loss: 0.0668637715280056\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.052382624636475854 | Validation loss: 0.09922036528587341\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.056416381580325276 | Validation loss: 0.09052975972493489\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.057161668745371014 | Validation loss: 0.07923702895641327\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.06068163279157419 | Validation loss: 0.07965557028849919\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.06255157041148497 | Validation loss: 0.30685415863990784\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.06131344331571689 | Validation loss: 0.0656268410384655\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.06062774546444416 | Validation loss: 0.08304284016291301\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.05327921833556432 | Validation loss: 0.14299276967843375\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.05078572436020924 | Validation loss: 0.06972592696547508\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.04483967346067612 | Validation loss: 0.07364171991745631\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.047769702469500214 | Validation loss: 0.08219071726004283\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.04863498230966238 | Validation loss: 0.15322100122769675\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.04958262351843027 | Validation loss: 0.08630643784999847\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.05238688429101156 | Validation loss: 0.12523938218752542\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.05482054688036442 | Validation loss: 0.07240167011817296\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.05516412023168344 | Validation loss: 0.08454832683006923\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.05065158023857153 | Validation loss: 0.08039050673445065\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.047002577007963106 | Validation loss: 0.12330764532089233\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.04479805251153616 | Validation loss: 0.06863002230723698\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.042003152605432734 | Validation loss: 0.06680252775549889\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.040736994276253075 | Validation loss: 0.06924128532409668\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.04364607404344357 | Validation loss: 0.13677890847126642\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.04239509634387035 | Validation loss: 0.06394252677758534\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.048944419536453024 | Validation loss: 0.12318740785121918\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.050307344358700976 | Validation loss: 0.07770912845929463\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.0510884178085969 | Validation loss: 0.12004852046569188\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.044719476682635456 | Validation loss: 0.08277116964260738\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.0436299704015255 | Validation loss: 0.09767195334037145\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.038958583815166585 | Validation loss: 0.06731322159369786\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.03566374486455551 | Validation loss: 0.06604948143164317\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.03600069758697198 | Validation loss: 0.06159008666872978\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.03836567363200279 | Validation loss: 0.0633199227352937\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.037643212968340285 | Validation loss: 0.12101396421591441\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.04071113252295898 | Validation loss: 0.09344382832447688\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.044794661566041984 | Validation loss: 0.16241360704104105\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.04553248654477871 | Validation loss: 0.087947316467762\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.04601407094070545 | Validation loss: 0.06197854007283846\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.037852445259117164 | Validation loss: 0.07366656263669331\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.03565156388168152 | Validation loss: 0.0590630459288756\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.03224476340871591 | Validation loss: 0.0645342580974102\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.03184994045071877 | Validation loss: 0.06282762189706166\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.032962807740729586 | Validation loss: 0.06401825199524562\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.0340998714359907 | Validation loss: 0.07556230823198955\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.03651317825111059 | Validation loss: 0.07191591709852219\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03656492310647781 | Validation loss: 0.1600353717803955\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.04045305587351322 | Validation loss: 0.08703135202328365\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.03643202502280474 | Validation loss: 0.07275040199359258\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.03348856677229588 | Validation loss: 0.06988890593250592\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.030310515171060197 | Validation loss: 0.061166045566399894\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.027753474978873365 | Validation loss: 0.06391521046559016\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.02818346696977432 | Validation loss: 0.05979646369814873\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.02923572829996164 | Validation loss: 0.0680171325802803\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.030005195894493505 | Validation loss: 0.06576111788551013\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.03573801576231535 | Validation loss: 0.056423703829447426\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.036100911119809516 | Validation loss: 0.07376208404699962\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.03690812213776203 | Validation loss: 0.0762839342157046\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.03343923941541176 | Validation loss: 0.061920154839754105\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.02978512293730791 | Validation loss: 0.07674552748600642\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.029101334512233734 | Validation loss: 0.06387490406632423\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.026124813271543153 | Validation loss: 0.06534932677944501\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.025870294095231935 | Validation loss: 0.06444216519594193\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.025658548558847263 | Validation loss: 0.06507259979844093\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.028533661523117468 | Validation loss: 0.06774731601277988\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.02891709805967716 | Validation loss: 0.0724047174056371\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.03162456948596697 | Validation loss: 0.06366992741823196\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.03268601781187149 | Validation loss: 0.0641860527296861\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.03037001765691317 | Validation loss: 0.07524159799019496\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.02665410556185704 | Validation loss: 0.06862633923689525\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.02404931803735403 | Validation loss: 0.06612421944737434\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.023632952441962864 | Validation loss: 0.06416553010543187\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.023884588565963965 | Validation loss: 0.06173599263032278\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.02424594580840606 | Validation loss: 0.06624979277451833\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.025196982762561396 | Validation loss: 0.061767312387625374\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.02775773348716589 | Validation loss: 0.09692556907733281\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.032594263123778194 | Validation loss: 0.08224088450272878\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.029487412207974836 | Validation loss: 0.07490686575571696\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.02741387515113904 | Validation loss: 0.06623867899179459\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.027188565940237962 | Validation loss: 0.06423299262921016\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.022550587363254566 | Validation loss: 0.06976987918217976\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.022771345952955577 | Validation loss: 0.06576516106724739\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.02289973869203375 | Validation loss: 0.0628873569269975\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.023526311493836917 | Validation loss: 0.07104157408078511\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.024283254232544165 | Validation loss: 0.08539532621701558\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.026763396409268562 | Validation loss: 0.06554212172826131\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.03026583532874401 | Validation loss: 0.11454411596059799\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.028293991676316813 | Validation loss: 0.08389495313167572\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.02770799584686756 | Validation loss: 0.06585045903921127\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.023221183926440202 | Validation loss: 0.06650333975752194\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.02247353306470009 | Validation loss: 0.08126651247342427\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.019623086751940157 | Validation loss: 0.06778747588396072\n",
      "Validation loss (ends of cycles): [0.30131912 0.0881614  0.0841605  0.07378116 0.06766583 0.06590079\n",
      " 0.06686377 0.07364172 0.06680253 0.06604948 0.06453426 0.06391521\n",
      " 0.06534933 0.06416553 0.06576516 0.06778748]\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 2000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.1618801434250439 | Validation loss: 0.1752209154268106\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.14574233997686237 | Validation loss: 0.15481133138140044\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.11868300873274897 | Validation loss: 0.11752228438854218\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.10331503433339737 | Validation loss: 0.11742908507585526\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.09970792346433097 | Validation loss: 0.10259843369325002\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.09703811111987806 | Validation loss: 0.103461354970932\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.09149395820556902 | Validation loss: 0.10043870781858762\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.09101413066188495 | Validation loss: 0.12704111635684967\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.08387162901607215 | Validation loss: 0.10820480187733968\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.0832572073913088 | Validation loss: 0.1032949797809124\n",
      "Validation loss (ends of cycles): [0.17522092]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.07924356764438105 | Validation loss: 0.09691472351551056\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.08008078003630918 | Validation loss: 0.0957989643017451\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.08209384254672948 | Validation loss: 0.11568646629651387\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08243749655929267 | Validation loss: 0.09991084039211273\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.08606560250707701 | Validation loss: 0.1379929060737292\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.0824899130738249 | Validation loss: 0.09170163857440154\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.0803560282961995 | Validation loss: 0.09212791919708252\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07607325809259041 | Validation loss: 0.09057196353872617\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07439241616749297 | Validation loss: 0.09052857694526513\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.07024018408036699 | Validation loss: 0.09308399073779583\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06798860613329738 | Validation loss: 0.08868985312680404\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06862006760111042 | Validation loss: 0.09170038687686126\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06869713041712255 | Validation loss: 0.08607339983185132\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.07040536199130264 | Validation loss: 0.08300078163544337\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06987133454166207 | Validation loss: 0.09090227695802848\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0716366690090474 | Validation loss: 0.09582946076989174\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06975229413193815 | Validation loss: 0.08876596515377362\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06483151796547805 | Validation loss: 0.08653237794836362\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06122077154178245 | Validation loss: 0.08586440918346246\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.059484159245210534 | Validation loss: 0.09282346442341805\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.05785673750820113 | Validation loss: 0.0816469881683588\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.0590196952004643 | Validation loss: 0.09680157403151195\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.05919191201089644 | Validation loss: 0.08778925302127998\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.05938986396672679 | Validation loss: 0.09211701527237892\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.061850913380290945 | Validation loss: 0.0910467089464267\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.06223887621479876 | Validation loss: 0.10479206964373589\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.06194849370741377 | Validation loss: 0.08416965107123058\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.05916060693562031 | Validation loss: 0.09831625409424305\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.05644529353023744 | Validation loss: 0.0976157213250796\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.05409847791580593 | Validation loss: 0.07766535754005115\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.05131756507006346 | Validation loss: 0.07610398282607396\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.05128737677838288 | Validation loss: 0.07781509744624297\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.052724012411108204 | Validation loss: 0.07516794838011265\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.05487711676487736 | Validation loss: 0.10514147579669952\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.054588435363827965 | Validation loss: 0.08979047338167827\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.05867484726888292 | Validation loss: 0.094602112347881\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.056937527313244106 | Validation loss: 0.08283252144853274\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.05434086024030751 | Validation loss: 0.11367898931105931\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.05096290885087322 | Validation loss: 0.08029752473036449\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.049206791336045545 | Validation loss: 0.07943053916096687\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.046280208025492875 | Validation loss: 0.07776264660060406\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.045652902703367026 | Validation loss: 0.079695966715614\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.04672223851815158 | Validation loss: 0.07726358249783516\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.048943011930175855 | Validation loss: 0.10645054529110591\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.050548168921879695 | Validation loss: 0.08036942842106025\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.052031887001266666 | Validation loss: 0.10596027101079623\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.053062821793205595 | Validation loss: 0.10486301655570666\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.049803431091063166 | Validation loss: 0.10336205114920934\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.04522683836665808 | Validation loss: 0.09069225067893665\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.04493647981800285 | Validation loss: 0.07809621468186378\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.04065243574772395 | Validation loss: 0.07773153235514958\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.04221714835833101 | Validation loss: 0.07856796185175578\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.04279230407201776 | Validation loss: 0.08424294305344422\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.04577502148116336 | Validation loss: 0.09170131136973698\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.046164985600055435 | Validation loss: 0.08783026970922947\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.047662596930475795 | Validation loss: 0.08518970385193825\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.04445842315680256 | Validation loss: 0.07751050901909669\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.043508671991088814 | Validation loss: 0.08918310577670734\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.04179470343332665 | Validation loss: 0.09014023592074712\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.04049620671453429 | Validation loss: 0.07655073702335358\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.0396222028443042 | Validation loss: 0.07584572024643421\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03717943269978551 | Validation loss: 0.07880508278807004\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03973317442133146 | Validation loss: 0.08252802553276221\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.04049243540594391 | Validation loss: 0.09000153529147308\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.04049573047999658 | Validation loss: 0.10002372041344643\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.044400962179197985 | Validation loss: 0.07380459271371365\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.043414482737288755 | Validation loss: 0.07823999598622322\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.038739253839879646 | Validation loss: 0.08262713812291622\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03809597438164786 | Validation loss: 0.08333441180487473\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.035754169465280046 | Validation loss: 0.08620268727342288\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.03467226956112712 | Validation loss: 0.07951171075304349\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.03466332311212432 | Validation loss: 0.07921988517045975\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.03592760305778653 | Validation loss: 0.0850036001453797\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.036594210959532684 | Validation loss: 0.07627586275339127\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.04111481717258107 | Validation loss: 0.07233001788457234\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.040537830971765755 | Validation loss: 0.09824792668223381\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03904353648278059 | Validation loss: 0.08698291952411334\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03920765275902608 | Validation loss: 0.0835754635433356\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.03601997488123529 | Validation loss: 0.08621295168995857\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.03377638346351245 | Validation loss: 0.07983815545837085\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.0327324680801408 | Validation loss: 0.0762726366519928\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.03165368520307774 | Validation loss: 0.08496354147791862\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.03344015656586956 | Validation loss: 0.07823167617122333\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.03262264915175882 | Validation loss: 0.0848035675783952\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.03703135953230016 | Validation loss: 0.07858358137309551\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.03976100869476795 | Validation loss: 0.10528316410879295\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.03727892081381059 | Validation loss: 0.07412890965739886\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03417914097799974 | Validation loss: 0.07945749287803967\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.0325212050229311 | Validation loss: 0.07739517899851005\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.03150020338887093 | Validation loss: 0.08356203138828278\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.029827139632520722 | Validation loss: 0.07726814846197765\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.029131623405013598 | Validation loss: 0.07960248303910096\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.029651211191187885 | Validation loss: 0.077345651263992\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.03169410891246562 | Validation loss: 0.07760117140909036\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.03362071163514081 | Validation loss: 0.08115199704964955\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03455840410920335 | Validation loss: 0.09261715412139893\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.034202438523518106 | Validation loss: 0.08733148748675983\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.031660995080920996 | Validation loss: 0.08463405569394429\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.030696076669675464 | Validation loss: 0.07534050444761912\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.028993837371030274 | Validation loss: 0.08378856008251508\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.027884817408288225 | Validation loss: 0.07827197946608067\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.026954872278021832 | Validation loss: 0.0768817663192749\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.027306516892185398 | Validation loss: 0.07335130125284195\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.02984478463437043 | Validation loss: 0.07736792415380478\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.030624039343320857 | Validation loss: 0.07951478908459346\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.03410996135105105 | Validation loss: 0.09334786732991536\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.03268755740467824 | Validation loss: 0.08859992027282715\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.0311201336592728 | Validation loss: 0.07669213910897572\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.02850773309667905 | Validation loss: 0.076536084835728\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.026045327659185026 | Validation loss: 0.07779801202317078\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.025879897213741846 | Validation loss: 0.07725021553536256\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 121\n",
      "Training loss: 0.025006736263486685 | Validation loss: 0.08035719518860181\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 122\n",
      "Training loss: 0.02521455225845178 | Validation loss: 0.07811126050849755\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 123\n",
      "Training loss: 0.02737539444191783 | Validation loss: 0.07882043470939\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 124\n",
      "Training loss: 0.02847288306072062 | Validation loss: 0.08231854687134425\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 125\n",
      "Training loss: 0.03284748344152581 | Validation loss: 0.0926503340403239\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 126\n",
      "Training loss: 0.029583644267975117 | Validation loss: 0.09233872592449188\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 127\n",
      "Training loss: 0.02809212899164242 | Validation loss: 0.08092666106919448\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 128\n",
      "Training loss: 0.02684425281397268 | Validation loss: 0.07736630427340667\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 129\n",
      "Training loss: 0.026525435640531426 | Validation loss: 0.08286652341485023\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022]\n",
      "------------------------------\n",
      "Epoch: 130\n",
      "Training loss: 0.026220443165477586 | Validation loss: 0.07740283012390137\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 131\n",
      "Training loss: 0.024342383404134537 | Validation loss: 0.08926018575827281\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 132\n",
      "Training loss: 0.025032186953752648 | Validation loss: 0.09302532176176707\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 133\n",
      "Training loss: 0.0252848844244784 | Validation loss: 0.07852401832739513\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 134\n",
      "Training loss: 0.02585682207170655 | Validation loss: 0.08621748288472493\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 135\n",
      "Training loss: 0.027297223589437848 | Validation loss: 0.09372994552055995\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 136\n",
      "Training loss: 0.029613328915016324 | Validation loss: 0.07932470242182414\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 137\n",
      "Training loss: 0.027742074400770898 | Validation loss: 0.08915846919020017\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 138\n",
      "Training loss: 0.024932185973168586 | Validation loss: 0.09161436433593433\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 139\n",
      "Training loss: 0.02556919655306082 | Validation loss: 0.09213371202349663\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283]\n",
      "------------------------------\n",
      "Epoch: 140\n",
      "Training loss: 0.024974070051137137 | Validation loss: 0.07853892631828785\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 141\n",
      "Training loss: 0.022545989748894 | Validation loss: 0.09043761218587558\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 142\n",
      "Training loss: 0.02345672288142583 | Validation loss: 0.08282394955555598\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 143\n",
      "Training loss: 0.024325473727110553 | Validation loss: 0.11688203240434329\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 144\n",
      "Training loss: 0.026514568840902225 | Validation loss: 0.09078371028105418\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 145\n",
      "Training loss: 0.027508027358528447 | Validation loss: 0.08296988283594449\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 146\n",
      "Training loss: 0.027422879256454168 | Validation loss: 0.11777617782354355\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 147\n",
      "Training loss: 0.024141497910022736 | Validation loss: 0.09811192005872726\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 148\n",
      "Training loss: 0.024877404100170322 | Validation loss: 0.0829291803141435\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 149\n",
      "Training loss: 0.02423349623659662 | Validation loss: 0.08197802864015102\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893]\n",
      "------------------------------\n",
      "Epoch: 150\n",
      "Training loss: 0.02233779888746201 | Validation loss: 0.08017101387182872\n",
      "Validation loss (ends of cycles): [0.17522092 0.09691472 0.08868985 0.08164699 0.07610398 0.07776265\n",
      " 0.07773153 0.07584572 0.07951171 0.07627264 0.07726815 0.07827198\n",
      " 0.07725022 0.07740283 0.07853893 0.08017101]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 5000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.21679930271595482 | Validation loss: 0.20318472385406494\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.1668136041230104 | Validation loss: 0.11816636696457863\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.1115807911131795 | Validation loss: 0.11339729763567448\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.10118959852912295 | Validation loss: 0.12549301534891127\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.09859782768281426 | Validation loss: 0.10448834672570229\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.09670952816061147 | Validation loss: 0.10199640865127245\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.09333653369639802 | Validation loss: 0.1617287278175354\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.090622306662047 | Validation loss: 0.0932340698937575\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.08455809977228247 | Validation loss: 0.08920024012525876\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.08212190136078774 | Validation loss: 0.08311140177150568\n",
      "Validation loss (ends of cycles): [0.20318472]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.0791853968728715 | Validation loss: 0.08080732847253481\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.07991005939350823 | Validation loss: 0.08217759616672993\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.07952956224637707 | Validation loss: 0.09287703956166903\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.08107227450750006 | Validation loss: 0.09761275698741277\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.0815588255214879 | Validation loss: 0.09132462789614995\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.08069409350827923 | Validation loss: 0.1021335686246554\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07824799048853671 | Validation loss: 0.09792800272504489\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07535720156051043 | Validation loss: 0.09600358655055365\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07134035119684193 | Validation loss: 0.07297346604367097\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06730523676149489 | Validation loss: 0.06935091987252236\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06681856215293483 | Validation loss: 0.06999336344500383\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06591901843120733 | Validation loss: 0.0713375985622406\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06646224384115437 | Validation loss: 0.07043899595737457\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06865779084600801 | Validation loss: 0.0846476435661316\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06966026428001602 | Validation loss: 0.08409541994333267\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.06914794953965296 | Validation loss: 0.07454115003347397\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06687090094164601 | Validation loss: 0.12878460387388865\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.065276972186847 | Validation loss: 0.08846696764230728\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06052851867605382 | Validation loss: 0.06936634952823321\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05739320233816237 | Validation loss: 0.06372248244782289\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.056034141623481054 | Validation loss: 0.06305947179595629\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.05621948521437607 | Validation loss: 0.0632335669050614\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.05801960817120207 | Validation loss: 0.06638292744755744\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.059636385510052285 | Validation loss: 0.07299174045523008\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.06075522271314944 | Validation loss: 0.06601005693276724\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.06310464410976631 | Validation loss: 0.07540631766120592\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.061312416993726894 | Validation loss: 0.0751885324716568\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.05711426153251036 | Validation loss: 0.07411545490225156\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.05412266294904581 | Validation loss: 0.06427605276306471\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.05176926941031546 | Validation loss: 0.060552603627244635\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.0486671953777394 | Validation loss: 0.06134359017014503\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.05110002699212765 | Validation loss: 0.05918064539631208\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.051530711455490645 | Validation loss: 0.060740908980369566\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.05298666653084004 | Validation loss: 0.07054131577412287\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.05645980859013993 | Validation loss: 0.07371640031536421\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.05717385922536606 | Validation loss: 0.08709836999575298\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.05534336075010732 | Validation loss: 0.0652000146607558\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.05249954535677208 | Validation loss: 0.06557390540838241\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.04942367346150669 | Validation loss: 0.0619909405708313\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.04723699968748205 | Validation loss: 0.05703059149285158\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.04512434300240569 | Validation loss: 0.057147355874379475\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.04565585554232748 | Validation loss: 0.058827751750747365\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.04680207665041676 | Validation loss: 0.06423094595472018\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.04919423680664517 | Validation loss: 0.07155880083640416\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.05087821399719696 | Validation loss: 0.06875351990262667\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.053621325627204 | Validation loss: 0.12530262917280197\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.05069674629219404 | Validation loss: 0.0649179848531882\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.04963773845394296 | Validation loss: 0.0683489166200161\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.046637567097511816 | Validation loss: 0.05715753951420387\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.04308643450183192 | Validation loss: 0.05415662241478761\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.04191293368688014 | Validation loss: 0.05487368342777093\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.04219510956982693 | Validation loss: 0.056085166583458586\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.04427421400983503 | Validation loss: 0.061968022212386134\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.045907203914729626 | Validation loss: 0.06518676156798998\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.04781815905387946 | Validation loss: 0.07564251323541006\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.04938608843569211 | Validation loss: 0.08950053478280703\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.047823689848653914 | Validation loss: 0.05974354532857736\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.0452634461928071 | Validation loss: 0.05870025667051474\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.042983035578971776 | Validation loss: 0.0612156942486763\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.040601052402511356 | Validation loss: 0.055788303911685946\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.03814817611568087 | Validation loss: 0.05492573517064254\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03847261846769513 | Validation loss: 0.05461364078025023\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03995766030169848 | Validation loss: 0.05567261067529519\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.04270468235719861 | Validation loss: 0.06666113883256912\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.043942760514814085 | Validation loss: 0.059602606544891995\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.04800796254325335 | Validation loss: 0.06048930970331033\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.044324284302085404 | Validation loss: 0.058276225626468656\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.04246668216193051 | Validation loss: 0.06340857942899068\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.039575646829417374 | Validation loss: 0.06758413364489874\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.037527156377753876 | Validation loss: 0.0539743257065614\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.03588872146976041 | Validation loss: 0.05481154695153236\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.03649109230912107 | Validation loss: 0.054990808169047035\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.03719184185400253 | Validation loss: 0.06046265438199043\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03993967960141306 | Validation loss: 0.05727590061724186\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.040751541143385916 | Validation loss: 0.06313362121582031\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.04366428489378822 | Validation loss: 0.06349676909546058\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.04226917906950309 | Validation loss: 0.056340149914224945\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03940869762202886 | Validation loss: 0.06956625630458196\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.037179867033003355 | Validation loss: 0.056858163326978683\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.03474178528926504 | Validation loss: 0.054771875590085985\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.03365579730735754 | Validation loss: 0.05424569162229697\n",
      "Validation loss (ends of cycles): [0.20318472 0.08080733 0.06999336 0.06305947 0.06134359 0.05714736\n",
      " 0.05487368 0.05492574 0.05481155 0.05424569]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 10000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.3208344729514573 | Validation loss: 0.318280567382944\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.19803376548637555 | Validation loss: 0.13973277755852404\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.10369020112328173 | Validation loss: 0.1150948526016597\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.09644094917659216 | Validation loss: 0.10184345497139569\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.09403398349939839 | Validation loss: 0.1028368263665972\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.09193474704062375 | Validation loss: 0.1109759640590898\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.08814766762528832 | Validation loss: 0.09330829733918453\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.0837278733484623 | Validation loss: 0.08856733438783679\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.08128497258239374 | Validation loss: 0.08120295495308678\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.07837615451064166 | Validation loss: 0.07797302755302396\n",
      "Validation loss (ends of cycles): [0.31828057]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.07498718809893751 | Validation loss: 0.07726067600065264\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.07560104029033128 | Validation loss: 0.07749038266724553\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.07756251360722415 | Validation loss: 0.08580444444870126\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.07727028810837137 | Validation loss: 0.0858236273814892\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.07767378302716364 | Validation loss: 0.08244718315786329\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.07831902486660819 | Validation loss: 0.08229162158637211\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.07570499881339354 | Validation loss: 0.07988743625324347\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.07201992783431463 | Validation loss: 0.07582339567357096\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.07011749405472532 | Validation loss: 0.07034371587736853\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.06716613902703045 | Validation loss: 0.0669062534539864\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.06499362923938223 | Validation loss: 0.06683127569227383\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.06468080149919499 | Validation loss: 0.06583795465272048\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.06689566351825327 | Validation loss: 0.06987852204976411\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.06782534108387203 | Validation loss: 0.07301809394667888\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.06932942601522123 | Validation loss: 0.07833132944230375\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.0697620217138388 | Validation loss: 0.07771215652083528\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.06711326695714645 | Validation loss: 0.07463829681791108\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.06510572683975452 | Validation loss: 0.07238342150531966\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.06255879405680603 | Validation loss: 0.06996388707695336\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.05977157332501778 | Validation loss: 0.06343879846149478\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.057576970637255295 | Validation loss: 0.06144785264442707\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.05884648127642673 | Validation loss: 0.061254425552384605\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.06008706400244255 | Validation loss: 0.06730351057545893\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.06149038758979538 | Validation loss: 0.06669318663149044\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.06343353877768038 | Validation loss: 0.08957863676136937\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.06404111976551963 | Validation loss: 0.07944964103657624\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.06280158764470047 | Validation loss: 0.08468614403029968\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.06029054083986076 | Validation loss: 0.06408045762057962\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.05761603463265136 | Validation loss: 0.05974189076444198\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.05507340776462724 | Validation loss: 0.05976667499233936\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.053342985795942814 | Validation loss: 0.05916541262433447\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.054149042385974976 | Validation loss: 0.059428418761697306\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.05521612206664611 | Validation loss: 0.06161621251496775\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.05741074803013971 | Validation loss: 0.07564911323374715\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.059064230778965894 | Validation loss: 0.08295242616842533\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.06096260105149718 | Validation loss: 0.0736706999355349\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.058049106735765466 | Validation loss: 0.08584050705720639\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.05583139895424834 | Validation loss: 0.06207926982435687\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.053796297921909125 | Validation loss: 0.06523714240255027\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.05198358270654997 | Validation loss: 0.05872965193000333\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.0499456028314674 | Validation loss: 0.05686888162945879\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.05126144335876534 | Validation loss: 0.056591979195845535\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.05224344710724091 | Validation loss: 0.05856672815721611\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.054411778094496314 | Validation loss: 0.07042139074925718\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.055745888523405465 | Validation loss: 0.06894256778318307\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.05707524980117721 | Validation loss: 0.06330927410002413\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.05516272258952143 | Validation loss: 0.06743599166130197\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.05250089465985148 | Validation loss: 0.06211624667048454\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.051365907509319894 | Validation loss: 0.06060262486852448\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.04909465120973315 | Validation loss: 0.05675130376014216\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.04706054231826597 | Validation loss: 0.05575629599906247\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.04808880996296373 | Validation loss: 0.05616617549596162\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.04886479155109154 | Validation loss: 0.05892433553677181\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.05041125721085494 | Validation loss: 0.06897250832668667\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.05188799403813176 | Validation loss: 0.062462263580026295\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.054635593799624856 | Validation loss: 0.08355555824678519\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.0523650164781945 | Validation loss: 0.06177988340114725\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.051539598399494575 | Validation loss: 0.05858155166537597\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.04865856434211252 | Validation loss: 0.058884155133674884\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.04666972260071537 | Validation loss: 0.05566656884962115\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563 ]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.044823936340729086 | Validation loss: 0.054462554125950254\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.0449102942824481 | Validation loss: 0.054748651667915545\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.046292417118047165 | Validation loss: 0.059005388540440594\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.048082990483857516 | Validation loss: 0.05723131852674073\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.04956973357287448 | Validation loss: 0.05968912909257001\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.051790501929702255 | Validation loss: 0.05894576591150514\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.049998988016090526 | Validation loss: 0.059786687123364414\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.048110113376531545 | Validation loss: 0.061614472932856656\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.04663481412631437 | Validation loss: 0.06338580736312373\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.04410836506473619 | Validation loss: 0.05611117794338999\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.04312163801849123 | Validation loss: 0.05400283981500001\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.043554561714986416 | Validation loss: 0.056934723437860095\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.044636147390083064 | Validation loss: 0.0660295981014597\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.0459298248894102 | Validation loss: 0.06809257998548705\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.04766980750764918 | Validation loss: 0.06455176233731467\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.04948333185899446 | Validation loss: 0.08151760183531663\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.04826224101780672 | Validation loss: 0.07454486461035137\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.04570534480662327 | Validation loss: 0.058246461867258466\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.04386360907384495 | Validation loss: 0.058009152525457845\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.04169096091512855 | Validation loss: 0.054397762720954826\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.041218897184782374 | Validation loss: 0.05259814074841039\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.040955901549263735 | Validation loss: 0.05439365038584019\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.042126013860693125 | Validation loss: 0.054666623344709134\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.043382279897534 | Validation loss: 0.054504195998968746\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.0458415110822855 | Validation loss: 0.07741936720136938\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.04743661339886076 | Validation loss: 0.05820449994042002\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.04553815280270623 | Validation loss: 0.08208692163742821\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.044244329628395286 | Validation loss: 0.05583971597511193\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.04123691922625688 | Validation loss: 0.05591851151708899\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.04009729196266161 | Validation loss: 0.05433603845022876\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.03822168787780005 | Validation loss: 0.053116938934243955\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.038870895644429866 | Validation loss: 0.054241204441621386\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.04008115804937529 | Validation loss: 0.055827350130882754\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.041735652746178034 | Validation loss: 0.057072933634807324\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.043364248979162044 | Validation loss: 0.05602561297087834\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.045468564585261925 | Validation loss: 0.056964905804087376\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.044400842112349716 | Validation loss: 0.05765183186479684\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.04242724046023108 | Validation loss: 0.05811212301768105\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.03993679110811451 | Validation loss: 0.05642743761940249\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.03817879260099662 | Validation loss: 0.05428962120465163\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.03728278963569933 | Validation loss: 0.05238519949388915\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 111\n",
      "Training loss: 0.0370556244765973 | Validation loss: 0.05351860776286701\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 112\n",
      "Training loss: 0.03803750030755058 | Validation loss: 0.05330655434779052\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 113\n",
      "Training loss: 0.03953701080974397 | Validation loss: 0.05610055540656221\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 114\n",
      "Training loss: 0.042156051558361746 | Validation loss: 0.056494539603590965\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 115\n",
      "Training loss: 0.04371086021107951 | Validation loss: 0.059335493065159894\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 116\n",
      "Training loss: 0.04238700263172857 | Validation loss: 0.0607527428916816\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 117\n",
      "Training loss: 0.04007319592702107 | Validation loss: 0.059337090572406506\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 118\n",
      "Training loss: 0.03805773765376703 | Validation loss: 0.059619187216820385\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 119\n",
      "Training loss: 0.036783665865953045 | Validation loss: 0.05485150924530523\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852 ]\n",
      "------------------------------\n",
      "Epoch: 120\n",
      "Training loss: 0.03522829567705552 | Validation loss: 0.05326138205569366\n",
      "Validation loss (ends of cycles): [0.31828057 0.07726068 0.06683128 0.06144785 0.05916541 0.05686888\n",
      " 0.0557563  0.05446255 0.05400284 0.05259814 0.05311694 0.0523852\n",
      " 0.05326138]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 20000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.1937587873558321 | Validation loss: 0.16695564253288403\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.10741124820162559 | Validation loss: 0.09033931210114245\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.0909428682097434 | Validation loss: 0.08505091312945935\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.08447369038911731 | Validation loss: 0.08490002619331344\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.08043632089459803 | Validation loss: 0.0835269209193556\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07768446769146524 | Validation loss: 0.07836407364199036\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.07283868803413074 | Validation loss: 0.07246115702416814\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.06800214434134419 | Validation loss: 0.07295098345269237\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.06450905432627047 | Validation loss: 0.06473837368059576\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.06113856292056026 | Validation loss: 0.06293978251255396\n",
      "Validation loss (ends of cycles): [0.16695564]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05838155703843228 | Validation loss: 0.058973234833071105\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.0590651414707979 | Validation loss: 0.06184358401387407\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.06017314939147975 | Validation loss: 0.06425718737668112\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.061172162533482385 | Validation loss: 0.06329335121993433\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.06214155544043763 | Validation loss: 0.06571824806170505\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.062408381161928414 | Validation loss: 0.06477978142599265\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05940165142556267 | Validation loss: 0.06300883746722288\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05636550784772317 | Validation loss: 0.06063326899158327\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.053919580950598275 | Validation loss: 0.05690223664829606\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.05147737494249198 | Validation loss: 0.054520211787077416\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.049701670721114505 | Validation loss: 0.051985655927605796\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.050548364601191685 | Validation loss: 0.05337116988212393\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.05188654155949869 | Validation loss: 0.055069943040347936\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.05248021238522064 | Validation loss: 0.05857947808608674\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.05433177806554579 | Validation loss: 0.055786194933349624\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.05556651474852886 | Validation loss: 0.05825148323518142\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.053006139650015084 | Validation loss: 0.05651293652491611\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.0507659353499523 | Validation loss: 0.054801122726578465\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.048554422462421525 | Validation loss: 0.05305930501536319\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.04574381269117784 | Validation loss: 0.0514153082315859\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.04418007252991376 | Validation loss: 0.048620141879246945\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.045360717239111836 | Validation loss: 0.05021227791643979\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.046448042166200736 | Validation loss: 0.05336992553713029\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04778991164217099 | Validation loss: 0.05832194040218989\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.04944647156451581 | Validation loss: 0.0614949915100608\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.050366569427990115 | Validation loss: 0.06288488016447477\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.048872271721679315 | Validation loss: 0.05419730365668472\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04679563639711229 | Validation loss: 0.05351380425456323\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.04441320958236853 | Validation loss: 0.05135479059658552\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.042865205434916995 | Validation loss: 0.04892440996410554\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.04114158953405932 | Validation loss: 0.04640055317104908\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.04168075290407537 | Validation loss: 0.04898981227163683\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.04272683222339116 | Validation loss: 0.04995410621427653\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.04432621974813985 | Validation loss: 0.0549667902421533\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04593426139044338 | Validation loss: 0.05504765328869485\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.04727091159693588 | Validation loss: 0.054771556371920986\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.045680517350971936 | Validation loss: 0.05265137781960923\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.04370591644550392 | Validation loss: 0.05152795839597259\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.04130219880866229 | Validation loss: 0.05182074438453766\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.03948871645878053 | Validation loss: 0.04831022197347984\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03837553308303128 | Validation loss: 0.044728756146995646\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.03918741399484744 | Validation loss: 0.04656248438384449\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.040425572272470955 | Validation loss: 0.048068265806426085\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.041100266152585045 | Validation loss: 0.048309058506499254\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.043417033105647776 | Validation loss: 0.05213266104590474\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.044981646573784555 | Validation loss: 0.05113487236463187\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.04303855704661657 | Validation loss: 0.0511682754741949\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.04084054631395982 | Validation loss: 0.04861888168543054\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.038960600914745874 | Validation loss: 0.04695257093561323\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03764710921970521 | Validation loss: 0.04477690320396632\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.03553954229912403 | Validation loss: 0.043018419851075136\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03677014895959131 | Validation loss: 0.045161824052532516\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.03763243454073131 | Validation loss: 0.047717598740730366\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.039285847094255325 | Validation loss: 0.04886620066929282\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.041000166599302605 | Validation loss: 0.05117675080372576\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.042431575734778154 | Validation loss: 0.05887748026534131\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.04095806826559985 | Validation loss: 0.04888384812103029\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.03861006007105465 | Validation loss: 0.04978298546190847\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.03706407239862683 | Validation loss: 0.04655743958918672\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03537350038481652 | Validation loss: 0.044241009476153476\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.033808498057358596 | Validation loss: 0.04274106940679383\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03488694527048684 | Validation loss: 0.04366510390843216\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03567285149099206 | Validation loss: 0.04591469525506622\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.03710445848567305 | Validation loss: 0.048434995899074955\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.03909004668482897 | Validation loss: 0.04879440321472653\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.0402635611626788 | Validation loss: 0.049998423476752485\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03877175844933919 | Validation loss: 0.047474542403953116\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03686592971108839 | Validation loss: 0.049321869518934636\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.03513916212830259 | Validation loss: 0.04538330256023951\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03325431909195388 | Validation loss: 0.04405715048574565\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.032414121551464885 | Validation loss: 0.041407169022581035\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.03273387079694744 | Validation loss: 0.04332864193017023\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.03379245271219246 | Validation loss: 0.04393824611447359\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.036216145730347794 | Validation loss: 0.04601557598563663\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03738668957390846 | Validation loss: 0.0478383689269162\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.03850440040260142 | Validation loss: 0.046575586066434256\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.037245836072184395 | Validation loss: 0.04904622127089584\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.035356279124581134 | Validation loss: 0.04652073737560657\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.034129417027462514 | Validation loss: 0.04397078325743215\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.032686933013564616 | Validation loss: 0.04174699555886419\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.03151373515232545 | Validation loss: 0.040432571188399664\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "Training loss: 0.031794840381783845 | Validation loss: 0.04145001789979767\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "Training loss: 0.03241383722070348 | Validation loss: 0.04404820838387598\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "Training loss: 0.034046458082247884 | Validation loss: 0.045810290955399215\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "Training loss: 0.03595437882449735 | Validation loss: 0.059114713091076465\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "Training loss: 0.037035941107738654 | Validation loss: 0.047888670345408876\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "Training loss: 0.035954072046473884 | Validation loss: 0.04650187672099523\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "Training loss: 0.03383453256272412 | Validation loss: 0.04721131607105857\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "Training loss: 0.032827389764524186 | Validation loss: 0.042917570001200625\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "Training loss: 0.03113685309872237 | Validation loss: 0.04149719017247359\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257]\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "Training loss: 0.030135778122429076 | Validation loss: 0.03988342725655489\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 101\n",
      "Training loss: 0.03010753776944248 | Validation loss: 0.04145811442612556\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 102\n",
      "Training loss: 0.03123408529165346 | Validation loss: 0.04319112502822751\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 103\n",
      "Training loss: 0.032564907242898525 | Validation loss: 0.044689477470360305\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 104\n",
      "Training loss: 0.033901535042541384 | Validation loss: 0.04784849746838996\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 105\n",
      "Training loss: 0.03641212473511402 | Validation loss: 0.051434298160306195\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 106\n",
      "Training loss: 0.03419514418041418 | Validation loss: 0.045509515918399156\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 107\n",
      "Training loss: 0.0329815001140626 | Validation loss: 0.046876753043187294\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 108\n",
      "Training loss: 0.031264307952800094 | Validation loss: 0.043929761667784895\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 109\n",
      "Training loss: 0.030383295491631684 | Validation loss: 0.04097266080217403\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343]\n",
      "------------------------------\n",
      "Epoch: 110\n",
      "Training loss: 0.029282903673468964 | Validation loss: 0.0398062279840049\n",
      "Validation loss (ends of cycles): [0.16695564 0.05897323 0.05198566 0.04862014 0.04640055 0.04472876\n",
      " 0.04301842 0.04274107 0.04140717 0.04043257 0.03988343 0.03980623]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 30000\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.1400897194288279 | Validation loss: 0.12329615446574547\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.09554748258210327 | Validation loss: 0.08580511917962748\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.08333759619609306 | Validation loss: 0.0800603346351315\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07792517879585686 | Validation loss: 0.07561177874312681\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.07436479456526668 | Validation loss: 0.07901463929344626\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.07109708578668927 | Validation loss: 0.06837146382998018\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.06542817449785376 | Validation loss: 0.0787474691429559\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.061334840936194124 | Validation loss: 0.05965573042631149\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.057118404958103046 | Validation loss: 0.0656482653144528\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.05406631466344391 | Validation loss: 0.05449213265057872\n",
      "Validation loss (ends of cycles): [0.12329615]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.05106979912931198 | Validation loss: 0.052335235345013\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.052069277808952485 | Validation loss: 0.05276231921332724\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.05316668891494996 | Validation loss: 0.05438405183308265\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.054588352700107194 | Validation loss: 0.05557882632402813\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05497209098386137 | Validation loss: 0.05784036999239641\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05595910447208505 | Validation loss: 0.06857345152427169\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.05293017792956609 | Validation loss: 0.07677049303756041\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.05036535177526898 | Validation loss: 0.054857921183985824\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.0480112096307015 | Validation loss: 0.04789778322857969\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.045682432433884396 | Validation loss: 0.04683302770204404\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.04365749700289023 | Validation loss: 0.04525614475064418\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04457468941905781 | Validation loss: 0.046083587692940936\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04552650913155001 | Validation loss: 0.045803798384526195\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04733393179546846 | Validation loss: 0.04765025776098756\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.048526976326186404 | Validation loss: 0.05163861622705179\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04971253661144721 | Validation loss: 0.06940157834221335\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.047659809549192064 | Validation loss: 0.049176037705996455\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.04544191577006131 | Validation loss: 0.047405094828675776\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.04377149401943346 | Validation loss: 0.04654841545750113\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.041435345920341975 | Validation loss: 0.04301520901567796\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614]\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "Training loss: 0.039447522790808424 | Validation loss: 0.041631047142779126\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "Training loss: 0.04063450964412799 | Validation loss: 0.04336159229278565\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "Training loss: 0.04200547775253653 | Validation loss: 0.04426659119918066\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "Training loss: 0.04342988368711973 | Validation loss: 0.044159990264212384\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "Training loss: 0.04422340278611764 | Validation loss: 0.04507376104593277\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "Training loss: 0.04580574428760692 | Validation loss: 0.051384791403132325\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "Training loss: 0.04419933799531703 | Validation loss: 0.04598492960281232\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "Training loss: 0.04210811722165856 | Validation loss: 0.043686308317324694\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "Training loss: 0.04024575349727744 | Validation loss: 0.043796195002163155\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "Training loss: 0.038407098160027284 | Validation loss: 0.04090072779971011\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105]\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "Training loss: 0.036603522439193174 | Validation loss: 0.039455434132148234\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "Training loss: 0.03742256851278638 | Validation loss: 0.04058602978201473\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "Training loss: 0.03843546623829752 | Validation loss: 0.041917330859338534\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "Training loss: 0.039746915991418066 | Validation loss: 0.04393004884614664\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "Training loss: 0.04127714485542751 | Validation loss: 0.044884422421455385\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "Training loss: 0.0428990826157755 | Validation loss: 0.04536734256235992\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "Training loss: 0.04116344682284092 | Validation loss: 0.04351957151118447\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "Training loss: 0.039308764099290495 | Validation loss: 0.041029254938749704\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "Training loss: 0.03747508091067797 | Validation loss: 0.04191207048647544\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "Training loss: 0.035598073072584446 | Validation loss: 0.03979100354892366\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543]\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "Training loss: 0.03438328353835172 | Validation loss: 0.037895834172034966\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "Training loss: 0.0351846847768971 | Validation loss: 0.03996862692429739\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "Training loss: 0.03598356681973919 | Validation loss: 0.0409109014141209\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "Training loss: 0.03732215452351068 | Validation loss: 0.042976783128345714\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "Training loss: 0.039110059012952995 | Validation loss: 0.044927890949389516\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "Training loss: 0.04038672065852504 | Validation loss: 0.04456539171583512\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "Training loss: 0.03914046656757005 | Validation loss: 0.04287377705468851\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "Training loss: 0.037129987647609886 | Validation loss: 0.040416243420365976\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "Training loss: 0.03537750957705277 | Validation loss: 0.03983505024191211\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "Training loss: 0.03378118044378138 | Validation loss: 0.03855334082070519\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583]\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "Training loss: 0.03227280559949577 | Validation loss: 0.03684680106885293\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "Training loss: 0.03293099604963668 | Validation loss: 0.03870913982391357\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "Training loss: 0.03440849116121076 | Validation loss: 0.03925530374707545\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "Training loss: 0.03551614041636257 | Validation loss: 0.04271749407052994\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "Training loss: 0.03671016464451034 | Validation loss: 0.044847844102803396\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "Training loss: 0.038699115091003475 | Validation loss: 0.04217615879195578\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "Training loss: 0.036989314980363765 | Validation loss: 0.04112030182252912\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "Training loss: 0.035253367830361974 | Validation loss: 0.03938967145102865\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "Training loss: 0.033740292163565756 | Validation loss: 0.0396494365461609\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "Training loss: 0.03221185418017405 | Validation loss: 0.03731777299852932\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468 ]\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "Training loss: 0.030991236463581262 | Validation loss: 0.03593932203948498\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "Training loss: 0.03149883268572586 | Validation loss: 0.037519738275338624\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "Training loss: 0.03258418612868378 | Validation loss: 0.03948524081531693\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "Training loss: 0.033819790642806574 | Validation loss: 0.040817394786897825\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "Training loss: 0.0349594693140764 | Validation loss: 0.038657077379962976\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "Training loss: 0.037001777448887496 | Validation loss: 0.043351417955230266\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "Training loss: 0.03563734809132783 | Validation loss: 0.04313999725615277\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "Training loss: 0.03373342292799957 | Validation loss: 0.04140989732216386\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "Training loss: 0.031958367353256203 | Validation loss: 0.03882313241415164\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "Training loss: 0.03071572730121644 | Validation loss: 0.038228221697842374\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932]\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "Training loss: 0.02992848748292186 | Validation loss: 0.03572125563069301\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "Training loss: 0.030003602803978874 | Validation loss: 0.03800442082916989\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "Training loss: 0.030959714718751218 | Validation loss: 0.0395452414146241\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "Training loss: 0.03212108367833456 | Validation loss: 0.043741495951133615\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "Training loss: 0.03381063934832223 | Validation loss: 0.04081341168459724\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "Training loss: 0.035855333057330234 | Validation loss: 0.041724231225602766\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "Training loss: 0.03387873944654865 | Validation loss: 0.03887862777885269\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "Training loss: 0.03237174124898095 | Validation loss: 0.039358641645487615\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "Training loss: 0.030933335009276083 | Validation loss: 0.03714800371843226\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "Training loss: 0.029362504421978405 | Validation loss: 0.03726502972490647\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126]\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "Training loss: 0.02861379960943994 | Validation loss: 0.03534455056137899\n",
      "Validation loss (ends of cycles): [0.12329615 0.05233524 0.04525614 0.04163105 0.03945543 0.03789583\n",
      " 0.0368468  0.03593932 0.03572126 0.03534455]\n",
      "Early stopping!\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: 10 | Size: 40132\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------\n",
      "Epoch: 0\n",
      "Training loss: 0.22611369123667713 | Validation loss: 0.17660081768985345\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 1\n",
      "Training loss: 0.10115880248188151 | Validation loss: 0.08647104853813627\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Training loss: 0.0806360789464684 | Validation loss: 0.07522331774894116\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "Training loss: 0.07291009324221454 | Validation loss: 0.06868328935409014\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "Training loss: 0.06777285844982257 | Validation loss: 0.060847439992744314\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "Training loss: 0.06530867708575597 | Validation loss: 0.05860075982600714\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "Training loss: 0.0605629275820592 | Validation loss: 0.060367112501268895\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "Training loss: 0.056550232517555005 | Validation loss: 0.0516667994546943\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "Training loss: 0.05290380347424781 | Validation loss: 0.05032209189921881\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "Training loss: 0.04968396053414821 | Validation loss: 0.046576345174581604\n",
      "Validation loss (ends of cycles): [0.17660082]\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "Training loss: 0.04691924053559622 | Validation loss: 0.04340929457241983\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "Training loss: 0.04784105510054904 | Validation loss: 0.04530351345254257\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "Training loss: 0.04946117783808626 | Validation loss: 0.04510788741496812\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "Training loss: 0.05063574022694132 | Validation loss: 0.0505708995052671\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "Training loss: 0.05166313559271571 | Validation loss: 0.05817923330561777\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "Training loss: 0.05248750614801671 | Validation loss: 0.0493360729311156\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "Training loss: 0.049662602678542646 | Validation loss: 0.05467164823043663\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "Training loss: 0.04728653871703629 | Validation loss: 0.052291345279828635\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "Training loss: 0.04459606154132488 | Validation loss: 0.05458670186983273\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "Training loss: 0.04198971974542438 | Validation loss: 0.03901625990010468\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929]\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "Training loss: 0.03987340637625701 | Validation loss: 0.03802946162105134\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "Training loss: 0.04096880447445804 | Validation loss: 0.038806912893083245\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "Training loss: 0.04220846461638163 | Validation loss: 0.0440736518167289\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "Training loss: 0.04363556982084053 | Validation loss: 0.04022814183438246\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "Training loss: 0.04542448289974732 | Validation loss: 0.046775908052789424\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "Training loss: 0.04642166053568284 | Validation loss: 0.04878175716880148\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "Training loss: 0.044440456977715405 | Validation loss: 0.04138228518113626\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "Training loss: 0.041865324622614115 | Validation loss: 0.04312394201689589\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "Training loss: 0.039823972675986 | Validation loss: 0.03708740910597607\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "Training loss: 0.03766148964009123 | Validation loss: 0.036518600266591636\n",
      "Validation loss (ends of cycles): [0.17660082 0.04340929 0.03802946]\n",
      "------------------------------\n",
      "Epoch: 30\n"
     ]
    }
   ],
   "source": [
    "for seed in tqdm(seeds):\n",
    "    perfs_by_size = OrderedDict({'seed': [], 'n_samples': [], 'test_score': [], 'n_epochs': []})\n",
    "    \n",
    "    for size in training_size:\n",
    "        print(80*'-')\n",
    "        print(f'Seed: {seed} | Size: {size}')\n",
    "        print(80*'-')\n",
    "        idx = np.random.choice(len(X), size, replace=False)\n",
    "        \n",
    "        # Train/test split\n",
    "        data = train_test_split(X[idx, :], \n",
    "                                y[idx], \n",
    "                                depth_order[idx,1], \n",
    "                                test_size=split_ratio,\n",
    "                                random_state=seed)\n",
    "        X_train, X_test, y_train, y_test, tax_order_train, tax_order_test = data\n",
    "        data_test = X_test, y_test, tax_order_test\n",
    "\n",
    "        # Further train/valid split\n",
    "        data = train_test_split(X_train, \n",
    "                                y_train,\n",
    "                                tax_order_train,\n",
    "                                test_size=split_ratio, \n",
    "                                random_state=seed)\n",
    "        X_train, X_valid, y_train, y_valid, tax_order_train, tax_order_valid = data\n",
    "        data_train = X_train, y_train, tax_order_train\n",
    "        data_valid = X_valid, y_valid, tax_order_valid\n",
    "        \n",
    "        dls = DataLoaders(data_train, \n",
    "                          data_valid,\n",
    "                          data_test,\n",
    "                          transform=SNV_transform(),\n",
    "                          batch_size=32)\n",
    "\n",
    "        training_generator, validation_generator, test_generator = dls.loaders()\n",
    "        \n",
    "        # Modeling\n",
    "        model = Model(X.shape[1], out_channel=16).to(device)\n",
    "        opt = Adam(model.parameters(), lr=1e-4)\n",
    "        model = model.apply(weights_init)\n",
    "        scheduler = CyclicLR(opt, base_lr=base_lr, max_lr=max_lr,\n",
    "                             step_size_up=step_size_up, mode='triangular',\n",
    "                             cycle_momentum=False)\n",
    "\n",
    "        early_stopper = partial(is_plateau, delta=delta, verbose=False)\n",
    "\n",
    "        learner = Learner(model, criterion, opt, n_epochs=n_epochs, \n",
    "                          scheduler=scheduler, early_stopper=early_stopper,\n",
    "                          tax_lookup=tax_lookup.values(), verbose=True)\n",
    "        model, losses = learner.fit(training_generator, validation_generator)\n",
    "\n",
    "        y_hat, y_true = learner.predict(test_generator)\n",
    "        perfs = eval_reg(y_true, y_hat)\n",
    "\n",
    "        perfs_by_size['seed'].append(seed)\n",
    "        perfs_by_size['n_samples'].append(size)\n",
    "        perfs_by_size['n_epochs'].append(len(losses['train']))\n",
    "        perfs_by_size['test_score'].append(perfs['r2'])\n",
    "\n",
    "    with open(dest_dir/f'cnn-lc-seed-{seed}.pickle', 'wb') as f: \n",
    "        pickle.dump(perfs_by_size, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all(src_dir):\n",
    "    dumps = []\n",
    "    for file in glob.glob(str(src_dir/'*.pickle')):\n",
    "        with open(file, 'rb') as f: \n",
    "            dumps.append(pickle.load(f))\n",
    "    return dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps/cnn/learning_curve')\n",
    "dumps = load_all(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b9b31fc2-cf45-4d17-9323-7556ee259b54\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">n_epochs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.413549</td>\n",
       "      <td>0.184284</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>47.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.453063</td>\n",
       "      <td>0.073105</td>\n",
       "      <td>79.333333</td>\n",
       "      <td>17.224014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.534975</td>\n",
       "      <td>0.051113</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>33.466401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.607885</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>23.380904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.677188</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>28.284271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9b31fc2-cf45-4d17-9323-7556ee259b54')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b9b31fc2-cf45-4d17-9323-7556ee259b54 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b9b31fc2-cf45-4d17-9323-7556ee259b54');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          test_score              n_epochs           \n",
       "                mean       std        mean        std\n",
       "n_samples                                            \n",
       "500         0.413549  0.184284  111.000000  47.749346\n",
       "1000        0.453063  0.073105   79.333333  17.224014\n",
       "2000        0.534975  0.051113  121.000000  33.466401\n",
       "5000        0.607885  0.039500   87.666667  23.380904\n",
       "10000       0.677188  0.020144  101.000000  28.284271"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.DataFrame(perf) for perf in dumps])\n",
    "grps = df.groupby(['n_samples']).agg({'test_score':['mean','std'], 'n_epochs':['mean','std']})\n",
    "grps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAELCAYAAADX3k30AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAApUklEQVR4nO3deXxU5dn/8c9F2GWRVWTfQfYlBBERcQEsLYj0Z3ErWBE31Me2KC7V1qUufR7XulFEfEAFl0Kx2oJWUHGBJIACYQt7gkLYl5CQ5fr9kYFnjIEMZJJJJt/36zUvcs657zPXJMM3J+c+cx9zd0REJHpViHQBIiJSvBT0IiJRTkEvIhLlFPQiIlFOQS8iEuUU9CIiUa5ipAvIr379+t6yZctIlyEiUqYkJibucvcGBW0rdUHfsmVLEhISIl2GiEiZYmZbTrRNp25ERKKcgl5EJMop6EVEolypO0dfkKysLFJSUsjIyIh0KVIMqlatStOmTalUqVKkSxGJSmUi6FNSUqhZsyYtW7bEzCJdjoSRu7N7925SUlJo1apVpMsRiUpl4tRNRkYG9erVU8hHITOjXr16+mtNpBiViaAHFPJRTD9bEdh1KJOVqfuLZd9lJugjbd++fbz00kun1ffZZ58lPT09zBWJSDQ4mJHFMx+vY+BTC7hr1nKK4x4hCvoQlaWgz87OLrHnEpHTk5mdw2uLNjHwLwt57j/rGdihAS9f27tY/sJV0Ido0qRJbNiwgR49ejBx4kT+8pe/0KdPH7p168ZDDz0EwOHDhxk2bBjdu3enS5cuzJo1i+eff57t27czaNAgBg0aVOC+c3JyGDt2LF26dKFr164888wzACQnJ3PJJZfQvXt3evXqxYYNG3B3Jk6ceLztrFmzAFi4cCEDBgxg+PDhdOrUiZycHCZOnHi8xldfffWEr23hwoUMHDiQESNG0Lp1ayZNmsSbb75JXFwcXbt2ZcOGDQCkpaUxatQo+vTpQ58+ffjyyy8BWLJkCf369aNnz56cd955rF27FoBp06ZxxRVXMHToUNq1a8fdd98dnh+GSBmWk+u8m7CNi/77Mx75ZxLnnF2Tf9zWn5eu6U3bhjWK5TnLxFU3wf70wSqSth8I6z47Na7FQ7/ofNI2TzzxBCtXrmT58uXMnz+f9957jyVLluDuDB8+nM8//5y0tDQaN27Mhx9+CMD+/fupXbs2Tz/9NAsWLKB+/foF7nv58uWkpqaycuVKIO+vB4BrrrmGSZMmMXLkSDIyMsjNzeXvf/87y5cv59tvv2XXrl306dOHCy64AIClS5eycuVKWrVqxeTJk6lduzbx8fFkZmbSv39/Bg8efMIrW7799ltWr15N3bp1ad26NePGjWPJkiU899xzvPDCCzz77LPceeed3HXXXZx//vls3bqVIUOGsHr1ajp27MgXX3xBxYoV+eSTT7jvvvt4//33j7+2ZcuWUaVKFTp06MDtt99Os2bNTvlnJFLWuTsfJ+3gL/PWsn7nIbo2qc2To7pxfruCcyGcylzQlwbz589n/vz59OzZE4BDhw6xfv16BgwYwO9+9zvuuecefv7znzNgwICQ9te6dWs2btzI7bffzrBhwxg8eDAHDx4kNTWVkSNHAnnXmgMsWrSIq666ipiYGM466ywGDhxIfHw8tWrVIi4u7niQz58/n++++4733nsPyPuls379+hMGfZ8+fTj77LMBaNOmDYMHDwaga9euLFiwAIBPPvmEpKSk430OHDjAoUOH2L9/P2PGjGH9+vWYGVlZWcfbXHzxxdSuXRuATp06sWXLFgW9lDvfbNzNk/9ew7Kt+2hd/wxeuqYXl3VpVGIXIpS5oC/syLskuDv33nsvN91000+2LV26lI8++ogHHniAiy++mAcffLDQ/dWpU4dvv/2WefPm8corr/DOO+/w3HPPnXJdZ5xxxo9qfOGFFxgyZEhIfatUqXL86woVKhxfrlChwvFz/rm5uXzzzTfHf+kcM2HCBAYNGsTs2bPZvHkzF154YYH7jYmJ0fiBlCurtu/nqX+v5bN1aTSqVZUnrujKL3s3pWJMyZ411zn6ENWsWZODBw8CMGTIEKZOncqhQ4cASE1NZefOnWzfvp3q1atz7bXXMnHiRJYuXfqTvgXZtWsXubm5jBo1ikcffZSlS5dSs2ZNmjZtypw5cwDIzMwkPT2dAQMGMGvWLHJyckhLS+Pzzz8nLi7uJ/scMmQIL7/88vGj63Xr1nH48OEifQ8GDx7MCy+8cHx5+fLlQN5fC02aNAHyzsuLlHebdx3mjreXMez5RSzfto97L+vIwokXMjqueYmHPJTBI/pIqVevHv3796dLly5cdtllXH311fTr1w+AGjVqMGPGDJKTk5k4cSIVKlSgUqVKvPzyywCMHz+eoUOH0rhx4+OnQYKlpqZy/fXXk5ubC8Djjz8OwPTp07npppt48MEHqVSpEu+++y4jR47k66+/pnv37pgZTz31FI0aNWLNmjU/2ue4cePYvHkzvXr1wt1p0KDB8V8ap+v555/ntttuo1u3bmRnZ3PBBRfwyiuvcPfddzNmzBgeffRRhg0bVqTnECnLdh7I4PlP1zNzyTYqxhi3DWrD+AvaULtaZKf3sOK4ZrMoYmNjPf989KtXr+acc86JUEVSEvQzlrJs/5EsXv1sA1O/3ER2jjM6rhl3XNSOhrWqFt45TMws0d1jC9qmI3oRkdOUkZXDG19t5qWFG9h/JIvh3Rvz20vb07L+GYV3LkEK+hLWt29fMjMzf7Ru+vTpdO3atdife8WKFVx33XU/WlelShUWL15c7M8tEk0ys3N4LzGFF/6TzA8HMriwQwMmDulA58a1I11agUIKejMbCjwHxABT3P2JfNufAY59Gqg60NDdzwxsywFWBLZtdffhYai7zIpkqHbt2vX4AKqInLpDmdm8tXgLU77YxM6DmfRsfibPju7Bua3rRbq0kyo06M0sBngRuBRIAeLNbK67H7+g2t3vCmp/O9AzaBdH3L1H2CoWESlhuw9lMu2rzbzx1WYOZGRzXpt6PH1lD/q3LRuz6oZyRB8HJLv7RgAzmwmMAJJO0P4q4KHwlPd/3L1MfEPl1JW2CwJEjknZm86ULzYxM34rGVm5DOl8Frdc2JYezc6MdGmnJJSgbwJsC1pOAfoW1NDMWgCtgE+DVlc1swQgG3jC3eecapFVq1Zl9+7dmpM+Ch278Uj+D2GJRNL6HQd5+bMNzF2+HYDLezbh5oGtaduwZoQrOz3hHowdDbzn7jlB61q4e6qZtQY+NbMV7r4huJOZjQfGAzRv3vwnO23atCkpKSmkpaWFuVwpDY7dSlAk0pZt3ctLCzfwcdIOqlWK4bp+LbhxQGsan1kt0qUVSShBnwoET07SNLCuIKOB24JXuHtq4N+NZraQvPP3G/K1mQxMhrzr6PPvtFKlSrrNnIgUC3fni/W7eGlhMt9s3EPtapW44+J2jD2vJXXPqBzp8sIilKCPB9qZWSvyAn40cHX+RmbWEagDfB20rg6Q7u6ZZlYf6A88FY7CRUSKIifX+ffKH3j5s2RWph7grFpVeGDYOYyOa06NKtF15Xmhr8bds81sAjCPvMsrp7r7KjN7GEhw97mBpqOBmf7jkbVzgFfNLJe8eXWeCL5aR0SkpGVm5zB7aSqvfr6RTbsO06r+GTw5qiuX92xClYoxkS6vWJSJKRBERIoq/zXwXZrU4tYL2zKkcyNiKpT9izw0BYKIlFsFXQP/P1d25/y29cvNVXwKehGJSqn7jvC3zzf+6Br4mwe2oWfzOpEurcQp6EUkqkTbNfDhoKAXkaiwfNs+XlqQzPyga+DHDWhNkzJ+DXw4KOhFpEzbc/goj3+0mncTU6LyGvhwUNCLSJnk7ry/NJXHPkziYEY2Nw9sw4SL2kbdNfDhoO+IiJQ5yTsP8cCcFXyzcQ+9W9ThsZFd6NioVqTLKrUU9CJSZmRk5fDSwg28snADVStV4M8juzK6TzMqRMF18MVJQS8iZcKXybt4YM5KNu06zIgejXlgWCca1KwS6bLKBAW9iJRquw5l8tiHq5m9LJUW9aoz/YY4BrRrEOmyyhQFvYiUSrm5zjsJ23j8X2tIP5rN7Re15bZBbalaKTrnoylOCnoRKXXW7TjI/bNXEL95L3Et6/LnK7qU6w88FZWCXkRKjYysHF74dD2vfraRGlUr8tSobvyyd1MNthaRgl5ESoXP1qXxhzkr2bonnVG9mnLfzzpSr4YGW8NBQS8iEbXzYAaP/HM1H3y7ndb1z+CtG/tyXpv6kS4rqijoRSQicnOdN5ds5al/ryEzK5e7LmnPzRe2jtqbf0SSgl5EStzq7w9w3+wVLNu6j/Pa1OPRy7vQukGNSJcVtRT0IlJi0o9m89wn65myaBO1q1Xi6Su7M7Jnk3JzA5BIUdCLSIn4z+odPPiPVaTuO8LoPs2YdFlHzqyuGSZLQoVQGpnZUDNba2bJZjapgO3PmNnywGOdme0L2jbGzNYHHmPCWLuIlAE/7M/glhmJ3PBGAtUrx/DOTf14YlQ3hXwJKvSI3sxigBeBS4EUIN7M5rp70rE27n5XUPvbgZ6Br+sCDwGxgAOJgb57w/oqRKTUycl1pn+9mf+ev46snFwmDunAjQNaU7liSMeXEkahnLqJA5LdfSOAmc0ERgBJJ2h/FXnhDjAE+Njd9wT6fgwMBd4uStEiUrqtTN3PfbNX8F3Kfi5o34BHRnSmRb0zIl1WuRVK0DcBtgUtpwB9C2poZi2AVsCnJ+nbpIB+44HxAM2bNw+hJBEpjQ5lZvP0/HVM+2oTdc+owvNX9eQX3c7WYGuEhXswdjTwnrvnnEond58MTAaIjY31MNckIiVg3qof+OPcVfxwIIOr45pz99CO1K5WKdJlCaEFfSrQLGi5aWBdQUYDt+Xre2G+vgtDL09ESrvt+47w0NxVfJy0g46NavLiNb3o1bxOpMuSIKEEfTzQzsxakRfco4Gr8zcys45AHeDroNXzgD+b2bGf+mDg3iJVLCKlQnZOLtO+2szTH6/DHe69rCO/Ob8VlWI02FraFBr07p5tZhPIC+0YYKq7rzKzh4EEd58baDoamOnuHtR3j5k9Qt4vC4CHjw3MikjZtXzbPu77+wqSvj/ARR0b8qfhnWlWt3qky5ITsKBcLhViY2M9ISEh0mWISAEOZGTx3/PWMv2bLTSsWYU//qIzQ7s00mBrKWBmie4eW9A2fTJWRArl7ny04gf+9MEq0g5lMqZfS343uD01q2qwtSxQ0IvISW3bk86D/1jJgrVpdGlSiyljYunW9MxIlyWnQEEvIgXKysnltUWbePaTdcSY8Yefd2JMvxZU1GBrmaOgF5GfSNyyl/tnr2DNDwcZ3Oks/ji8M43PrBbpsuQ0KehF5Lj96Vk8OW8Nby3eSuPaVZl8XW8Gd24U6bKkiBT0IoK7M/fb7TzyzyT2HD7KuPNbcdel7TmjiiIiGuinKFLOZWTl8MCclbyXmEL3prWZdn0cXZrUjnRZEkYKepFybNuedG55M5GVqQe44+J23HlxO2Iq6Jr4aKOgFymnPl+Xxh0zl5GT67w2JpaLzzkr0iVJMVHQi5QzubnOSwuT+Z+P19HhrJq8cm1vWtbXXPHRTEEvUo4cyMjid+98y8dJOxjRozGPX9GV6pUVA9FOP2GRcmLtDwe5eUYi2/ak89AvOjH2vJaao6acUNCLlAMffLudu9/7jhpVK/L2+HPp07JupEuSEqSgF4liWTm5PPGvNby2aBOxLerw0jW9aFiraqTLkhKmoBeJUmkHM5nw1lIWb9rDmH4tuH9YJypX1Dw15ZGCXiQKLd26l1tmJLL/SBbP/Ko7I3s2jXRJEkEKepEo4u7MWLyVhz9Yxdm1q/H3W+Lo1LhWpMuSCFPQi0SJjKwc7p+9kveXpjCoQwOe/VVPalfXjUEEQjphZ2ZDzWytmSWb2aQTtLnSzJLMbJWZvRW0PsfMlgcecwvqKyJFs21POqNe/or3l6Zw58XteG1MH4W8HFfoEb2ZxQAvApcCKUC8mc1196SgNu2Ae4H+7r7XzBoG7eKIu/cIb9kicsxn69K44+1luDtTx8ZyUUdNZSA/Fsqpmzgg2d03ApjZTGAEkBTU5kbgRXffC+DuO8NdqIj8WG6u8+KCZJ7+JG8qg1ev602LeprKQH4qlFM3TYBtQcspgXXB2gPtzexLM/vGzIYGbatqZgmB9ZcX9ARmNj7QJiEtLe1U6hcpl/YfyWL89AT+5+N1jOjemNm39lfIywmFazC2ItAOuBBoCnxuZl3dfR/Qwt1Tzaw18KmZrXD3DcGd3X0yMBkgNjbWw1STSFRa+8NBbpqeQMreI/zxF50Yo6kMpBChBH0q0CxouWlgXbAUYLG7ZwGbzGwdecEf7+6pAO6+0cwWAj2BDYjIKZv77Xbu0VQGcopCOXUTD7Qzs1ZmVhkYDeS/emYOeUfzmFl98k7lbDSzOmZWJWh9f358bl9EQpCVk8vDHyRxx9vL6NKkFh/efr5CXkJW6BG9u2eb2QRgHhADTHX3VWb2MJDg7nMD2wabWRKQA0x0991mdh7wqpnlkvdL5Yngq3VEpHA7D2Yw4a1lLNm0h7HnteT+YedQKUZTGUjozL10nRKPjY31hISESJchUiokbtnDrW8uZf+RLJ64ohuX98x/HYRIHjNLdPfYgrbpk7EipZC7M/2bLTzyzyQan1mNadfHcc7ZmspATo+CXqSUOXI0h/tnr+Dvy1K5qGNDnrmyhz7lKkWioBcpRbbuTuemGYms+eEAd13SntsvakuFCrp0UopGQS9SSixYu5P/mrk8byqDMX0Y1LFh4Z1EQqCgF4mw3FznhU+TefY/6+jYqBavXNtLn3KVsFLQi0TQ/iNZ/HbWcv6zZicjezbhzyO7Uq1yTKTLkiijoBeJkNXfH+DmGYmk7j3Cn4Z35tf9WmgqAykWCnqRCPjH8lTuef87alWtxMzx5xKrT7lKMVLQi5SgrJxcHvtwNdO+2kxcy7r89ZqeNKxZNdJlSZRT0IuUkJ0HMrjtraXEb97L9f1bct/PNJWBlAwFvUgJSNicN5XBwYxsnhvdgxE9NJWBlBwFvUgxcnfe+Gozj364miZ1qvG/N8TRsZGmMpCSpaAXKSZHjuZw3+wVzF6WysUdG/L0r3pQu5qmMpCSp6AXKQZbdh/mpumJrN1xkN9e2p4JgzSVgUSOgl4kzBas2cmdM5dhZkwd24dBHTSVgUSWgl4kTHJznec/Xc9z/1nPOY1q8cq1vWler3qkyxJR0IuEw65Dmdz93nd8umYnV/RqwmOXayoDKT0U9CJFkJvrvJu4jT9/tIb0o9k8MqIz156rqQykdAnp0xpmNtTM1ppZsplNOkGbK80sycxWmdlbQevHmNn6wGNMuAoXibT1Ow4yevI33PP+CjqcVZN/3TmA6/q1VMhLqVPoEb2ZxQAvApcCKUC8mc0Nvsm3mbUD7gX6u/teM2sYWF8XeAiIBRxIDPTdG/6XIlIyMrJyeHFBMq98toHqlSvy1Khu/LJ3U11VI6VWKKdu4oBkd98IYGYzgRFAUlCbG4EXjwW4u+8MrB8CfOzuewJ9PwaGAm+Hp3yRkrVo/S4emLOCzbvTuaJnE+4bdg71a1SJdFkiJxVK0DcBtgUtpwB987VpD2BmXwIxwB/d/d8n6KvPfkuZs+tQJo99uJrZy1JpWa86M27oy/nt6ke6LJGQhGswtiLQDrgQaAp8bmZdQ+1sZuOB8QDNmzcPU0kiRZd/sPWOi9py66C2VK2kK2qk7Agl6FOBZkHLTQPrgqUAi909C9hkZuvIC/5U8sI/uO/C/E/g7pOByQCxsbEeYu0ixWr9joPcP3slSzbvIa5lXf58RRfaNqwZ6bJETlkoQR8PtDOzVuQF92jg6nxt5gBXAa+bWX3yTuVsBDYAfzazOoF2g8kbtBUptTTYKtGm0KB392wzmwDMI+/8+1R3X2VmDwMJ7j43sG2wmSUBOcBEd98NYGaPkPfLAuDhYwOzIqWRBlslGpl76TpTEhsb6wkJCZEuQ8qZ/IOtj17eVYOtUqaYWaK7xxa0TZ+MlXJNg61SHijopdzSYKuUFwp6KXc02CrljYJeyhUNtkp5pKCXckGfbJXyTEEvUU2DrSIKeoliGmwVyaOgl6ijwVaRH1PQS1TRYKvITynoJSposFXkxBT0UqZpsFWkcAp6KbM02CoSGgW9lDkabBU5NQp6KVM02Cpy6hT0UiZosFXk9CnopVTTYKtI0SnopdTSYKtIeCjopdTRYKtIeIUU9GY2FHiOvHvGTnH3J/JtHwv8hbybhwP81d2nBLblACsC67e6+/Aw1C1RSoOtIuFXaNCbWQzwInApkALEm9lcd0/K13SWu08oYBdH3L1HkSuVqKbBVpHiE8oRfRyQ7O4bAcxsJjACyB/0IqdMg60ixS+UoG8CbAtaTgH6FtBulJldAKwD7nL3Y32qmlkCkA084e5zilCvRBENtoqUjHANxn4AvO3umWZ2E/AGcFFgWwt3TzWz1sCnZrbC3TcEdzaz8cB4gObNm4epJCmtNNgqUrJCCfpUoFnQclP+b9AVAHffHbQ4BXgqaFtq4N+NZrYQ6AlsyNd/MjAZIDY21kMvX8oaDbaKlLxQgj4eaGdmrcgL+NHA1cENzOxsd/8+sDgcWB1YXwdIDxzp1wf6E/RLQMoPDbaKRE6hQe/u2WY2AZhH3uWVU919lZk9DCS4+1zgDjMbTt55+D3A2ED3c4BXzSwXqEDeOXoN4pYjGmwViTxzL11nSmJjYz0hISHSZUgYaLBVpOSYWaK7xxa0TZ+MlbDTYKtI6aKgl7DSYKtI6aOgl7DIysnlD3NWMjN+mwZbRUoZBb0U2ZGjOdz6ZiIL1qZx88A2/Ncl7TTYKlKKKOilSPYfyWLcG/EkbNnL41d05ao4feBNpLRR0MtpSzuYya+nLiF550H+elUvhnU7O9IliUgBFPRyWrbtSee61xaz40Amr43pwwXtG0S6JBE5AQW9nLL1Ow5y3WtLSD+azYxxfendok6kSxKRk1DQyylZvm0fY19fQqWYCrxzcz86NqoV6ZJEpBAKegnZl8m7uPF/E6hfowozbuhL83rVI12SiIRAQS8h+ffKH7jj7WW0qn8G02+Io2GtqpEuSURCpKCXQr2TsI1J739Hj2Zn8vrYOGpXrxTpkkTkFCjo5aSmfLGRRz9czYB29Xn1ut5Ur6y3jEhZo/+1UiB353/mr+OvC5IZ1vVsnv5Vd6pU1KddRcoiBb38RE6u8+A/VvLm4q1cFdeMRy/vSoxmnhQpsxT08iNHs3P57TvL+ed333PLhW24e0gHzBTyImWZgl6OO3I0h1veTGTh2jTuvawjNw1sE+mSRCQMFPQC5E1OdsO0eJZu3csTV3RltCYnE4kaCnph58EMxkyNz5uc7Ope/KyrJicTiSYVQmlkZkPNbK2ZJZvZpAK2jzWzNDNbHniMC9o2xszWBx5jwlm8FN22Pen8v1e+Zsvuw0wd20chLxKFCj2iN7MY4EXgUiAFiDezue6elK/pLHefkK9vXeAhIBZwIDHQd29YqpciWbfjINe9tpiMrFxmjOtLr+aanEwkGoVyRB8HJLv7Rnc/CswERoS4/yHAx+6+JxDuHwNDT69UCadlW/dy5atf4w7v3NRPIS8SxUIJ+ibAtqDllMC6/EaZ2Xdm9p6ZNTuVvmY23swSzCwhLS0txNLldC1av4trpiymVtVKvH/LeXRoVDPSJYlIMQrpHH0IPgBauns38o7a3ziVzu4+2d1j3T22QQPdwKI4/Xvl9/xmWjzN61bnvZv70ayuZqAUiXahBH0q0CxouWlg3XHuvtvdMwOLU4DeofaVkvNO/DZufXMpXZrUYtb4fpqBUqScCCXo44F2ZtbKzCoDo4G5wQ3MLPhSjeHA6sDX84DBZlbHzOoAgwPrpIRN/nwDd7//Hee3a8CMcX01A6VIOVLoVTfunm1mE8gL6BhgqruvMrOHgQR3nwvcYWbDgWxgDzA20HePmT1C3i8LgIfdfU8xvA45AXfnL/PW8tLCDQzrdjbPXNmDyhXDdcZORMoCc/dI1/AjsbGxnpCQEOkyokJOrvOHf6zkrcVbuSquOY9e3kWTk4lEKTNLdPfYgrbpk7FRKnhyslsvbMNETU4mUm4p6KNQ+tFsbpmxlM/WpXHfzzoy/gJNTiZSninoo8z+9Cx+80Y8y7bu5clRXflVH01OJlLeKeijyM4DGfx66hI2ph3mxat7cZnmrRERFPRRY9uedK59bTFpBzOZOrYP57erH+mSRKSUUNBHgbU/5E1Olpmdy5vj+tJT89aISBAFfRm3dOtern89nqqVKvDuzf1of5bmrRGRH1PQl2FfrE/jpumJNKhZhRk39NW8NSJSIAV9GfXRiu+5c+Yy2jSowf/eEEfDmpq3RkQKpqAvg2Yu2cp9s1fQs3kdpo7po3lrROSkFPRlzKufbeDxf61hYPsGvHxtL6pX1o9QRE5OKVFGuDtPzVvLyws38PNuZ/O0JicTkRAp6MuAnFzngTkreXvJVq7p25yHR2hyMhEJnYK+lDuanctds5bz4YrvuW1QG34/WJOTicipUdCXYulHs7lpeiJfrN/F/T87hxsvaB3pkkSkDFLQl1L707O4ftoSlm/bx1OjunFln2aFdxIRKYCCvhQKnpzspWt6MbSLJicTkdOnoC9ltu7Om5xs16FMXr++D/3banIyESmakK7PM7OhZrbWzJLNbNJJ2o0yMzez2MBySzM7YmbLA49XwlV4NFrzwwF++cpXHMjI4q0bz1XIi0hYFHpEb2YxwIvApUAKEG9mc909KV+7msCdwOJ8u9jg7j3CU2502rTrMK8t2sh7iSnUrlaJd27S5GQiEj6hnLqJA5LdfSOAmc0ERgBJ+do9AjwJTAxrhVHK3VmyaQ9/+2IT/1mzg0oVKjCyZxPuuKQdTc6sFunyRCSKhBL0TYBtQcspQN/gBmbWC2jm7h+aWf6gb2Vmy4ADwAPu/kX+JzCz8cB4gObNo/vWd1k5uXy04nteW7SJ71L2U6d6JW4f1Jbr+rWkQc0qkS5PRKJQkQdjzawC8DQwtoDN3wPN3X23mfUG5phZZ3c/ENzI3ScDkwFiY2O9qDWVRgcyspi5ZCvTvtzM9v0ZtG5wBo+N7MKoXk2pWikm0uWJSBQLJehTgeCLuJsG1h1TE+gCLAx8YrMRMNfMhrt7ApAJ4O6JZrYBaA8khKH2MmHbnnSmfbWZWfHbOJSZzbmt6/LI5V0Y1KEhFTSNgYiUgFCCPh5oZ2atyAv40cDVxza6+37g+OUhZrYQ+L27J5hZA2CPu+eYWWugHbAxjPWXWsu37eNvX2zkXyu+p4IZP+92NuMGtKZLk9qRLk1EyplCg97ds81sAjAPiAGmuvsqM3sYSHD3uSfpfgHwsJllAbnAze6+JxyFl0Y5uc7HSTuY8sVGErbspWbVitx4QWvGnteSs2trgFVEIsPcS9cp8djYWE9IKFtndtKPZvNuQgpTv9zElt3pNKtbjd/0b8X/i21GjSr6TJqIFD8zS3T32IK2KYWKYMeBDKZ9tZm3Fm9l/5EsejY/k3uGdmRI50aaRlhESg0F/WlI2n6AKYs28sG328nJdYZ0bsS4Aa3p3aJOpEsTEfkJBX2IcnOdz9alMWXRRr5M3k31yjFc07cFv+nfiub1qke6PBGRE1LQFyIjK4c5y1KZsmgTyTsP0ahWVSZd1pGr4ppTu5puyi0ipZ+C/gR2H8pk+jdbmP71FnYfPkrnxrV49lc9+FnXs3WvVhEpUxT0+STvPMhrizbx/tJUjmbncnHHhtwwoBX9WtfTLfxEpExS0JM3wdjXG3bzty82smBtGlUqVuCXvZvym/6taNuwRqTLExEpknId9Eezc/nnd9uZ8sUmkr4/QP0albnrkvZce25z6tXQBGMiEh3KZdDvT8/irSVbmfbVJnYcyKRdwxo8OaorI3o00QRjIhJ1ylXQb9l9mNe/3Mw7CdtIP5rD+W3r8+Sobgxs30Dn30UkapWLoE/csoe/fb6JeUk/ULGCMbx7E8YNaMU5Z9eKdGkiIsUuaoM+OyeXeat2MGXRRpZt3UftapW49cI2/LpfS86qVTXS5YmIlJioC/pDmdnMit/G619uImXvEVrUq87DIzrzy95NqV456l6uiEihoib59h4+ysufbeDtxVs5mJlNXMu6/OHnnbjknLM0wZiIlGtRE/Rm8PbirQzs0IBxA1rTo9mZkS5JRKRUiJqgP7N6Zb669yJqVtX8MyIiwaJq0haFvIjIT0VV0IuIyE+FFPRmNtTM1ppZsplNOkm7UWbmZhYbtO7eQL+1ZjYkHEWLiEjoCj1Hb2YxwIvApUAKEG9mc909KV+7msCdwOKgdZ2A0UBnoDHwiZm1d/ec8L0EERE5mVCO6OOAZHff6O5HgZnAiALaPQI8CWQErRsBzHT3THffBCQH9iciIiUklKBvAmwLWk4JrDvOzHoBzdz9w1PtKyIixavIg7FmVgF4GvhdEfYx3swSzCwhLS2tqCWJiEiQUII+FWgWtNw0sO6YmkAXYKGZbQbOBeYGBmQL6wuAu09291h3j23QoMGpvQIRETkpc/eTNzCrCKwDLiYvpOOBq9191QnaLwR+7+4JZtYZeIu88/KNgf8A7U42GGtmacCWU38px9UG9hdT+1DaFtamsO31gV0h1lManer3v7Q9X1H2dzp99f4Lr/L8/mvh7gUfKbt7oQ/gZ+SF/Qbg/sC6h4HhBbRdCMQGLd8f6LcWuCyU5yvKA5hcXO1DaVtYmxC2JxT396g0ff9L2/MVZX+n01fvv9L9fijp5yuu+kOaAsHdPwI+yrfuwRO0vTDf8mPAY6E8T5h8UIztQ2lbWJtTra+sKenXF+7nK8r+Tqev3n/hVZ7ffydU6KkbKVlmluDusYW3FAk/vf+ik6ZAKH0mR7oAKdf0/otCOqIXEYlyOqIXEYlyCnoRkSinoBcRiXIK+lLMzM4xs1fM7D0zuyXS9Uj5Y2ZnBKYn+Xmka5HTp6AvYWY21cx2mtnKfOt/Mue/u69295uBK4H+kahXosupvP8C7gHeKdkqJdwU9CVvGjA0eEXQnP+XAZ2AqwJz+WNmw4EPyfeBNZHTNI0Q339mdimQBOws6SIlvKLm5uBlhbt/bmYt860+Puc/gJkdm/M/yd3nkjdJ3IfkzRskctpO8f1XAziDvPA/YmYfuXtuSdYr4aGgLx0Kmre/r5ldCFwBVEFH9FJ8Cnz/ufsEADMbC+xSyJddCvpSzN0XkjdJnEjEuPu0SNcgRaNz9KVDSPP2ixQTvf+inIK+dIgH2plZKzOrTN4N1edGuCYpP/T+i3IK+hJmZm8DXwMdzCzFzG5w92xgAjAPWA284ye4sYtIUej9Vz5pUjMRkSinI3oRkSinoBcRiXIKehGRKKegFxGJcgp6EZEop6AXEYlyCnoRkSinoBcpRmY21sz+Guk6pHxT0IuIRDkFvUQFM2tpZqvN7G9mtsrM5ptZtRO0vcPMkszsu8Dc65hZnJl9bWbLzOwrM+sQWD/WzOaY2cdmttnMJpjZbwPtvjGzuoF2C83sOTNbbmYrzSyugOdtYGbvm1l84NE/sH5goN/ywH5rFt93SsojBb1Ek3bAi+7eGdgHjDpBu0lAT3fvBtwcWLcGGODuPYEHgT8Hte9C3n0B+gCPAemBdl8Dvw5qV93dewC3AlMLeN7ngGfcvU+gtimB9b8Hbgv0HQAcCfH1ioRE89FLNNnk7ssDXycCLU/Q7jvgTTObA8wJrKsNvGFm7QAHKgW1X+DuB4GDZrYf+CCwfgXQLajd23D8Lk61zOzMfM97CdDJzI4t1zKzGsCXwNNm9ibwd3dPCenVioRIR/QSTTKDvs7hxAcyw8i7R2ovIN7MKgKPkBfoXYBfAFVPsN/coOXcfM+Rf4bA/MsVgHPdvUfg0cTdD7n7E8A4oBrwpZl1PNmLFDlVCnopV8ysAtDM3RcA95B3JF8j8O+xm22MPc3d/yrwHOcD+919f77t84Hbg2rpEfi3jbuvcPcnyZsbXkEvYaWgl/ImBphhZiuAZcDz7r4PeAp43MyWcfqnNDMC/V8Bbihg+x1AbGAQOIn/Gx/4r8AA7ndAFvCv03x+kQJpPnqRMDCzhcDv3T0h0rWI5KcjehGRKKcjeolaZvYi0D/f6ufc/fVI1CMSKQp6EZEop1M3IiJRTkEvIhLlFPQiIlFOQS8iEuUU9CIiUe7/A4iuLgW1UlXwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grps.columns = grps.columns.map('_'.join)\n",
    "grps.reset_index().plot(x='n_samples', y='test_score_mean', logx=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = Path('/content/drive/MyDrive/research/predict-k-mirs-dl/dumps')\n",
    "\n",
    "with open(dest_dir/'cnn_test_perf_vs_n_samples.pickle', 'wb') as f: \n",
    "    pickle.dump(grps, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
